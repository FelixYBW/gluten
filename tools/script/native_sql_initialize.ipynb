{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Silencing error messages in the notebook -->\n",
    "<style>\n",
    "div.output_stderr {\n",
    "background: #ffdd;\n",
    "display: none;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Making the cells take up the full width of the window -->\n",
    "<style>\n",
    ".container { width:100% !important; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Changing the font of code cells -->\n",
    "<style>\n",
    ".CodeMirror{font-family: \"Courier New\";font-size: 12pt;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Changing the size of tables to 20px -->\n",
    "<style>\n",
    ".rendered_html table, .rendered_html td, .rendered_html th {font-size: 20px;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Set Envrionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'SPARK_HOME' not in os.environ or os.environ['SPARK_HOME']==\"\":\n",
    "    os.environ['SPARK_HOME']='/home/binweiyang/spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the os.environ object to a dictionary and then to a DataFrame\n",
    "env_df = pd.DataFrame(list(dict(os.environ).items()), columns=['Variable', 'Value'])\n",
    "\n",
    "# Display the DataFrame\n",
    "from IPython.display import display\n",
    "\n",
    "display(env_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "spark_version=!head  -n1 $SPARK_HOME/RELEASE | awk '{print $2}'\n",
    "spark_version = spark_version[0]\n",
    "\n",
    "print(f\"Spark version: {spark_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "localhost=socket.gethostname()\n",
    "local_ip=socket.gethostbyname(localhost)\n",
    "\n",
    "print(f'localhost: {localhost}')\n",
    "print(f'local_ip: {local_ip}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "EMON_EVENTS_ALL='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "CYCLE_ACTIVITY.STALLS_L3_MISS\n",
    "CYCLE_ACTIVITY.CYCLES_MEM_ANY\n",
    "OFFCORE_REQUESTS_OUTSTANDING.L3_MISS_DEMAND_DATA_RD\n",
    "OFFCORE_REQUESTS.L3_MISS_DEMAND_DATA_RD\n",
    "LONGEST_LAT_CACHE.MISS\n",
    "UNC_CHA_TOR_OCCUPANCY.IA_MISS_DRD_LOCAL\n",
    "UNC_CHA_TOR_INSERTS.IA_MISS_DRD_LOCAL\n",
    "UNC_CHA_CLOCKTICKS\n",
    "UNC_CHA_TOR_INSERTS.IA_MISS_DRD_PREF\n",
    "UNC_M_WPQ_OCCUPANCY_PCH0\n",
    "UNC_M_WPQ_INSERTS.PCH0\n",
    "UNC_M_CAS_COUNT.RD\n",
    "UNC_M_CAS_COUNT.WR\n",
    "UNC_IIO_DATA_REQ_OF_CPU.MEM_WRITE.PART2\n",
    "UNC_IIO_DATA_REQ_OF_CPU.MEM_READ.PART2\n",
    "UNC_IIO_DATA_REQ_BY_CPU.MEM_WRITE.PART2\n",
    "UNC_IIO_DATA_REQ_BY_CPU.MEM_READ.PART2\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "L2_RQSTS.MISS\n",
    "OFFCORE_REQUESTS_OUTSTANDING.DEMAND_DATA_RD\n",
    "OFFCORE_REQUESTS.DEMAND_DATA_RD\n",
    "MEM_LOAD_RETIRED.L3_MISS\n",
    "UOPS_ISSUED.STALL_CYCLES\n",
    "UNC_CHA_TOR_OCCUPANCY.IA_MISS_DRD_REMOTE\n",
    "UNC_CHA_TOR_INSERTS.IA_MISS_DRD_REMOTE\n",
    "UNC_CHA_TOR_INSERTS.IA_MISS_RFO\n",
    "UNC_CHA_TOR_INSERTS.IA_MISS_RFO_PREF\n",
    "UNC_M_RPQ_OCCUPANCY_PCH0\n",
    "UNC_M_RPQ_INSERTS.PCH0\n",
    "UNC_M_CLOCKTICKS\n",
    "UNC_IIO_COMP_BUF_OCCUPANCY.CMPD.ALL_PARTS\n",
    "UNC_IIO_COMP_BUF_INSERTS.CMPD.ALL_PARTS\n",
    "UNC_IIO_CLOCKTICKS\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEM_EMON_1='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "CYCLE_ACTIVITY.CYCLES_L2_MISS\n",
    "CYCLE_ACTIVITY.STALLS_TOTAL\n",
    "CYCLE_ACTIVITY.STALLS_L2_MISS\n",
    "CYCLE_ACTIVITY.STALLS_L3_MISS\n",
    " \n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "CYCLE_ACTIVITY.CYCLES_L1D_MISS\n",
    "CYCLE_ACTIVITY.STALLS_L1D_MISS\n",
    "CYCLE_ACTIVITY.CYCLES_MEM_ANY\n",
    "CYCLE_ACTIVITY.CYCLES_L3_MISS\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "MEM_EMON_2='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "CYCLE_ACTIVITY.STALLS_MEM_ANY\n",
    "UOPS_EXECUTED.CYCLES_GE_1\n",
    "UOPS_EXECUTED.CYCLES_GE_2\n",
    "UOPS_EXECUTED.CYCLES_GE_3\n",
    "\n",
    " \n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "UOPS_EXECUTED.CYCLES_GE_4\n",
    "UOPS_EXECUTED.THREAD\n",
    "UOPS_EXECUTED.STALLS\n",
    "UOPS_EXECUTED.CORE\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "MEM_EMON_3='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "UOPS_EXECUTED.CORE_CYCLES_GE_1\n",
    "UOPS_EXECUTED.CORE_CYCLES_GE_2\n",
    "UOPS_EXECUTED.CORE_CYCLES_GE_3\n",
    "UOPS_EXECUTED.CORE_CYCLES_GE_4\n",
    "\n",
    " \n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "UOPS_EXECUTED.X87\n",
    "UOPS_EXECUTED.CORE_CYCLES_NONE\n",
    "UOPS_EXECUTED.READ_PORT_STALLS\n",
    "UOPS_RETIRED.STALLS\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "MEM_EMON_4='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "UOPS_RETIRED.CYCLES\n",
    "UOPS_RETIRED.STALL_PERIODS\n",
    "UOPS_RETIRED.TOTAL_CYCLES\n",
    "RESOURCE_STALLS.ANY\n",
    "\n",
    " \n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "UOPS_ISSUED.STALLS\n",
    "UOPS_ISSUED.MRN_LDCHECK\n",
    "RS_EMPTY.COUNT\n",
    "RS_EMPTY.CYCLES\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "MEM_EMON_5='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "RS_EMPTY.BACKEND\n",
    "RS_EMPTY.RESOURCE\n",
    "RS_EMPTY.BAD_SPEC_ANY\n",
    "RS_EMPTY.OTHER\n",
    "\n",
    " \n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "IDQ_UOPS_NOT_DELIVERED.CYCLES_0_UOPS_DELIV.CORE\n",
    "L1D_PEND_MISS.PENDING_CYCLES\n",
    "L1D_PEND_MISS.L2_STALLS\n",
    "L1D_PEND_MISS.FB_FULL\n",
    ")\n",
    "'''\n",
    "\n",
    "MEM_EMON_6='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "TOPDOWN.SLOTS\n",
    "TOPDOWN.BACKEND_BOUND_SLOTS\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_4\n",
    "\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "TOPDOWN.SLOTS\n",
    "TOPDOWN.BAD_SPEC_SLOTS\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_8\n",
    ")\n",
    "'''\n",
    "\n",
    "MEM_EMON_7='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "TOPDOWN.SLOTS\n",
    "TOPDOWN.BR_MISPREDICT_SLOTS\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_16\n",
    "\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "TOPDOWN.SLOTS\n",
    "TOPDOWN.MEMORY_BOUND_SLOTS\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_32\n",
    ")\n",
    "'''\n",
    "\n",
    "MEM_EMON_8='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "MEM_INST_RETIRED.ALL_LOADS\n",
    "MEM_INST_RETIRED.ALL_STORES\n",
    "MEM_INST_RETIRED.ANY\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_64\n",
    "\n",
    " \n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "MEM_INST_RETIRED.LOCK_LOADS\n",
    "MEM_INST_RETIRED.LOCK_STORES\n",
    "MEM_INST_RETIRED.SPLIT_LOADS\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_128\n",
    ")\n",
    "'''\n",
    "\n",
    "MEM_EMON_9='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "MEM_INST_RETIRED.SPLIT_STORES\n",
    "MEM_INST_RETIRED.STLB_MISS_LOADS\n",
    "MEM_INST_RETIRED.STLB_MISS_STORES\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_256\n",
    "\n",
    " \n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "MISALIGN_MEM_REF.LOAD_PAGE_SPLIT\n",
    "MISALIGN_MEM_REF.STORE_PAGE_SPLIT\n",
    "IDQ_UOPS_NOT_DELIVERED.CYCLES_GE_1_UOP_DELIV.CORE\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_512\n",
    ")\n",
    "'''\n",
    "\n",
    "MEM_EMON_10='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "IDQ_UOPS_NOT_DELIVERED.CYCLES_LE_1_UOP_DELIV.CORE\n",
    "IDQ_UOPS_NOT_DELIVERED.CYCLES_LE_2_UOP_DELIV.CORE\n",
    "IDQ_UOPS_NOT_DELIVERED.CYCLES_LE_3_UOP_DELIV.CORE\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_1024\n",
    " \n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "MEM_TRANS_RETIRED.LOAD_LATENCY_GT_2048\n",
    "IDQ_UOPS_NOT_DELIVERED.OT_IDQ_EMPTY_CYCLES\n",
    "IDQ_UOPS_NOT_DELIVERED.OT_IDQ_EMPTY\n",
    "IDQ_UOPS_NOT_DELIVERED.FE_STALL_OT_IDQ_EMPTY\n",
    ")\n",
    "'''\n",
    "\n",
    "MEM_EMON_11='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "DTLB_LOAD_MISSES.WALK_ACTIVE\n",
    "DTLB_STORE_MISSES.WALK_ACTIVE\n",
    "LOAD_HIT_PRE.HW_PF\n",
    "LD_BLOCKS.STORE_FORWARD\n",
    "\n",
    " \n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "PAGE_WALKER_LOADS.DTLB_L1\n",
    "PAGE_WALKER_LOADS.DTLB_L2\n",
    "PAGE_WALKER_LOADS.DTLB_L3\n",
    "PAGE_WALKER_LOADS.DTLB_MEMORY\n",
    ")\n",
    "'''\n",
    "\n",
    "MEM_EMON_12='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "MEM_UOP_RETIRED.STA\n",
    "MEM_UOP_RETIRED.LOAD\n",
    "LD_BLOCKS.ADDRESS_ALIAS\n",
    "LD_BLOCKS.STORE_EARLY\n",
    "\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "LD_BLOCKS.NO_SR\n",
    "LD_BLOCKS.STORE_BLOCK\n",
    "LD_BLOCKS.STORE_ADDR_BLK\n",
    "LD_BLOCKS.DATA_UNKNOWN\n",
    ")\n",
    "'''\n",
    "\n",
    "MEM_EMON_TOPDOWN='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "TOPDOWN.SLOTS:perf_metrics\n",
    ")\n",
    "'''\n",
    "\n",
    "MEM_EMON_STALL='''\n",
    "-q -c -experimental -t0.5 -l100000 -u\n",
    "-C (\n",
    "INST_RETIRED.ANY\n",
    "CPU_CLK_UNHALTED.REF_TSC\n",
    "CPU_CLK_UNHALTED.THREAD\n",
    "UOPS_ISSUED.ANY:c1\n",
    "UOPS_RETIRED.CYCLES\n",
    "UOPS_EXECUTED.THREAD:c1\n",
    "RS.EMPTY\n",
    "RS_EMPTY.BAD_SPEC_ANY\n",
    "INT_MISC.CLEAR_RESTEER_CYCLES\n",
    "INT_MISC.UOP_DROPPING\n",
    "IDQ_UOPS_NOT_DELIVERED.CORE \n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System-level Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters. Need to be modified manually when migrating to new machine.\n",
    "\n",
    "clients=[localhost] # Workers\n",
    "\n",
    "# Labsever proxy used to connect to sr525.\n",
    "\n",
    "gluten_home='/home/sparkuser/gluten/'\n",
    "\n",
    "master=f'spark://{localhost}:7077'\n",
    "\n",
    "executors_per_node=8\n",
    "cores_per_executor=8\n",
    "gluten_tpch_task_per_core=2\n",
    "gluten_tpcds_task_per_core=3\n",
    "vanilla_tpch_task_per_core=8\n",
    "vanilla_tpcds_task_per_core=8\n",
    "\n",
    "memory_per_node='500g'\n",
    "\n",
    "vanilla_offheap_ratio=2.0 # onheap:offheap = 1:2\n",
    "gluten_offheap_ratio=7.0 # onheap:offheap = 1:7\n",
    "\n",
    "disk_dev=['nvme1n1p1', 'nvme2n1p1']\n",
    "nic_dev=['ens786f0']\n",
    "\n",
    "# Set collect_emon=False if emon cannot be installed on system.\n",
    "collect_emon=True\n",
    "# Copy EMON_EVENTS_ALL into file `emon.list`. Supported emon events on platform can be verified via `emon -i emon.list`\n",
    "emon_events=EMON_EVENTS_ALL\n",
    "# base_dir on sr525\n",
    "base_dir=\"spr_hbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters. Need to be modified manually when migrating to new machine.\n",
    "\n",
    "# Labsever proxy used to connect to sr525.\n",
    "proxy=\"\"\n",
    "\n",
    "master='yarn'\n",
    "\n",
    "disk_dev=['nvme0n1p1','nvme1n1p1', 'nvme2n1p1','nvme3n1p1','nvme6n1p1','nvme7n1p1','nvme8n1p1']\n",
    "nic_dev=['enx00e04c680304']\n",
    "\n",
    "collect_emon=True\n",
    "\n",
    "basedir=\"spr_hbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vars = vars()\n",
    "for k in [\n",
    "    'clients', \n",
    "    'proxy', \n",
    "    'master',\n",
    "    'disk_dev',\n",
    "    'nic_dev',\n",
    "    'collect_emon',\n",
    "    'emon_events']:\n",
    "    if global_vars.get(k):\n",
    "        print(f\"{k}: {global_vars[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "findspark.init(os.environ['SPARK_HOME'])\n",
    "os.environ.setdefault('SPARK_SUBMIT_OPTS', '-Dscala.usejavacp=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atexit\n",
    "import collections\n",
    "import gzip\n",
    "import importlib\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import signal\n",
    "import subprocess\n",
    "import tempfile\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas\n",
    "import platform\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import spylon_kernel\n",
    "from collections import namedtuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import date\n",
    "from functools import reduce\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import rcParams\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql import SparkSession, SQLContext, Window\n",
    "from pyspark.sql.functions import col, floor, lit, rank, to_date\n",
    "from pyspark.sql.types import (DoubleType, FloatType, IntegerType,\n",
    "                               StringType, StructField, StructType,\n",
    "                               TimestampType)\n",
    "\n",
    "from spylon_kernel import register_ipython_magics\n",
    "from spylon.spark.utils import SparkJVMHelpers\n",
    "\n",
    "register_ipython_magics()\n",
    "\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "rcParams['font.sans-serif'] = 'Courier New'\n",
    "rcParams['font.family'] = 'Courier New'\n",
    "rcParams['font.size'] = '12'\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"https_proxy\"] = proxy\n",
    "#os.environ[\"http_proxy\"] = proxy\n",
    "\n",
    "#print(f'proxy: {proxy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_event_dir=''\n",
    "local_event_dir=''\n",
    "\n",
    "def get_spark_eventlog_dir(path):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('spark.eventLog.dir'):\n",
    "                    return line.split(' ')[-1].strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No spark conf file: {path}\")\n",
    "    return None\n",
    "\n",
    "spark_defaults_conf = None\n",
    "\n",
    "if 'SPARK_CONF_DIR' in os.environ:\n",
    "    spark_defaults_conf = os.path.join(os.environ['SPARK_CONF_DIR'], 'spark-defaults.conf')\n",
    "elif 'SPARK_HOME' in os.environ:\n",
    "    spark_defaults_conf = os.path.join(os.environ['SPARK_HOME'], 'conf', 'spark-defaults.conf')\n",
    "\n",
    "if spark_defaults_conf:\n",
    "    event_log_dir = get_spark_eventlog_dir(spark_defaults_conf)\n",
    "    if event_log_dir:\n",
    "        print(f\"spark.eventLog.dir: {event_log_dir}\")\n",
    "        if event_log_dir[:7] == 'hdfs://':\n",
    "            hdfs_event_dir = event_log_dir\n",
    "        elif event_log_dir[:6] == 'file:/':\n",
    "            local_event_dir = event_log_dir[6:]\n",
    "    else:\n",
    "        print(\"spark.eventLog.dir not found in configuration file\")\n",
    "else:\n",
    "    print(\"Cannot get `spark.eventLog.dir`. Neither SPARK_CONF_DIR nor SPARK_HOME defined in envrionment variables.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_py4jzip():\n",
    "    spark_home=os.environ['SPARK_HOME']\n",
    "    py4jzip = !ls {spark_home}/python/lib/py4j*.zip\n",
    "    return py4jzip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def uniquify(path):\n",
    "    filename = path\n",
    "    counter = 1\n",
    "\n",
    "    while os.path.exists(path):\n",
    "        path = '.'.join([filename, str(counter)])\n",
    "        counter += 1\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import time\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from threading import Thread, Event\n",
    "\n",
    "class QATCollector:\n",
    "    qat_devs = ['6b', '70', '75', '7a', 'e8', 'ed', 'f2', 'f7']\n",
    "\n",
    "    def __init__(self, appid):\n",
    "        self.appid = appid\n",
    "        self.collect_qat_t = None\n",
    "        self.collect_qat_finished = None\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_responses(s):\n",
    "        responses = 0\n",
    "        for l in s:\n",
    "            if 'Responses' in l:\n",
    "                matched = re.findall(r'(\\d+) \\|', l)\n",
    "                if matched:\n",
    "                    responses += int(matched[0])\n",
    "        return responses\n",
    "\n",
    "    def collect_qat(self):\n",
    "        devs = self.qat_devs\n",
    "        end = [0] * len(devs)\n",
    "        fds = [open(f'{home}/profile/{self.appid}/{clients[0]}/qat{i}.csv', 'w+', buffering=1) for i in range(len(devs))]\n",
    "        for fd in fds:\n",
    "            fd.write('ts fw_counters_delta\\n')\n",
    "\n",
    "        for i, d in enumerate(devs):\n",
    "            fw_counters = !sudo cat /sys/kernel/debug/qat_4xxx_0000\\:{d}\\:00.0/fw_counters\n",
    "            end[i] = self.cal_responses(fw_counters)\n",
    "\n",
    "        while not self.collect_qat_finished.is_set():\n",
    "            time.sleep(1)\n",
    "            ts = time.strftime('%T')\n",
    "            for i, d in enumerate(devs):          \n",
    "                begin = end[i]\n",
    "                fw_counters = !sudo cat /sys/kernel/debug/qat_4xxx_0000\\:{d}\\:00.0/fw_counters\n",
    "                end[i] = self.cal_responses(fw_counters)\n",
    "                sub = end[i] - begin\n",
    "                fds[i].write(f'{ts} {sub}\\n')\n",
    "        for fd in fds:\n",
    "            fd.close()\n",
    "        print('QAT collect finished.')\n",
    "\n",
    "    def start_collect_qat(self):\n",
    "        self.collect_qat_finished = Event()\n",
    "        self.collect_qat_t = Thread(target=self.collect_qat)\n",
    "        self.collect_qat_t.start()\n",
    "\n",
    "    def stop_collect_qat(self):\n",
    "        self.collect_qat_finished.set()\n",
    "\n",
    "    def draw_qat(self, axs = None):\n",
    "        if not axs:\n",
    "            fig, axs = plt.subplots(1, 1, sharex=True, figsize=(30, 5*3))\n",
    "        for i, d in enumerate(self.qat_devs):\n",
    "            df = pandas.read_csv(f'{home}/profile/{self.appid}/{clients[0]}/qat{i}.csv', header=1, delim_whitespace=True, parse_dates=True)\n",
    "            df.columns=['time']+list(df.columns[1:])\n",
    "            df['time']=pandas.to_datetime(df['time'],errors='ignore',utc=True).astype(int)/1000000000\n",
    "            df['time']=df['time']-df['time'].min()\n",
    "            axs.plot(df['time'], df['fw_counters_delta'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyhdfs\n",
    "import socket\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "class Monitor:\n",
    "    def __init__(self, home_dir, clients):\n",
    "        self.home_dir=home_dir\n",
    "        self.clients=clients\n",
    "        self.qat_collector=None\n",
    "\n",
    "    def create_directory(self, path):\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def run_ssh_command(host, command):\n",
    "        subprocess.run(f'ssh {host} \"{command}\"', shell=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def kill_process(host, process_name):\n",
    "        pids = run_ssh_command(host, f\"ps aux | grep -w {process_name} | grep -v grep | tr -s ' ' | cut -d' ' -f2\")\n",
    "        for pid in pids:\n",
    "            run_ssh_command(host, f\"kill -9 {pid} > /dev/null 2>&1\")\n",
    "\n",
    "    def kill_monitors(self):\n",
    "        for client in self.clients:\n",
    "            self.kill_process(client, 'numactl')\n",
    "            self.kill_process(client, 'sar')\n",
    "            self.kill_process(client, 'pidstat')\n",
    "            self.kill_process(client, 'perf')\n",
    "            self.run_ssh_command(client, \"emon -stop > /dev/null 2>&1\")\n",
    "\n",
    "    def startmonitor(self, appid, **kwargs):\n",
    "        self.kill_monitors()\n",
    "        \n",
    "        local_profile_dir = os.path.join(self.home_dir, \"profile\")\n",
    "        app_profile_dir = os.path.join(local_profile_dir, appid)\n",
    "        self.create_directory(app_profile_dir)\n",
    "\n",
    "        # Details...\n",
    "        emon_list = os.path.join(self.home_dir, \"emon.list\")\n",
    "        if kwargs.get(\"collect_emon\",False):\n",
    "            emon_events = kwargs.get(\"emon_events\", EMON_EVENTS_ALL)\n",
    "            with open(emon_list,'w+') as f:\n",
    "                f.write(emon_events)\n",
    "            for l in self.clients:\n",
    "                !scp {emon_list} {l}:{emon_list} > /dev/null 2>&1\n",
    "\n",
    "        for l in self.clients:\n",
    "            client_profile_dir = os.path.join(app_profile_dir, l)\n",
    "            # Create client dir on master\n",
    "            self.create_directory(client_profile_dir)\n",
    "            # Create client dir on each client\n",
    "            self.run_ssh_command(l, f\"mkdir -p {client_profile_dir}\")\n",
    "\n",
    "            # Save uptime\n",
    "            !ssh root@{l} 'cat /proc/uptime | cut -d\" \" -f 1 | xargs -I ^ date -d \"- ^ seconds\"  +%s.%N' > $client_profile_dir/uptime.txt\n",
    "            # Start sar\n",
    "            !ssh $l \"sar -o {client_profile_dir}/sar.bin -r -u -d -B -n DEV 1 >/dev/null 2>&1 &\"\n",
    "            # Start pidstat\n",
    "            if kwargs.get(\"collect_pid\",False):\n",
    "                !ssh $l \"jps | grep CoarseGrainedExecutorBackend | head -n 1 | cut -d' ' -f 1 | xargs  -I % pidstat -h -t -p % 1  > {client_profile_dir}/pidstat.out  2>/dev/null &\"\n",
    "            if kwargs.get(\"collect_emon\",False):\n",
    "                !ssh {l} \"emon -i {emon_list} -f {client_profile_dir}/emon.rst >/dev/null 2>&1 & \"\n",
    "            if kwargs.get(\"collect_sched\",False):\n",
    "                !ssh root@{l} 'perf trace -e \"sched:sched_switch\" -C 8-15 -o {client_profile_dir}/sched.txt -T -- sleep 10000 >/dev/null 2>/dev/null &'\n",
    "            if kwargs.get(\"collect_perf_syscall\", False):\n",
    "                !ssh root@{l} \"perf stat -e 'syscalls:sys_exit_poll,syscalls:sys_exit_epoll_wait' -a -I 1000 -o {client_profile_dir}/perfstat.txt  >/dev/null 2>&1 & \"\n",
    "            if kwargs.get(\"collect_hbm\",False):\n",
    "                hbm_nodes = kwargs.get(\"hbm_nodes\")\n",
    "                if hbm_nodes is not None:\n",
    "                    print(\"collect_hbm\")\n",
    "                    hbm_nodes = '\\|'.join([\"node \" + str(i) for i in hbm_nodes])\n",
    "                    %env hbm_numa_nodes={hbm_nodes}\n",
    "                    %env hbm_l = {l}\n",
    "                    %env hbm_prof = {app_profile_dir}\n",
    "                    !ssh $hbm_l \"echo timestamp, size, free > $hbm_prof/$hbm_l/numactl.csv\"\n",
    "                    !ssh $hbm_l \"while :; do echo \\$(numactl -H | grep '$hbm_numa_nodes' | grep 'size' | awk '{ print \\$4 }' | awk '{ s += \\$1 } END { print s }'), \\$(numactl -H | grep '$hbm_numa_nodes' | grep 'free' | awk '{ print \\$4 }' | awk '{ s += \\$1 } END { print s }') | ts '%Y-%m-%d %H:%M:%S,' >> $hbm_prof/$hbm_l/numactl.csv; sleep 1; done >/dev/null 2>&1 &\"\n",
    "                else:\n",
    "                    print(\"Missing argument: hbm_nodes. e.g. hbm_nodes = list(range(8,16))\")\n",
    "\n",
    "        return prof\n",
    "\n",
    "    def stopmonitor(self, appid, **kwargs):\n",
    "        self.kill_monitors()\n",
    "        if self.qat_collector is not None:\n",
    "            self.qat_collector.stop_collect_qat()\n",
    "        \n",
    "        local_profile_dir = os.path.join(self.home_dir, \"profile\")\n",
    "        app_profile_dir = os.path.join(local_profile_dir, appid)\n",
    "        self.create_directory(app_profile_dir)\n",
    "\n",
    "        with open(os.path.join(app_profile_dir, \"starttime\"), \"w\") as f:\n",
    "            f.write(f\"{int(time.time() * 1000)}\")\n",
    "\n",
    "        # Details...\n",
    "        prof=app_profile_dir\n",
    "        for l in clients:\n",
    "            !ssh {l} \"sar -f {prof}/{l}/sar.bin -r > {prof}/{l}/sar_mem.sar;sar -f {prof}/{l}/sar.bin -u > {prof}/{l}/sar_cpu.sar;sar -f {prof}/{l}/sar.bin -d > {prof}/{l}/sar_disk.sar;sar -f {prof}/{l}/sar.bin -n DEV > {prof}/{l}/sar_nic.sar;sar -f {prof}/{l}/sar.bin -B > {prof}/{l}/sar_page.sar;\" \n",
    "#             targetdir=f\"{prof}/{l}\"\n",
    "#             if l==socket.gethostname():\n",
    "#                 !grep -rI xgbtck --no-filename {os.environ[\"SPARK_HOME\"]}/work/{appid}/* | sed 's/^ //g'  > {prof}/{l}/xgbtck.txt\n",
    "#             else:\n",
    "#                 !ssh {l} \"grep -rI xgbtck --no-filename {home}/hadoop-2.7.0/logs/userlogs/{appid}/* | sed 's/^ //g'  > {prof}/{l}/xgbtck.txt\"\n",
    "#             !ssh {l} \"{java_home}/bin/jps | grep CoarseGrainedExecutorBackend | head -n 2 | tail -n 1 | cut -d' ' -f 1  | xargs -I % ps -To tid p % \" > {targetdir}/sched_threads.txt\n",
    "            !ssh {l} \"source ~/sep_install/sep_vars.sh>/dev/null 2>&1; emon -v \" > {prof}/{l}/emonv.txt\n",
    "            !ssh {l} \"sar -V \" > {prof}/{l}/sarv.txt\n",
    "            !test -f {prof}/{l}/perfstat.txt && head -n 1 {prof}/{l}/perfstat.txt > {prof}/{l}/perfstarttime\n",
    "            if l!= localhost:\n",
    "                !scp -r {l}:{prof}/{l} {prof}/ > /dev/null 2>&1\n",
    "\n",
    "#         xgbfiles=!find {home}/profile/{appid} | grep xgbtck\n",
    "#         !cat /tmp/training_time >> {xgbfiles[0]}\n",
    "\n",
    "        if hdfs_event_dir != '':\n",
    "            !hadoop fs -copyToLocal $hdfs_event_dir/$appid $prof/app.log\n",
    "        elif local_event_dir != '':\n",
    "            !cp {local_event_dir}/{appid}  $prof/app.log\n",
    "        !cd {local_profile_dir} && tar zcvf {appid}.tar.gz {appid}\n",
    "        if kwargs.get(\"upload_sr525\",False):\n",
    "            upload525(base_dir, appid)\n",
    "        print('History eventlog: http://{:s}:18080/history/{:s}/jobs'.format(localhost,appid))\n",
    "        print('Traceview: http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{:s}.json'.format(appid))\n",
    "        print(appid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#%env http_proxy=\n",
    "#%env https_proxy=\n",
    "import pyhdfs\n",
    "fs_monitor = pyhdfs.HdfsClient(hosts='10.0.2.125:50070', user_name='yuzhou')\n",
    "\n",
    "import socket\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "# def upload525(base_dir, appid):\n",
    "#     sr525_local_dir=os.path.join('/home/yuzhou/PAUS/', base_dir)\n",
    "#     sr525_local_profile_dir=os.path.join(sr525_local_dir, 'profile')\n",
    "#     sr525_hdfs_dir=f'/{base_dir}/'\n",
    "#     !ssh sr525 \"mkdir -p {sr525_local_profile_dir}\"\n",
    "#     !scp ~/profile/{appid}.tar.gz sr525:{sr525_local_profile_dir}\n",
    "#     !ssh sr525 \"cd {sr525_local_profile_dir} && tar zxf {appid}.tar.gz > /dev/null 2>&1\"\n",
    "#     !ssh sr525 \"hdfs dfs -mkdir -p {sr525_hdfs_dir} && hdfs dfs -put {sr525_local_profile_dir}/{appid} {sr525_hdfs_dir}\"\n",
    "\n",
    "def upload525(base_dir, appid):\n",
    "    local_profile_dir=os.path.join(home, 'profile')\n",
    "    fs_monitor.mkdirs(f\"/{base_dir}/\")\n",
    "    v=[os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(os.path.join(local_profile_dir, appid))) for f in fn]\n",
    "    for f in v:\n",
    "        print(f)\n",
    "        paths=os.path.split(f)\n",
    "        fs_monitor.mkdirs(\"/\"+base_dir+paths[0][len(local_profile_dir):])\n",
    "        fs_monitor.copy_from_local(f,\"/\"+base_dir+paths[0][len(local_profile_dir):]+\"/\"+paths[1],overwrite=True)\n",
    "\n",
    "\n",
    "def killsar(clients):\n",
    "    out=!ps aux | grep -w sar | grep -v grep | tr -s ' ' | cut -d' ' -f2\n",
    "    for x in out:\n",
    "        !kill $x > /dev/null 2>&1\n",
    "    for l in clients:\n",
    "        out=!ps aux | grep -w pidstat | grep -v grep | tr -s ' ' | cut -d' ' -f2\n",
    "        for x in out:\n",
    "            !kill $x > /dev/null 2>&1\n",
    "\n",
    "def killnumactl(clients):\n",
    "    for l in clients:\n",
    "        out =!ps aux | grep numactl | grep bash | tr -s ' ' | cut -d' ' -f2\"\n",
    "        for x in out:\n",
    "            !kill $x > /dev/null 2>&1\"\n",
    "\n",
    "def killqat(clients):\n",
    "    for l in clients:\n",
    "        grep_qat='ps -ef | grep collect_qat.py | grep -v grep  | awk '\"'\"'{print $2}'\"'\"''\n",
    "        qat_collector_pids=!ssh root@{l} {grep_qat}\n",
    "        for pid in qat_collector_pids:\n",
    "            !ssh root@{l} \"kill {pid}\"\n",
    "\n",
    "def startmonitor(clients,appid,**kwargs):\n",
    "    local_profile_dir=home+\"/profile/\"\n",
    "    prof=local_profile_dir+appid+\"/\"\n",
    "    !mkdir -p $prof\n",
    "    \n",
    "    for l in clients:\n",
    "        !ssh root@$l date\n",
    "    \n",
    "    killsar(clients)\n",
    "    killqat(clients)\n",
    "    \n",
    "    if kwargs.get(\"collect_emon\",False):\n",
    "        with open(home+\"/emon.list\",'w+') as f:\n",
    "            f.write(emon_events)\n",
    "        for l in clients:\n",
    "            !scp {home}/emon.list {l}:{home}/emon.list  > /dev/null 2>&1\n",
    "    \n",
    "    perfsyscalls=kwargs.get(\"collect_perf_syscall\",None)\n",
    "    \n",
    "    for l in clients:\n",
    "        !mkdir -p $prof/$l/\n",
    "        !ssh $l mkdir -p $prof/$l/\n",
    "        !sar -o {prof}/{l}/sar.bin -r -u -d -B -n DEV 1 >/dev/null 2>&1 &\"\n",
    "        !ssh root@{l} \"jps | grep CoarseGrainedExecutorBackend | cut -d' ' -f 1 | xargs -I % bash -c '(cat /proc/%/status >> {prof}/{l}/%.stat; cat /proc/%/io >> {prof}/{l}/%.stat)'\"\n",
    "        if kwargs.get(\"collect_pid\",False):\n",
    "            !jps | grep CoarseGrainedExecutorBackend | head -n 1 | cut -d' ' -f 1 | xargs  -I % pidstat -h -t -p % 1  > {prof}/{l}/pidstat.out  2>/dev/null &\"\n",
    "        !ssh root@{l} 'cat /proc/uptime  | cut -d\" \" -f 1 | xargs -I ^ date -d \"- ^ seconds\"  +%s.%N' > $prof/$l/uptime.txt\n",
    "        if kwargs.get(\"collect_sched\",False):\n",
    "            !ssh root@{l} 'perf trace -e \"sched:sched_switch\" -C 8-15 -o {prof}/{l}/sched.txt -T -- sleep 10000 >/dev/null 2>/dev/null &'\n",
    "        if kwargs.get(\"collect_emon\",False):\n",
    "            !ssh {l} \"emon -i {home}/emon.list -f {prof}/{l}/emon.rst >/dev/null 2>&1 & \"\n",
    "        if perfsyscalls is not None:\n",
    "            !ssh root@{l} \"perf stat -e 'syscalls:sys_exit_poll,syscalls:sys_exit_epoll_wait' -a -I 1000 -o {prof}/{l}/perfstat.txt  >/dev/null 2>&1 & \"\n",
    "        if kwargs.get(\"collect_hbm\",False):\n",
    "            hbm_nodes = kwargs.get(\"hbm_nodes\")\n",
    "            if hbm_nodes is not None:\n",
    "                print(\"collect_hbm\")\n",
    "                hbm_nodes = '\\|'.join([\"node \" + str(i) for i in hbm_nodes])\n",
    "                %env hbm_numa_nodes={hbm_nodes}\n",
    "                %env hbm_l = {l}\n",
    "                %env hbm_prof = {prof}\n",
    "                !ssh $hbm_l \"echo timestamp, size, free > $hbm_prof/$hbm_l/numactl.csv\"\n",
    "                !ssh $hbm_l \"while :; do echo \\$(numactl -H | grep '$hbm_numa_nodes' | grep 'size' | awk '{ print \\$4 }' | awk '{ s += \\$1 } END { print s }'), \\$(numactl -H | grep '$hbm_numa_nodes' | grep 'free' | awk '{ print \\$4 }' | awk '{ s += \\$1 } END { print s }') | ts '%Y-%m-%d %H:%M:%S,' >> $hbm_prof/$hbm_l/numactl.csv; sleep 1; done >/dev/null 2>&1 &\"\n",
    "            else:\n",
    "                print(\"Missing argument: hbm_nodes. e.g. hbm_nodes = list(range(8,16))\")\n",
    "        if kwargs.get(\"collect_qat\",False):\n",
    "            blank='{}'\n",
    "            newline='\\\\n'\n",
    "            qat_collector_s = f'''\n",
    "import re\n",
    "import time\n",
    "from threading import Thread, Event\n",
    "\n",
    "class QATCollector:\n",
    "    qat_devs = ['6b', '70', '75', '7a', 'e8', 'ed', 'f2', 'f7']\n",
    "\n",
    "    def __init__(self):\n",
    "        self.collect_qat_t = None\n",
    "        self.collect_qat_finished = None\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_responses(s):\n",
    "        responses = 0\n",
    "        for l in s:\n",
    "            if 'Responses' in l:\n",
    "                matched = re.findall(r'(\\d+) \\|', l)\n",
    "                if matched:\n",
    "                    responses += int(matched[0])\n",
    "        return responses\n",
    "\n",
    "    def collect_qat(self):\n",
    "        devs = self.qat_devs\n",
    "        end = [0] * len(devs)\n",
    "        fds = [open('{home}/profile/{appid}/{l}/qat{blank}.csv'.format(i), 'w+', buffering=1) for i in range(len(devs))]\n",
    "        for fd in fds:\n",
    "            fd.write('ts ampm fw_counters_delta{newline}')\n",
    "\n",
    "        for i, d in enumerate(devs):\n",
    "            fw_counters_f = open('/sys/kernel/debug/qat_4xxx_0000:{blank}:00.0/fw_counters'.format(d))\n",
    "            fw_counters = fw_counters_f.readlines()\n",
    "            fw_counters_f.close()\n",
    "            end[i] = self.cal_responses(fw_counters)\n",
    "\n",
    "        while not self.collect_qat_finished.is_set():\n",
    "            time.sleep(1)\n",
    "            ts = time.strftime('%r')\n",
    "            for i, d in enumerate(devs):          \n",
    "                begin = end[i]\n",
    "                fw_counters_f = open('/sys/kernel/debug/qat_4xxx_0000:{blank}:00.0/fw_counters'.format(d))\n",
    "                fw_counters = fw_counters_f.readlines()\n",
    "                fw_counters_f.close()\n",
    "                end[i] = self.cal_responses(fw_counters)\n",
    "                sub = end[i] - begin\n",
    "                fds[i].write('{blank} {blank}{newline}'.format(ts, sub))\n",
    "        for fd in fds:\n",
    "            fd.close()\n",
    "        print('QAT collect finished.')\n",
    "\n",
    "    def start_collect_qat(self):\n",
    "        self.collect_qat_finished = Event()\n",
    "        self.collect_qat_t = Thread(target=self.collect_qat)\n",
    "        self.collect_qat_t.start()\n",
    "        self.collect_qat_t.join()\n",
    "\n",
    "    def stop_collect_qat(self):\n",
    "        self.collect_qat_finished.set()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    qat_collector = QATCollector()\n",
    "    qat_collector.start_collect_qat()\n",
    "            '''\n",
    "            with open('/tmp/collect_qat.py', 'w+') as f:\n",
    "                f.writelines(qat_collector_s)\n",
    "            !scp /tmp/collect_qat.py {l}:{prof}/{l}/\n",
    "            !ssh root@{l} \"nohup python3 {prof}/{l}/collect_qat.py >/dev/null 2>&1 &\"\n",
    "\n",
    "    return prof\n",
    "\n",
    "def stopmonitor(clients, sc, appid,**kwargs):\n",
    "    %cd ~\n",
    "    \n",
    "    local_profile_dir=home+\"/profile/\"\n",
    "    prof=local_profile_dir+appid+\"/\"\n",
    "    !mkdir -p $prof\n",
    "    \n",
    "    killsar(clients)\n",
    "    killnumactl(clients)\n",
    "    killqat(clients)\n",
    "    \n",
    "    with open(f\"{prof}/starttime\",\"w\") as f:\n",
    "        f.write(\"{:d}\".format(int(time.time()*1000)))\n",
    "    \n",
    "    for l in clients:\n",
    "        !ssh {l} \"sar -f {prof}/{l}/sar.bin -r > {prof}/{l}/sar_mem.sar;sar -f {prof}/{l}/sar.bin -u > {prof}/{l}/sar_cpu.sar;sar -f {prof}/{l}/sar.bin -d > {prof}/{l}/sar_disk.sar;sar -f {prof}/{l}/sar.bin -n DEV > {prof}/{l}/sar_nic.sar;sar -f {prof}/{l}/sar.bin -B > {prof}/{l}/sar_page.sar;\" \n",
    "        !ssh root@{l} \"jps | grep CoarseGrainedExecutorBackend | cut -d' ' -f 1 | xargs -I % bash -c '(cat /proc/%/status >> {prof}/{l}/%.stat; cat /proc/%/io >> {prof}/{l}/%.stat);chown binweiyang:binweiyang {prof}/{l}/%.stat'\"\n",
    "#         if l==socket.gethostname():\n",
    "#             !grep -rI xgbtck --no-filename {os.environ[\"SPARK_HOME\"]}/work/{appid}/* | sed 's/^ //g'  > {prof}/{l}/xgbtck.txt\n",
    "#         else:\n",
    "#             !ssh {l} \"grep -rI xgbtck --no-filename {home}/hadoop-2.7.0/logs/userlogs/{appid}/* | sed 's/^ //g'  > {prof}/{l}/xgbtck.txt\"\n",
    "        targetdir=f\"{prof}/{l}\"\n",
    "#        !ssh {l} \"{java_home}/bin/jps | grep CoarseGrainedExecutorBackend | head -n 2 | tail -n 1 | cut -d' ' -f 1  | xargs -I % ps -To tid p % \" > {targetdir}/sched_threads.txt\n",
    "        !ssh {l} \"source ~/sep_install/sep_vars.sh>/dev/null 2>&1; emon -v \" > {prof}/{l}/emonv.txt\n",
    "        !ssh {l} \"sar -V \" > {prof}/{l}/sarv.txt\n",
    "        !test -f {prof}/{l}/perfstat.txt && head -n 1 {prof}/{l}/perfstat.txt > {prof}/{l}/perfstarttime\n",
    "        if l!= socket.gethostname():\n",
    "            !scp -r {l}:{prof}/{l} {prof}/ > /dev/null 2>&1\n",
    "    if sc is not None:\n",
    "        sc.stop()\n",
    "    \n",
    "#    xgbfiles=!find {home}/profile/{appid} | grep xgbtck\n",
    "#    !cat /tmp/training_time >> {xgbfiles[0]}\n",
    "    \n",
    "    if hdfs_event_dir != '':\n",
    "        !hadoop fs -copyToLocal $hdfs_event_dir/$appid $prof/app.log\n",
    "    elif local_event_dir != '':\n",
    "        !cp {local_event_dir}/{appid}  $prof/app.log\n",
    "    !cd {local_profile_dir} && tar zcvf {appid}.tar.gz {appid}\n",
    "    if kwargs.get(\"upload_sr525\",False):\n",
    "        upload525(base_dir, appid)\n",
    "    print('http://{:s}:18080/history/{:s}/jobs'.format(socket.gethostname(),appid))\n",
    "    print('http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{:s}.json'.format(appid))\n",
    "    print(appid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pinexecutor(clients):\n",
    "    for client in clients:\n",
    "        pids=!ssh $client \"jps | grep CoarseGrainedExecutorBackend | cut -d' ' -f1\"\n",
    "        print(client,\":\",len(pids),\" \",\"\\t\".join(map(str,pids)))\n",
    "        \n",
    "        cpunum = !ssh {clients[0]} \"grep 'processor' /proc/cpuinfo | wc -l\"\n",
    "        cpunum = int(cpunum[0])\n",
    "\n",
    "        \n",
    "        start_c = 0\n",
    "        end_c = cores_per_executor - 1\n",
    "        for l in pids:\n",
    "            cpus='-'.join([str(start_c), str(end_c)])\n",
    "            print(f\" {cpus} {l}\",)\n",
    "            !ssh $client \"taskset -a -p -c $cpus $l > /dev/null 2>&1 \"\n",
    "            start_c = end_c + 1\n",
    "            end_c = end_c + cores_per_executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinexecutor_numa(clients):\n",
    "    cpunum = !ssh {clients[0]} \"grep 'processor' /proc/cpuinfo | wc -l\"\n",
    "    cpunum = int(cpunum[0])\n",
    "        \n",
    "    numanodes=!ssh {clients[0]} \"cat /sys/devices/system/node/node*/cpulist\"\n",
    "    numanodes = list(filter(lambda x: x != '', numanodes))\n",
    "    print(numanodes)\n",
    "    for client in clients:\n",
    "        pids=!ssh $client \"jps | grep CoarseGrainedExecutorBackend | cut -d' ' -f1\"\n",
    "        print(client,\":\",len(pids),\" \",\"\\t\".join(map(str,pids)))\n",
    "        \n",
    "        cpunum_c = !ssh {client} \"grep 'processor' /proc/cpuinfo | wc -l\"\n",
    "        cpunum_c = int(cpunum_c[0])\n",
    "        if cpunum_c != cpunum:\n",
    "            print(f\"client {client} cpunum not match!\")\n",
    "            return\n",
    "        numanodes_c=!ssh {client} \"cat /sys/devices/system/node/node*/cpulist\"\n",
    "        numanodes_c = list(filter(lambda x: x != '', numanodes))\n",
    "        time.sleep(1)\n",
    "        print(numanodes_c)\n",
    "        if numanodes_c != numanodes:\n",
    "            print(f\"client {client} numanodes not match!\")\n",
    "            return\n",
    "        \n",
    "        idx = 0\n",
    "        nodes=len(numanodes)\n",
    "        for i in range(nodes):\n",
    "            cpus = numanodes[i]\n",
    "            for l in pids[idx:idx+int(len(pids)/nodes)]: #  executors on 1 numanode\n",
    "                print(f\" {cpus} {l}\")\n",
    "                !ssh $client \"taskset -a -p -c $cpus $l > /dev/null 2>&1 \"\n",
    "            idx += int(len(pids)/nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def fixfreq(clients, freq):\n",
    "    freqstr=\"0x\"+hex(int(freq*10))[2:]*8\n",
    "    print(\"fix cpu freq to \", freq, \", set msr0x1ad to \",freqstr)\n",
    "    for client in clients:\n",
    "        !ssh root@$client \"/opt/intel/sep/bin64/emon --write-msr 0x1ad=$freqstr > /dev/null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixchafreq(clients, freq):\n",
    "    msr_freq={\n",
    "             1600:\"0x1010\",\n",
    "             1700:\"0x1111\",\n",
    "             1800:\"0x1212\",\n",
    "             1900:\"0x1313\",\n",
    "             2000:\"0x1414\",\n",
    "             2100:\"0x1515\",\n",
    "             2200:\"0x1616\"}    \n",
    "    freqstr=msr_freq[freq]\n",
    "    print(\"fix cha freq to \", freq, \", set msr0x1ad to \",freqstr)\n",
    "    for client in clients:\n",
    "        !ssh root@$client \"/opt/intel/sep/bin64/emon --write-msr 0x620=$freqstr > /dev/null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gettrainingtime(clients, appid):\n",
    "    xgbtck=[]\n",
    "    for l in clients:\n",
    "        o=!ssh $l \"cd /home/binweiyang/hadoop-2.7.0/logs/userlogs/{appid}/; grep xgbtck */stdout\"\n",
    "        xgbtck.extend(o)\n",
    "    if len(xgbtck)==0:\n",
    "        return\n",
    "    xgbtck=[l.split(\" \") for l in xgbtck]\n",
    "    begins=[int(l[3]) for l in xgbtck if l[1]==\"begin\"]\n",
    "    end=[int(l[3]) for l in xgbtck if l[1]==\"end\"]\n",
    "    trainingtime=max([(l[1]-l[0])/1000 for l in list(zip(begins,end))])\n",
    "    display(HTML(('Training Time(sec): <font size=6pt color=red>{:f}</font>'.format(trainingtime))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_pagecache(clients, run_gluten=True):\n",
    "    for l in clients:\n",
    "        if run_gluten:\n",
    "            !ssh root@$l \"echo 80 > /proc/sys/vm/dirty_ratio\"\n",
    "            !ssh root@$l \"echo 50 > /proc/sys/vm/dirty_background_ratio\"\n",
    "            !ssh root@$l \"echo 360000 > /proc/sys/vm/dirty_expire_centisecs\"\n",
    "            !ssh root@$l \"echo 3000 > /proc/sys/vm/dirty_writeback_centisecs\"\n",
    "\n",
    "        else:\n",
    "            !ssh root@$l \"echo 10 > /proc/sys/vm/dirty_ratio\"\n",
    "            !ssh root@$l \"echo 20 > /proc/sys/vm/dirty_background_ratio\"\n",
    "            !ssh root@$l \"echo 3000 > /proc/sys/vm/dirty_expire_centisecs\"\n",
    "            !ssh root@$l \"echo 500 > /proc/sys/vm/dirty_writeback_centisecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kernel_params(clietns):\n",
    "    params = {\n",
    "        'transparent hugepage': '/sys/kernel/mm/transparent_hugepage/enabled',\n",
    "        'auto numa balancing': '/proc/sys/kernel/numa_balancing',\n",
    "        'scaling governor': '/sys/devices/system/cpu/cpu*/cpufreq/scaling_governor',\n",
    "        'scaling max freq': '/sys/devices/system/cpu/cpu*/cpufreq/scaling_max_freq',\n",
    "        'scaling cur freq': '/sys/devices/system/cpu/cpu*/cpufreq/scaling_cur_freq',\n",
    "        'dirty_ratio': '/proc/sys/vm/dirty_ratio',\n",
    "        'dirty_background_ratio': '/proc/sys/vm/dirty_background_ratio',\n",
    "        'dirty_expire_centisecs': '/proc/sys/vm/dirty_expire_centisecs',\n",
    "        'dirty_writeback_centisecs': '/proc/sys/vm/dirty_writeback_centisecs'\n",
    "    }\n",
    "    for k, param in params.items():\n",
    "        print()\n",
    "        print(f'{k} ({param})')\n",
    "        for l in clients:\n",
    "            print(l + \": \", end='')\n",
    "            res = !ssh root@$l \"cat {param}\"\n",
    "            print(*res)\n",
    "    # print numactl\n",
    "    print()\n",
    "    print(\"numactl -H\")\n",
    "    for l in clients:\n",
    "        print(l + \":\")\n",
    "        res = !ssh $l \"numactl -H\"\n",
    "        print('\\n'.join(res))\n",
    "    # print memory freq\n",
    "    print()\n",
    "    print(\"Memory Frequency\")\n",
    "    for l in clients:\n",
    "        print(l + \":\")\n",
    "        res= !ssh root@$l \"dmidecode -t memory | grep Speed\"\n",
    "        print('\\n'.join(res))\n",
    "    print()\n",
    "    print(\"CPU Frequency\")\n",
    "    for l in clients:\n",
    "        print(l + \":\")\n",
    "        res= !ssh root@$l \"/opt/intel/sep/bin64/emon --read-msr 0x1ad\"\n",
    "        print('\\n'.join(res))\n",
    "    print()\n",
    "    print(\"CHA Frequency\")\n",
    "    for l in clients:\n",
    "        print(l + \":\")\n",
    "        res= !ssh root@$l \"/opt/intel/sep/bin64/emon --read-msr 0x620\"\n",
    "        print('\\n'.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dropcache(clients):\n",
    "    for l in clients:\n",
    "        #!ssh root@$l \"sync && echo 3 > /proc/sys/vm/drop_caches; free -h\"\n",
    "        !ssh root@$l \"sync && echo 3 > /proc/sys/vm/drop_caches; echo 1 >/proc/sys/vm/compact_memory; free -h\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sync_clocks(nodes):\n",
    "    !ssh root@127.0.0.1 \"/usr/sbin/ntpdate -u 10.0.0.100 2\"\n",
    "    for node in nodes:\n",
    "        !ssh root@$node \"/usr/sbin/ntpdate -u 10.0.0.100 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def start_yarn(c):\n",
    "    from pathlib import Path\n",
    "    home = str(Path.home())\n",
    "    !{home}/hadoop-2.7.0/sbin/stop-yarn.sh > /dev/null 2>&1\n",
    "    with open(f\"{home}/hadoop-2.7.0/etc/hadoop/slaves\",'w') as f:\n",
    "        for x in c:\n",
    "            f.write(x+\"\\n\")\n",
    "    !{home}/hadoop-2.7.0/sbin/start-yarn.sh > /dev/null 2>&1\n",
    "    display(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_codegen(nodes):\n",
    "    for c in nodes:\n",
    "        !ssh {c} \"rm -rf ~/shuffle/codegen/tmp/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_yarn(nodes):\n",
    "    for c in nodes:\n",
    "        !ssh {c} \"rm -rf /mnt/DP_disk*/sparkxgboost/yarn/local/usercache/binweiyang/appcache/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def get_io_stats(appid,  client):\n",
    "    file_path = os.path.join(home,'profile',appid,client)\n",
    "    statf = [f for f in os.listdir(file_path) if f.endswith('.stat')]\n",
    "    statmap=[]\n",
    "    for f in statf:\n",
    "        statmap.append({'pid':f[:-len(\".stat\")]})\n",
    "        with open(os.path.join(file_path, f),\"r\") as fi:\n",
    "            cnts=fi.readlines()\n",
    "        for l in cnts:\n",
    "            for fld in ['rchar','wchar','syscr','syscw','read_bytes','write_bytes','cancelled_write_bytes']:\n",
    "                if l.startswith(fld):\n",
    "                    if not fld in statmap[-1]:\n",
    "                        statmap[-1][fld]=int(l.split(\" \")[-1].strip())\n",
    "                    else:\n",
    "                        statmap[-1][fld]=(int(l.split(\" \")[-1].strip())-statmap[-1][fld])/1024/1024/1024\n",
    "\n",
    "    df = pd.DataFrame(statmap).drop('pid', axis=1).sum().to_frame()\n",
    "    df.columns = ['sum']\n",
    "    return df\n",
    "\n",
    "# Preprocess 'time' column\n",
    "def process_time(dataframes):\n",
    "    for df in dataframes:\n",
    "        df.columns=['time']+list(df.columns[1:])\n",
    "        df = df[df.time != 'Average:']\n",
    "        df['time'] = pd.to_datetime(df['time'], format='%H:%M:%S').dt.time\n",
    "        df['time'] = df['time'].apply(lambda dt: dt.hour*3600 + dt.minute*60 + dt.second)\n",
    "\n",
    "        offset = 12 * 3600 # half-day seconds\n",
    "        for i in range(1, len(df)):\n",
    "            if df['time'].iloc[i] < df['time'].iloc[i-1]:  # Detect AM->PM or PM->AM\n",
    "                for j in range(i, len(df)): # Apply offset until end\n",
    "                    df['time'].iloc[j] += offset\n",
    "\n",
    "        df['time'] = df['time'].astype(int)\n",
    "        yield df\n",
    "\n",
    "def draw_sar(appid, qtime=None, disk_dev=None, nic_dev=None, client=None):\n",
    "    if client is None:\n",
    "        client = clients[0]\n",
    "\n",
    "    display(HTML('<font size=6pt color=red>{:s}</font>'.format(client)))\n",
    "\n",
    "    display(get_io_stats(appid, client))\n",
    "\n",
    "    # Read data\n",
    "    profile_dir = os.path.join(home,'profile',appid,client)\n",
    "    datafiles = [os.path.join(profile_dir, datafile) for datafile in ['sar_cpu.sar', 'sar_mem.sar', 'sar_disk.sar', 'sar_nic.sar', 'sar_page.sar']]\n",
    "    dataframes = [pd.read_csv(datafile, header=1, delim_whitespace=True, parse_dates=True) for datafile in datafiles]\n",
    "    \n",
    "    qatfiles = list(map(lambda x: os.path.join(profile_dir, x), filter(lambda x: 'qat' in x and x[-4:] =='.csv', os.listdir(profile_dir))))\n",
    "  \n",
    "    num_figs=6 if qatfiles else 5\n",
    "    fig, axs=plt.subplots(num_figs,1,sharex=True,figsize=(30,5*4))\n",
    "\n",
    "    [cpu_df, mem_df, disk_df, nic_df, page_df] = process_time(dataframes)\n",
    "\n",
    "    # CPU usage\n",
    "    cpu_df['total'] = cpu_df['%user'] + cpu_df['%system'] + cpu_df['%iowait']\n",
    "\n",
    "    starttime = cpu_df[cpu_df['total'] > 50]['time'].min() - 1\n",
    "    cpu_df['time'] -= starttime\n",
    "\n",
    "    axs[4].stackplot(cpu_df['time'], cpu_df['%user'], cpu_df['%system'], cpu_df['%iowait'], labels=['user','system','iowait'])\n",
    "    axs[4].legend(loc='upper left')\n",
    "\n",
    "    # Memory usage\n",
    "    mem_df['dirty_cached'] = mem_df['kbdirty'] * mem_df['%memused'] / mem_df['kbmemused']\n",
    "    mem_df['clean_cached'] = (mem_df['kbcached'] - mem_df['kbdirty']) * mem_df['%memused'] / mem_df['kbmemused']\n",
    "    mem_df['used'] = mem_df['kbmemused'] * mem_df['%memused'] / mem_df['kbmemused']\n",
    "\n",
    "    mem_df['time'] -= starttime\n",
    "\n",
    "    axs[0].stackplot(mem_df['time'], mem_df['used'], mem_df['clean_cached'], mem_df['dirty_cached'], labels=['used','clean cached','dirty cached'])\n",
    "    axs[0].legend(loc='upper left')\n",
    "\n",
    "    # Disk usage\n",
    "    if disk_dev is not None:\n",
    "        disk_df = disk_df[disk_df['DEV'].isin(disk_dev)]\n",
    "    disk_df['rkB/s'] = disk_df['rkB/s'].astype(float)\n",
    "    disk_df['wkB/s'] = disk_df['wkB/s'].astype(float)\n",
    "\n",
    "    disk_df = disk_df.groupby('time').agg({'rkB/s': 'sum', 'wkB/s': 'sum'}).reset_index()\n",
    "    disk_df['read'] = disk_df['rkB/s'] / 1024\n",
    "    disk_df['write'] = disk_df['wkB/s'] / 1024\n",
    "\n",
    "    disk_df['time'] -= starttime\n",
    "\n",
    "    axs[1].stackplot(disk_df['time'], disk_df['read'], disk_df['write'], labels=['read MB/s','write MB/s'])\n",
    "    axs[1].legend(loc='upper left')\n",
    "    \n",
    "    # Nic usage\n",
    "    if nic_dev is not None:\n",
    "        nic_df = nic_df[nic_df['IFACE'].isin(nic_dev)]\n",
    "    nic_df['rxkB/s'] = nic_df['rxkB/s'].astype(float)\n",
    "    nic_df['txkB/s'] = nic_df['txkB/s'].astype(float)\n",
    "    \n",
    "    nic_df = nic_df.groupby('time').agg({'rxkB/s': 'sum', 'txkB/s': \"sum\"}).reset_index()\n",
    "    nic_df['rx'] = nic_df['rxkB/s'] / 1024\n",
    "    nic_df['tx'] = nic_df['txkB/s'] / 1024\n",
    "    \n",
    "    nic_df['time'] -= starttime\n",
    "    \n",
    "    axs[2].stackplot(nic_df['time'], nic_df['rx'], nic_df['tx'], labels=['rx MB/s','tx MB/s'])\n",
    "    axs[2].legend(loc='upper left')\n",
    "\n",
    "    # Pagefaults\n",
    "    page_df['minflt/s'] = page_df['fault/s'] - page_df['majflt/s']\n",
    "    \n",
    "    page_df['time'] -= starttime\n",
    "\n",
    "    axs[3].stackplot(page_df['time'], page_df['minflt/s'], page_df['majflt/s'], labels=['minor_fault/s','major_fault/s'])\n",
    "    axs[3].legend(loc='upper left')\n",
    "    \n",
    "    # QAT\n",
    "    if qatfiles:\n",
    "        qatdfs = [pd.read_csv(datafile, header=0, delim_whitespace=True, parse_dates=True) for datafile in qatfiles]\n",
    "        for qatdf in process_time(qatdfs):\n",
    "            qatdf['time'] -= starttime\n",
    "            axs[5].plot(qatdf['time'], qatdf['fw_counters_delta'])\n",
    "            \n",
    "\n",
    "    # Add vertical lines and text for qtime, and calculate per query cpu%\n",
    "    if qtime is not None:\n",
    "        for ax in axs:\n",
    "            x = 0\n",
    "            ax.axvline(x = x, color = 'b')\n",
    "            for k, v in qtime.items():\n",
    "                x += v\n",
    "                ax.axvline(x = x, color = 'b')\n",
    "\n",
    "            tx = 0\n",
    "            for k, v in qtime.items():\n",
    "                if v / x > 15 / 772:\n",
    "                    ax.text(tx + v / 2 - 6 * x / 772, ax.get_ylim()[1] * 1.05, k)\n",
    "                tx += v\n",
    "\n",
    "        x = 0\n",
    "        qtime_se = {}\n",
    "        cols = ['%user','%system','%iowait']\n",
    "        for k, v in qtime.items():\n",
    "            filtered_df = cpu_df[(cpu_df['time'] >= x) & (cpu_df['time'] <= x+v)]\n",
    "            averages = filtered_df[cols].mean()\n",
    "            qtime_se[k] = averages.tolist()\n",
    "            x += v\n",
    "        perqcpu = pandas.DataFrame(qtime_se).T\n",
    "        perqcpu.columns = cols\n",
    "        display(perqcpu)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def copyfile():\n",
    "    base = \"/mnt/DP_disk2/tpch_sf1t_parquet_192p\"\n",
    "    new_bases = [\"/mnt/DP_disk1/tpch_sf1t_parquet_192p\", \"/mnt/DP_disk2/tpch_sf1t_parquet_192p\"]\n",
    "    subdirs = os.listdir(base)\n",
    "    for table in subdirs:\n",
    "        p = os.path.join(base, table)\n",
    "        files = sorted(os.listdir(p))\n",
    "        flag = 0\n",
    "        for f in files:\n",
    "            if f[0:5] == \"part-\":\n",
    "                oldf = os.path.join(p, f)\n",
    "                if flag != 1:\n",
    "                    newp = os.path.join(new_bases[flag], table)\n",
    "                    if not os.path.exists(newp):\n",
    "                        os.mkdir(newp)\n",
    "                    newf = os.path.join(newp, f)\n",
    "                    print(newf)\n",
    "                    shutil.move(oldf, newf)\n",
    "                flag = (flag + 1) % len(new_bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestTPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from dataclasses import dataclass\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "from typing import List \n",
    "\n",
    "class TestTPC:\n",
    "    @dataclass\n",
    "    class query_info:\n",
    "        tables: List[str]\n",
    "        sql: List[str]\n",
    "        \n",
    "    _result_collector = {}\n",
    "    query_infos = {}\n",
    "    query_ids =[]\n",
    "    \n",
    "    tpctables=[]\n",
    "    tpc_query_path = ''\n",
    "    \n",
    "    def __init__(self, spark, table_dir, data_source = 'parquet'):\n",
    "        self.spark = spark\n",
    "        self.sc = spark.sparkSession.sparkContext\n",
    "        self.appid = self.sc.applicationId\n",
    "        self.table_dir = table_dir\n",
    "        self.data_source = data_source\n",
    "        self.table_loaded = False\n",
    "        for l in os.listdir(self.tpc_query_path):\n",
    "            if (l[-3:] == 'sql'):\n",
    "                with open(self.tpc_query_path+l,\"r\") as f:\n",
    "                    self.query_infos[l.split(\".\")[0]]=self.query_info(self.tpctables,[\"\\n\".join(f.readlines())])\n",
    "        self.query_ids = sorted(self.query_infos.keys(), key=lambda x: str(len(x))+x if x[-1] != 'a' and x[-1] != 'b' else str(len(x)-1) + x)\n",
    "        self.monitor = Monitor(str(Path.home()), clients)\n",
    "        print(\"http://{}:18080/history/{}/jobs/\".format(local_ip, self.sc.applicationId))\n",
    "    \n",
    "    def start_monitor(self, **kw):\n",
    "        return self.monitor.startmonitor(self.appid, **kw)\n",
    "    \n",
    "    def stop_monitor(self, **kw):\n",
    "        self.monitor.stopmonitor(self.appid, **kw)\n",
    "        if self.sc is not None:\n",
    "            self.sc.stop()\n",
    "        \n",
    "    def load_table(self, table):\n",
    "        if type(self.table_dir)==list:\n",
    "            return self.spark.read.format(self.data_source).load([os.path.join(t, table) for t in self.table_dir])\n",
    "        else:\n",
    "            return self.spark.read.format(self.data_source).load(os.path.join(self.table_dir, table))\n",
    "    \n",
    "    def load_tables_as_tempview(self, tables):\n",
    "        for table in tables:\n",
    "            df = self.load_table(table)\n",
    "            df.createOrReplaceTempView(table)\n",
    "        \n",
    "    def load_all_tables_as_tempview(self):\n",
    "        print(f\"Loading all tables: {self.tpctables}\")\n",
    "        self.load_tables_as_tempview(self.tpctables)\n",
    "    \n",
    "    def load_query(self, query):\n",
    "        info = self.query_infos[query]\n",
    "        return [self.spark.sql(q) for q in info.sql]\n",
    "    \n",
    "    def run_query(self, query, explain = False, print_result=False, load_table=True):\n",
    "        if load_table:\n",
    "            self.load_all_tables_as_tempview()\n",
    "        start_time = timeit.default_timer()\n",
    "        print(\"start query \" + query + \", application id \" + self.sc.applicationId)\n",
    "        print(\"{} : {}\".format(\"Start time\", start_time))\n",
    "        self.sc.setJobDescription(query)\n",
    "\n",
    "        queries = self.load_query(query)\n",
    "        for q in queries:\n",
    "            if explain: q.explain()\n",
    "            collect=q.collect()\n",
    "        end_time = timeit.default_timer()\n",
    "        duration = end_time - start_time\n",
    "        display(HTML(('Completed Query. Time(sec): <font size=6pt color=red>{:f}</font>'.format(duration))))\n",
    "        \n",
    "        self._result_collector[query] = duration\n",
    "        if print_result:\n",
    "            print(collect)\n",
    "\n",
    "    def power_run(self, explain=False, print_result=False, load_table=True):\n",
    "        if load_table:\n",
    "            self.load_all_tables_as_tempview()\n",
    "        for l in self.query_ids:\n",
    "            self.run_query(l, explain=explain, print_result=print_result, load_table=False)\n",
    "    \n",
    "    @classmethod\n",
    "    def clear_result(cls):\n",
    "        cls._result_collector = {}\n",
    "    \n",
    "    @classmethod\n",
    "    def collect_result(cls):\n",
    "        return cls._result_collector\n",
    "    \n",
    "    @classmethod\n",
    "    def print_result(cls):\n",
    "        result = cls._result_collector\n",
    "        print(result)\n",
    "        print()\n",
    "        durations = [float(i) for i in result.values()]\n",
    "        print(\"total duration:\")\n",
    "        print(sum(durations))\n",
    "        print()\n",
    "        nb_name_appid=f'{nb_name[:-6]}-{appid}.ipynb'\n",
    "        print(f\"http://sr525:8888/tree/{basedir}/ipython/{nb_name_appid}\")\n",
    "        print(f\"http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{appid}.json\")\n",
    "        print(f\"http://sr525:18080/history/{appid}\")\n",
    "        print(\"http://sr525:8888/view/\" + basedir + \"/html/tpch_\" + appid + \".html\")\n",
    "        print(appid)\n",
    "        for i in durations:\n",
    "            print(i)\n",
    "\n",
    "    @classmethod\n",
    "    def exec_time(cls):\n",
    "        cls.print_result()\n",
    "    \n",
    "    \n",
    "class TestTPCH(TestTPC):\n",
    "    tpctables = ['customer', 'lineitem', 'nation', 'orders', 'part', 'partsupp', 'region', 'supplier']\n",
    "    tpc_query_path = f'{gluten_home}/tools/gluten-it/common/src/main/resources/tpch-queries/'\n",
    "        \n",
    "    def __init__(self, spark, table_dir, data_source = 'parquet'):\n",
    "        TestTPC.__init__(self,spark, table_dir, data_source)\n",
    "                \n",
    "class TestTPCDS(TestTPC):\n",
    "    tpctables = [ 'call_center',\n",
    "         'catalog_page',\n",
    "         'catalog_returns',\n",
    "         'catalog_sales',\n",
    "         'customer',\n",
    "         'customer_address',\n",
    "         'customer_demographics',\n",
    "         'date_dim',\n",
    "         'household_demographics',\n",
    "         'income_band',\n",
    "         'inventory',\n",
    "         'item',\n",
    "         'promotion',\n",
    "         'reason',\n",
    "         'ship_mode',\n",
    "         'store',\n",
    "         'store_returns',\n",
    "         'store_sales',\n",
    "         'time_dim',\n",
    "         'warehouse',\n",
    "         'web_page',\n",
    "         'web_returns',\n",
    "         'web_sales',\n",
    "         'web_site']\n",
    "    tpc_query_path = f'{gluten_home}/tools/gluten-it/common/src/main/resources/tpcds-queries/'\n",
    "    \n",
    "    def __init__(self, spark, table_dir, data_source = 'parquet'):\n",
    "        TestTPC.__init__(self,spark, table_dir, data_source)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def default_conf(executors_per_node, cores_per_executor, task_per_core, memory_per_node, extra_jars='', app_name='', master='yarn', run_gluten=False):\n",
    "    # Create a temp directory that gets cleaned up on exit\n",
    "    output_dir = os.path.abspath(tempfile.mkdtemp())\n",
    "    def cleanup():\n",
    "        shutil.rmtree(output_dir, True)\n",
    "    atexit.register(cleanup)\n",
    "    signal.signal(signal.SIGTERM, cleanup)\n",
    "\n",
    "##################################################\n",
    "    def convert_to_bytes(size):\n",
    "        units = {'k': 1, 'm': 2, 'g': 3}\n",
    "        size = size.lower()\n",
    "        if size[-1] in units:\n",
    "            return int(size[:-1]) * 1024 ** units[size[-1]]\n",
    "        else:\n",
    "            return int(size)\n",
    "\n",
    "    def yarn_padding(size):\n",
    "        min_size =  convert_to_bytes('1g')\n",
    "        step = min_size\n",
    "        while size > min_size:\n",
    "            min_size += step\n",
    "        return min_size - size\n",
    "    \n",
    "    num_nodes = len(clients)\n",
    "    num_executors = num_nodes*executors_per_node\n",
    "    parallelism = num_executors*cores_per_executor*task_per_core\n",
    "\n",
    "    if run_gluten:\n",
    "        offheap_ratio = gluten_offheap_ratio\n",
    "    else:\n",
    "        offheap_ratio = vanilla_offheap_ratio\n",
    "    driver_memory = convert_to_bytes('20g')\n",
    "    executor_memory_overhead = convert_to_bytes('1g')\n",
    "    \n",
    "    # Minimun executor memory\n",
    "    min_memory = convert_to_bytes('1g')\n",
    "\n",
    "    # Calculate executor onheap memory\n",
    "    num_driver = 1 if localhost in clients else 0\n",
    "    executor_memory = math.floor((convert_to_bytes(memory_per_node) - (executor_memory_overhead + min_memory)*executors_per_node - (driver_memory + min_memory)*num_driver)/(offheap_ratio*num_driver + (1+offheap_ratio)*executors_per_node))\n",
    "    executor_memory = max(executor_memory, min_memory)\n",
    "    # Calculate driver/executor offheap memory in MB\n",
    "    #offheap_memory_per_node = convert_to_bytes(memory_per_node) - (executor_memory + executor_memory_overhead) * executors_per_node\n",
    "    if offheap_ratio > 0:\n",
    "        enable_offheap = True\n",
    "        offheap_memory = math.floor(executor_memory*offheap_ratio)\n",
    "    else:\n",
    "        enable_offheap = False\n",
    "        offheap_memory = 0\n",
    "\n",
    "    byte_to_mb = lambda x: int(x/(1024 ** 2))\n",
    "    driver_memory_mb = byte_to_mb(driver_memory)\n",
    "    executor_memory_overhead_mb = byte_to_mb(executor_memory_overhead)\n",
    "    executor_memory_mb = byte_to_mb(executor_memory)\n",
    "    offheap_memory_mb = byte_to_mb(offheap_memory)\n",
    "    \n",
    "    executor_totalmem_mb = executor_memory_overhead_mb + executor_memory_mb + offheap_memory_mb\n",
    "    executor_totalmem_mb = yarn_padding(executor_totalmem_mb)\n",
    "    if byte_to_mb(convert_to_bytes(memory_per_node)) - executor_totalmem_mb*executors_per_node > executor_totalmem_mb:\n",
    "        executor_memory_overhead_mb += 1024\n",
    "    \n",
    "    print('''\n",
    "        executors per node: {:d}\n",
    "        parallelism: {:d}\n",
    "        executor memory: {:d}m\n",
    "        offheap memory: {:d}m\n",
    "    '''.format(executors_per_node, parallelism, executor_memory_mb, offheap_memory_mb))\n",
    "\n",
    "    conf = SparkConf() \\\n",
    "        .set('spark.app.name', app_name)\\\n",
    "        .set('spark.master',master)\\\n",
    "        .set('spark.executor.memory', '{:d}m'.format(executor_memory_mb))\\\n",
    "        .set('spark.memory.offHeap.enabled', enable_offheap)\\\n",
    "        .set('spark.memory.offHeap.size','{:d}m'.format(offheap_memory_mb))\\\n",
    "        .set('spark.sql.shuffle.partitions', parallelism)\\\n",
    "        .set('spark.executor.instances', '{:d}'.format(num_executors))\\\n",
    "        .set('spark.executor.cores','{:d}'.format(cores_per_executor))\\\n",
    "        .set('spark.task.cpus','{:d}'.format(1))\\\n",
    "        .set('spark.driver.memory', '{:d}m'.format(driver_memory_mb))\\\n",
    "        .set('spark.executor.memoryOverhead', '{:d}m'.format(executor_memory_overhead_mb))\\\n",
    "        .set('spark.driver.maxResultSize', '4g')\\\n",
    "        .set('spark.executor.extraJavaOptions',\\\n",
    "            '-XX:+UseParallelOldGC -XX:ParallelGCThreads=2 -XX:NewRatio=1 -XX:SurvivorRatio=1 -XX:+UseCompressedOops')\\\n",
    "        .set('spark.driver.extraClassPath', extra_jars) \\\n",
    "        .set('spark.executor.extraClassPath', extra_jars) \\\n",
    "        .set('spark.executorEnv.PYTHONPATH',f\"{os.environ['SPARK_HOME']}python:{get_py4jzip()}\") \\\n",
    "        .set(\"spark.repl.class.outputDir\", output_dir) \\\n",
    "        .set(\"spark.sql.broadcastTimeout\", \"4800\") \\\n",
    "        .set('spark.serializer','org.apache.spark.serializer.KryoSerializer')\\\n",
    "        .set('spark.kryoserializer.buffer.max','512m')\\\n",
    "        .set('spark.kryo.unsafe',False)\\\n",
    "        .set('spark.sql.adaptive.enabled',True)\\\n",
    "        .set('spark.sql.autoBroadcastJoinThreshold',\"10m\")\\\n",
    "        .set('spark.sql.catalogImplementation','hive')\\\n",
    "        .set('spark.sql.optimizer.dynamicPartitionPruning.enabled',True)\\\n",
    "        .set('spark.executorEnv.LD_LIBRARY_PATH',\"/home/binweiyang/intel/oneapi/compiler/2023.2.0/linux/compiler/lib/intel64/\") \\\n",
    "        .set('spark.yarn.appMasterEnv.LD_LIBRARY_PATH',\"/home/binweiyang/intel/oneapi/compiler/2023.2.0/linux/compiler/lib/intel64/\") \\\n",
    "        .set('spark.cleaner.periodicGC.interval', '10s') \\\n",
    "        .set('spark.executorEnv.MALLOC_CONF','background_thread:true,dirty_decay_ms:10000,muzzy_decay_ms:10000,invalid_flag:foo') \\\n",
    "        .set('spark.task.maxFailures',2) \\\n",
    "        .set('spark.excludeOnFailure.task.maxTaskAttemptsPerNode',1) \\\n",
    "        .set('spark.excludeOnFailure.stage.maxFailedTasksPerExecutor',1) \\\n",
    "        .set('spark.excludeOnFailure.stage.maxFailedExecutorsPerNode',1) \\\n",
    "        .set('spark.excludeOnFailure.application.maxFailedTasksPerExecutor',1) \\\n",
    "        .set('spark.excludeOnFailure.application.maxFailedExecutorsPerNode',1) \\\n",
    "        .set('spark.excludeOnFailure.application.fetchFailure.enabled','true') \\\n",
    "        .set('spark.stage.maxConsecutiveAttempts',1) \\\n",
    "        .set('spark.excludeOnFailure.enabled','true') \\\n",
    "        .set('spark.executor.heartbeatInterval','30s') \\\n",
    "        .set('spark.yarn.maxAppAttempts',1)\n",
    "\n",
    "\n",
    "    return conf\n",
    "\n",
    "\n",
    "def create_cntx_with_config(conf,conf_overwrite=None):\n",
    "\n",
    "    importlib.reload(pyspark.java_gateway)\n",
    "\n",
    "    def Popen(*args, **kwargs):\n",
    "        \"\"\"Wraps subprocess.Popen to force stdout and stderr from the child process\n",
    "        to pipe to this process without buffering.\n",
    "        \"\"\"\n",
    "        global spark_jvm_proc\n",
    "        # Override these in kwargs to avoid duplicate value errors\n",
    "        # Set streams to unbuffered so that we read whatever bytes are available\n",
    "        # when ready, https://docs.python.org/3.6/library/subprocess.html#popen-constructor\n",
    "        kwargs['bufsize'] = 0\n",
    "        # Capture everything from stdout for display in the notebook\n",
    "        kwargs['stdout'] = subprocess.PIPE\n",
    "        print(\"java proc gateway popen\")\n",
    "        spark_jvm_proc = subprocess.Popen(*args, **kwargs)\n",
    "        return spark_jvm_proc\n",
    "    pyspark.java_gateway.Popen = Popen\n",
    "\n",
    "    spylon_kernel.scala_interpreter.scala_intp=None\n",
    "    \n",
    "    if conf_overwrite is not None:\n",
    "        conf=conf_overwrite(conf)\n",
    "    print(\"spark.serializer: \",conf.get(\"spark.serializer\"))\n",
    "    print(\"master: \",conf.get(\"spark.master\"))\n",
    "    \n",
    "    sc = SparkContext(conf = conf,master=conf.get(\"spark.master\"))\n",
    "    sc.setLogLevel('ERROR')\n",
    "    \n",
    "    sc.addPyFile(f\"{os.environ['SPARK_HOME']}/python/lib/pyspark.zip\")\n",
    "    sc.addPyFile(get_py4jzip())\n",
    "    \n",
    "    spark = SQLContext(sc)\n",
    "    \n",
    "    time.sleep(15)\n",
    "    \n",
    "    for client in clients:\n",
    "        pids=!ssh $client \"jps | grep CoarseGrainedExecutorBackend | cut -d' ' -f1\"\n",
    "        print(client,\":\",len(pids),\" \",\"\\t\".join(map(str,pids)))\n",
    "        \n",
    "    spark_session = SparkSession(sc)\n",
    "    spark_jvm_helpers = SparkJVMHelpers(spark_session._sc)\n",
    "    spylon_kernel.scala_interpreter.spark_state = spylon_kernel.scala_interpreter.SparkState(spark_session, spark_jvm_helpers, spark_jvm_proc)\n",
    "    \n",
    "    print(\"appid: \",sc.applicationId)\n",
    "    print(\"SparkConf:\")\n",
    "\n",
    "    df = pandas.DataFrame(sc.getConf().getAll(), columns=['key', 'value'])\n",
    "    display(df)\n",
    "\n",
    "    return sc, spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vanilla_tpch_conf_overwrite(conf):\n",
    "    return conf\n",
    "\n",
    "def vanilla_tpcds_conf_overwrite(conf):\n",
    "    conf.set('spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold', '0')\\\n",
    "        .set('spark.sql.optimizer.runtime.bloomFilter.enabled', 'true')\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_cntx_vanilla(executors_per_node, cores_per_executor, task_per_core, memory_per_node, extra_jars, app_name='', master='yarn', conf_overwrite=None):\n",
    "    conf = default_conf(executors_per_node, cores_per_executor, task_per_core, memory_per_node, extra_jars, app_name, master, run_gluten=False)\n",
    "    conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\",20480)\\\n",
    "        .set(\"spark.sql.parquet.columnarReaderBatchSize\",20480)\\\n",
    "        .set(\"spark.sql.inMemoryColumnarStorage.batchSize\",20480)\n",
    "    return create_cntx_with_config(conf,conf_overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gluten_tpch_conf_overwrite(conf):\n",
    "    return conf\n",
    "\n",
    "def gluten_tpcds_conf_overwrite(conf):\n",
    "    conf.set('spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold', '0')\\\n",
    "        .set('spark.sql.optimizer.runtime.bloomFilter.enabled', 'true')\\\n",
    "        .set('spark.gluten.sql.columnar.joinOptimizationLevel', '18')\\\n",
    "        .set('spark.gluten.sql.columnar.physicalJoinOptimizeEnable', 'true')\\\n",
    "        .set('spark.gluten.sql.columnar.physicalJoinOptimizationLevel', '18')\\\n",
    "        .set('spark.gluten.sql.columnar.logicalJoinOptimizeEnable', 'true')\\\n",
    "        .set('spark.gluten.sql.columnar.logicalJoinOptimizationLevel', '19')\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cntx_gluten(executors_per_node, cores_per_executor, task_per_core, memory_per_node, extra_jars, app_name='', master='yarn', conf_overwrite=None):\n",
    "    conf = default_conf(executors_per_node, cores_per_executor, task_per_core, memory_per_node, extra_jars, app_name, master, run_gluten=True)\n",
    "    conf.set('spark.sql.files.maxPartitionBytes', '2g')\\\n",
    "        .set('spark.plugins','org.apache.gluten.GlutenPlugin')\\\n",
    "        .set('spark.shuffle.manager','org.apache.spark.shuffle.sort.ColumnarShuffleManager')\\\n",
    "        .set('spark.gluten.sql.columnar.backend.lib','velox')\\\n",
    "        .set('spark.gluten.sql.columnar.maxBatchSize',4096)\\\n",
    "        .set('spark.gluten.sql.columnar.forceshuffledhashjoin',True)\\\n",
    "        .set('spark.executorEnv.LD_PRELOAD',f'/home/binweiyang/tools/lib/libjemalloc.so.2')\\\n",
    "        .set('spark.gluten.sql.columnar.coalesce.batches', 'true')\n",
    "    \n",
    "    return create_cntx_with_config(conf,conf_overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "git=None\n",
    "\n",
    "def create_cntx(run_gluten=False, workload='tpch', app_conf_overwrite=None, app_name=''):\n",
    "    table_dir=''\n",
    "    extra_jars = ''\n",
    "    is_tpch_workload=False\n",
    "    is_tpcds_workload=False\n",
    "    workload_conf_overwrite=None\n",
    "    create_cntx_func=None\n",
    "    test_tpc=None\n",
    "\n",
    "    if workload.lower() == 'tpch':\n",
    "        if not app_name:\n",
    "            app_name = 'tpch_power'\n",
    "        tabledir = tpch_tabledir\n",
    "        is_tpch_workload=True\n",
    "    elif workload.lower() == 'tpcds':\n",
    "        if not app_name:\n",
    "            app_name = 'tpcds_power'\n",
    "        tabledir = tpcds_tabledir\n",
    "        is_tpcds_workload=True\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown workload: {workload}\")\n",
    "\n",
    "\n",
    "    lastgit = !git -C /home/binweiyang/gluten log -1 | grep ^commit | cut -d \" \" -f 2\n",
    "    lastgit = lastgit[0]\n",
    "\n",
    "    nodes=len(clients)\n",
    "\n",
    "    if run_gluten:\n",
    "        jars_base=\"/home/binweiyang/jars/\"+lastgit\n",
    "        \n",
    "        if git is None:\n",
    "            !ls -l {gluten_target_jar}\n",
    "            !mkdir -p {jars_base}\n",
    "            !rm -rf {jars_base}/*\n",
    "            !cp {gluten_target_jar} {jars_base}/\n",
    "\n",
    "        jars=!ls -d {jars_base}/*.jar\n",
    "        extra_jars=\":\".join([\"file://\"+j for j in jars])\n",
    "        print(f'extra_jars: {extra_jars}')\n",
    "\n",
    "        for c in clients:\n",
    "            if c!=localhost:\n",
    "                !ssh {c} \"rm -rf {jars_base}\"\n",
    "                !ssh {c} \"mkdir -p {jars_base}\"\n",
    "                !scp {jars_base}/*.jar {c}:{jars_base} >/dev/null 2>&1\n",
    "\n",
    "        app_name = ' '.join(['gluten', app_name, lastgit[:6]])\n",
    "        create_cntx_func=create_cntx_gluten\n",
    "        if is_tpch_workload:\n",
    "            task_per_core = gluten_tpch_task_per_core\n",
    "            workload_conf_overwrite = gluten_tpch_conf_overwrite\n",
    "        elif is_tpcds_workload:\n",
    "            task_per_core = gluten_tpcds_task_per_core\n",
    "            workload_conf_overwrite = gluten_tpcds_conf_overwrite\n",
    "    else:\n",
    "        app_name = ' '.join(['vanilla', app_name, lastgit[:6]])\n",
    "        create_cntx_func=create_cntx_vanilla\n",
    "        if is_tpch_workload:\n",
    "            task_per_core = vanilla_tpch_task_per_core\n",
    "            workload_conf_overwrite = vanilla_tpch_conf_overwrite\n",
    "        elif is_tpcds_workload:\n",
    "            task_per_core = vanilla_tpcds_task_per_core\n",
    "            workload_conf_overwrite = vanilla_tpcds_conf_overwrite\n",
    "    \n",
    "    conf_overwrite = lambda conf: app_conf_overwrite(workload_conf_overwrite(conf))\n",
    "    \n",
    "    sc, spark = create_cntx_func(executors_per_node, cores_per_executor, task_per_core, memory_per_node, extra_jars, app_name, master, conf_overwrite)\n",
    "    \n",
    "    # Pin executors to numa nodes for Gluten\n",
    "    #if run_gluten:\n",
    "    #    pinexecutor_numa(clients)\n",
    "\n",
    "    appid = sc.applicationId\n",
    "    print(\"start run: \", appid)\n",
    "        \n",
    "    return sc, spark, None"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "364.469px",
    "left": "2087.8px",
    "top": "150.516px",
    "width": "375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
