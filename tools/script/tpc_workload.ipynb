{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- Silencing error messages in the notebook -->\n",
       "<style>\n",
       "div.output_stderr {\n",
       "background: #ffdd;\n",
       "display: none;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- Making the cells take up the full width of the window -->\n",
       "<style>\n",
       ".container { width:100% !important; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- Changing the font of code cells -->\n",
       "<style>\n",
       ".CodeMirror{font-family: \"Courier New\";font-size: 12pt;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- Changing the size of tables to 20px -->\n",
       "<style>\n",
       ".rendered_html table, .rendered_html td, .rendered_html th {font-size: 20px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SHELL</td>\n",
       "      <td>/bin/bash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUDO_GID</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JAVA_HOME</td>\n",
       "      <td>/usr/lib/jvm/java-8-openjdk-amd64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUDO_COMMAND</td>\n",
       "      <td>/bin/bash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUDO_USER</td>\n",
       "      <td>binweiyang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PWD</td>\n",
       "      <td>/home/sparkuser/ipython</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LOGNAME</td>\n",
       "      <td>sparkuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOME</td>\n",
       "      <td>/home/sparkuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YARN_CONF_DIR</td>\n",
       "      <td>/home/sparkuser/hadoop/etc/hadoop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LANG</td>\n",
       "      <td>C.UTF-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LS_COLORS</td>\n",
       "      <td>rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LESSCLOSE</td>\n",
       "      <td>/usr/bin/lesspipe %s %s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PYTHONPATH</td>\n",
       "      <td>/home/sparkuser/spark/python:/home/sparkuser/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TERM</td>\n",
       "      <td>xterm-color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HADOOP_CONF_DIR</td>\n",
       "      <td>/home/sparkuser/hadoop/etc/hadoop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HADOOP_HOME</td>\n",
       "      <td>/home/sparkuser/hadoop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LESSOPEN</td>\n",
       "      <td>| /usr/bin/lesspipe %s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>USER</td>\n",
       "      <td>sparkuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SPARK_LOCAL_DIRS</td>\n",
       "      <td>/home/sparkuser/sparkuser/hdfs/yarn/local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SHLVL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SPARK_HOME</td>\n",
       "      <td>/home/sparkuser/spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PATH</td>\n",
       "      <td>/usr/lib/jvm/java-8-openjdk-amd64/bin:/home/sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DOCKER_CONFIG</td>\n",
       "      <td>/etc/.docker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SUDO_UID</td>\n",
       "      <td>28613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MAIL</td>\n",
       "      <td>/var/mail/sparkuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>OLDPWD</td>\n",
       "      <td>/home/sparkuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>_</td>\n",
       "      <td>/bin/nohup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PYDEVD_USE_FRAME_EVAL</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JPY_PARENT_PID</td>\n",
       "      <td>2705006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CLICOLOR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>FORCE_COLOR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CLICOLOR_FORCE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>PAGER</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GIT_PAGER</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MPLBACKEND</td>\n",
       "      <td>module://matplotlib_inline.backend_inline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Variable                                              Value\n",
       "0                   SHELL                                          /bin/bash\n",
       "1                SUDO_GID                                                100\n",
       "2               JAVA_HOME                  /usr/lib/jvm/java-8-openjdk-amd64\n",
       "3            SUDO_COMMAND                                          /bin/bash\n",
       "4               SUDO_USER                                         binweiyang\n",
       "5                     PWD                            /home/sparkuser/ipython\n",
       "6                 LOGNAME                                          sparkuser\n",
       "7                    HOME                                    /home/sparkuser\n",
       "8           YARN_CONF_DIR                  /home/sparkuser/hadoop/etc/hadoop\n",
       "9                    LANG                                            C.UTF-8\n",
       "10              LS_COLORS  rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35...\n",
       "11              LESSCLOSE                            /usr/bin/lesspipe %s %s\n",
       "12             PYTHONPATH  /home/sparkuser/spark/python:/home/sparkuser/s...\n",
       "13                   TERM                                        xterm-color\n",
       "14        HADOOP_CONF_DIR                  /home/sparkuser/hadoop/etc/hadoop\n",
       "15            HADOOP_HOME                             /home/sparkuser/hadoop\n",
       "16               LESSOPEN                             | /usr/bin/lesspipe %s\n",
       "17                   USER                                          sparkuser\n",
       "18       SPARK_LOCAL_DIRS          /home/sparkuser/sparkuser/hdfs/yarn/local\n",
       "19                  SHLVL                                                  1\n",
       "20             SPARK_HOME                              /home/sparkuser/spark\n",
       "21                   PATH  /usr/lib/jvm/java-8-openjdk-amd64/bin:/home/sp...\n",
       "22          DOCKER_CONFIG                                       /etc/.docker\n",
       "23               SUDO_UID                                              28613\n",
       "24                   MAIL                                /var/mail/sparkuser\n",
       "25                 OLDPWD                                    /home/sparkuser\n",
       "26                      _                                         /bin/nohup\n",
       "27  PYDEVD_USE_FRAME_EVAL                                                 NO\n",
       "28         JPY_PARENT_PID                                            2705006\n",
       "29               CLICOLOR                                                  1\n",
       "30            FORCE_COLOR                                                  1\n",
       "31         CLICOLOR_FORCE                                                  1\n",
       "32                  PAGER                                                cat\n",
       "33              GIT_PAGER                                                cat\n",
       "34             MPLBACKEND          module://matplotlib_inline.backend_inline"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.3.2\n",
      "localhost: devrestricted-binweiyang\n",
      "local_ip: 10.9.235.144\n",
      "proxy: http://10.239.44.250:8080\n",
      "spark.eventLog.dir: hdfs://devrestricted-binweiyang:8020/tmp/sparkEventLog\n"
     ]
    }
   ],
   "source": [
    "%run /home/sparkuser/ipython/native_sql_initialize.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "codec='zstd'\n",
    "tabledir='/tpcds_sf1000_zstd/'\n",
    "workload='tpcds'\n",
    "run_gluten=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "fixed-params"
    ]
   },
   "outputs": [],
   "source": [
    "# Fixed parameters. Need to be modified manually when migrating to new machine.\n",
    "clients=[localhost]\n",
    "\n",
    "executors_per_node=2\n",
    "cores_per_executor=8\n",
    "memory_per_node='60g'\n",
    "\n",
    "disk_dev=['dev259-0']\n",
    "\n",
    "collect_emon=False\n",
    "\n",
    "master=f'spark://{localhost}:7077'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload=\"tpch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 binweiyang users 81846284 Sep 24 07:58 /home/binweiyang/gluten-velox-bundle-spark3.3_2.12-ubuntu_20.04-1.1.0-SNAPSHOT.jar\n",
      "file:///home/sparkuser/jars/spark33/gluten-velox-bundle-spark3.3_2.12-ubuntu_20.04-1.1.0-SNAPSHOT.jar\n"
     ]
    }
   ],
   "source": [
    "# Fixed parameters decided by input params\n",
    "jars = ''\n",
    "nodes=len(clients)\n",
    "\n",
    "appname=''\n",
    "is_tpch_workload=False\n",
    "is_tpcds_workload=False\n",
    "\n",
    "if workload.lower() == 'tpch':\n",
    "    app_name = 'tpch_power'\n",
    "    is_tpch_workload=True\n",
    "elif workload.lower() == 'tpcds':\n",
    "    app_name = 'tpcds_power'\n",
    "    is_tpcds_workload=True\n",
    "else:\n",
    "    raise ValueError(f\"Unknown workload: {workload}\")\n",
    "\n",
    "if run_gluten:\n",
    "    target_jar = f'/home/binweiyang/gluten-velox-bundle-spark3.3_2.12-ubuntu_20.04-1.1.0-SNAPSHOT.jar'\n",
    "\n",
    "    !ls -l {target_jar}\n",
    "\n",
    "    jars_base=\"/home/sparkuser/jars/spark33\"\n",
    "    !mkdir -p {jars_base}\n",
    "    !rm -rf {jars_base}/*\n",
    "    !cp {target_jar} {jars_base}/\n",
    "    jars=!ls -d {jars_base}/*.jar\n",
    "    jars=\":\".join([\"file://\"+j for j in jars])\n",
    "    print(jars)\n",
    "\n",
    "    task_per_core=3\n",
    "    app_name = ' '.join(['gluten', app_name])\n",
    "else:\n",
    "    task_per_core=8\n",
    "    app_name = ' '.join(['vanilla', app_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sparkuser/jars/spark33/gluten-velox-bundle-spark3.3_2.12-ubuntu_20.04-1.1.0-SNAPSHOT.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/sparkuser/jars/spark33/gluten-velox-bundle-spark3.3_2.12-ubuntu_20.04-1.1.0-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_conf_overwrite(conf, workload_conf_overwrite=None):\n",
    "    if workload_conf_overwrite:\n",
    "        conf = workload_conf_overwrite(conf)\n",
    "#     conf.set(xxx)\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gluten_tpcds_conf_overwrite(conf):\n",
    "    conf.set('spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold', '0')\\\n",
    "        .set('spark.sql.optimizer.runtime.bloomFilter.enabled', 'true')\\\n",
    "        .set('spark.gluten.sql.columnar.joinOptimizationLevel', '18')\\\n",
    "        .set('spark.gluten.sql.columnar.physicalJoinOptimizeEnable', 'true')\\\n",
    "        .set('spark.gluten.sql.columnar.physicalJoinOptimizationLevel', '18')\\\n",
    "        .set('spark.gluten.sql.columnar.logicalJoinOptimizeEnable', 'true')\\\n",
    "        .set('spark.gluten.sql.columnar.logicalJoinOptimizationLevel', '19')\\\n",
    "        .set('spark.gluten.sql.columnar.shuffle.codec', codec) \\\n",
    "        .set('spark.gluten.sql.columnar.backend.velox.memoryCapRatio',0.75)\n",
    "    return conf\n",
    "\n",
    "def vanilla_tpcds_conf_overwrite(conf):\n",
    "    conf.set('spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold', '0')\\\n",
    "        .set('spark.sql.optimizer.runtime.bloomFilter.enabled', 'true')\\\n",
    "        .set('spark.io.compression.codec', codec)\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gluten_tpch_conf_overwrite(conf):\n",
    "    conf.set('spark.gluten.sql.columnar.shuffle.codec', codec)\n",
    "    return conf\n",
    "\n",
    "def vanilla_tpch_conf_overwrite(conf):\n",
    "    conf.set('spark.io.compression.codec', codec)\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.create_cntx_gluten(executors_per_node, cores_per_executor, task_per_core, memory_per_node, extra_jars, app_name='', master='yarn', conf_overwrite=None)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_cntx_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        executors per node: 2\n",
      "        parallelism: 48\n",
      "        executor memory: 1558m\n",
      "        offheap memory: 10907m\n",
      "    \n",
      "spark.serializer:  org.apache.spark.serializer.KryoSerializer\n",
      "master:  spark://devrestricted-binweiyang:7077\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.ClassNotFoundException: org.apache.spark.shuffle.sort.ColumnarShuffleManager\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:218)\n\tat org.apache.spark.util.Utils$.instantiateSerializerOrShuffleManager(Utils.scala:2693)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:318)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:194)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:279)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:464)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         workload_conf_overwrite \u001b[38;5;241m=\u001b[39m vanilla_tpch_conf_overwrite\n\u001b[1;32m     19\u001b[0m conf_overwrite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m conf: global_conf_overwrite(conf, workload_conf_overwrite)\n\u001b[0;32m---> 20\u001b[0m sc, spark\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_cntx_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutors_per_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcores_per_executor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_per_core\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_per_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapp_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_overwrite\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ipykernel_2708407/2118165905.py:12\u001b[0m, in \u001b[0;36mcreate_cntx_gluten\u001b[0;34m(executors_per_node, cores_per_executor, task_per_core, memory_per_node, extra_jars, app_name, master, conf_overwrite)\u001b[0m\n\u001b[1;32m      2\u001b[0m conf \u001b[38;5;241m=\u001b[39m default_conf(executors_per_node, cores_per_executor, task_per_core, memory_per_node, extra_jars, app_name, master, run_gluten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.sql.files.maxPartitionBytes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10g\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.plugins\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mio.glutenproject.GlutenPlugin\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.shuffle.manager\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morg.apache.spark.shuffle.sort.ColumnarShuffleManager\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.executorEnv.LD_PRELOAD\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/usr/lib/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplatform\u001b[38;5;241m.\u001b[39mmachine()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-linux-gnu/libjemalloc.so.2\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.gluten.sql.columnar.coalesce.batches\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_cntx_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconf_overwrite\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ipykernel_2708407/2028789598.py:117\u001b[0m, in \u001b[0;36mcreate_cntx_with_config\u001b[0;34m(conf, conf_overwrite)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.serializer: \u001b[39m\u001b[38;5;124m\"\u001b[39m,conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.serializer\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaster: \u001b[39m\u001b[38;5;124m\"\u001b[39m,conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.master\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 117\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmaster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.master\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m sc\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m sc\u001b[38;5;241m.\u001b[39maddPyFile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSPARK_HOME\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/python/lib/pyspark.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/spark/python/pyspark/context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/spark/python/pyspark/context.py:282\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/spark/python/pyspark/context.py:402\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.ClassNotFoundException: org.apache.spark.shuffle.sort.ColumnarShuffleManager\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:218)\n\tat org.apache.spark.util.Utils$.instantiateSerializerOrShuffleManager(Utils.scala:2693)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:318)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:194)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:279)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:464)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "\n",
    "workload_conf_overwrite=None\n",
    "create_cntx_func=None\n",
    "\n",
    "if run_gluten:\n",
    "    create_cntx_func=create_cntx_gluten\n",
    "    if is_tpcds_workload:\n",
    "        workload_conf_overwrite = gluten_tpcds_conf_overwrite\n",
    "    else:\n",
    "        workload_conf_overwrite = gluten_tpch_conf_overwrite\n",
    "else:\n",
    "    create_cntx_func=create_cntx_vanilla\n",
    "    if is_tpcds_workload:\n",
    "        workload_conf_overwrite = vanilla_tpcds_conf_overwrite\n",
    "    else:\n",
    "        workload_conf_overwrite = vanilla_tpch_conf_overwrite\n",
    "\n",
    "conf_overwrite = lambda conf: global_conf_overwrite(conf, workload_conf_overwrite)\n",
    "sc, spark=create_cntx_func(executors_per_node, cores_per_executor, task_per_core, memory_per_node, jars, app_name, master, conf_overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_gluten:\n",
    "    pinexecutor_numa(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appid = sc.applicationId\n",
    "print(\"start run: \", appid)\n",
    "\n",
    "if is_tpch_workload:\n",
    "    test_tpc = TestTPCH(spark, tabledir,'parquet')\n",
    "elif is_tpcds_workload:\n",
    "    test_tpc = TestTPCDS(spark, tabledir, 'parquet')\n",
    "else:\n",
    "    raise ValueError(f\"Unknown workload: {workload}\")\n",
    "\n",
    "test_tpc.clear_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startmonitor(clients, appid, collect_emon=collect_emon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tpc.load_all_tables_as_tempview()\n",
    "for l in test_tpc.query_ids:\n",
    "    x=test_tpc.run_query(l, explain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopmonitor(clients, sc, appid, upload_sr525=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_sar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_147107/4004571786.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdraw_sar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_tpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisk_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'draw_sar' is not defined"
     ]
    }
   ],
   "source": [
    "draw_sar(appid, qtime=test_tpc.collect_result(), disk_dev=disk_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tpc.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "428.672px",
    "left": "1861.91px",
    "top": "266.297px",
    "width": "456.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
