{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BKM:\n",
    "*****\n",
    ">1. Config slaves as \"cluster palcement group\", to get best and stable throughput among nodes.\n",
    "2. Open port accesses internally, don't expose port access to public\n",
    "3. Configure security.authorization and ip control through hadoop\n",
    "4. Network throughput depends on instance type. If master node is small instance, avoid to copy large file from master to slaves or HDFS, using one of slavers\n",
    "5. If you want to cache the files to memory, set replication to 1. Otherwise you can't make sure Yarn schedule the same task to the same node always.\n",
    "6. If you copy large file from one slave to whole HDFS, the distribution is biased. The slave holds most of the data in its HDFS. Solution is to copy the file from HDFS to HDFS again\n",
    "7. vcpu isn't map to physical CPU with the same index, so you can't make sure two vcpu doesn't share the same physical core. So pin executors to cores through the native OS policy leads to poor performance. We need to make clear how vcpu share the same core firstly\n",
    "8. \n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command from shell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############################################################init1_root.sh:#################################################\n",
    "\n",
    "\n",
    "#echo -e \"g\\nw\\n\" | fdisk /dev/nvme1n1\n",
    "#echo -e \"n\\n\\n\\n\\nw\\n\" | fdisk /dev/nvme1n1\n",
    "#mkfs.ext4 /dev/nvme1n1p1\n",
    "\n",
    "#echo -e \"g\\nw\\n\" | fdisk /dev/nvme2n1\n",
    "#echo -e \"n\\n\\n\\n\\nw\\n\" | fdisk /dev/nvme2n1\n",
    "#mkfs.ext4 /dev/nvme2n1p1\n",
    "\n",
    "\n",
    "mkdir -p /data1 /data2\n",
    "\n",
    "mount /dev/nvme1n1p1 /data1\n",
    "\n",
    "mount /dev/nvme2n1p1 /data2\n",
    "\n",
    "mkdir -p /data1/home/sparkuser\n",
    "\n",
    "ln -s /data1/home/sparkuser /home/sparkuser\n",
    "\n",
    "cp /etc/skel/.bashrc /home/sparkuser/\n",
    "cp /etc/skel/.bash_logout /home/sparkuser/\n",
    "cp /etc/skel/.profile /home/sparkuser/\n",
    "\n",
    "adduser --home /home/sparkuser sparkuser\n",
    "\n",
    "passwd -d sparkuser\n",
    "\n",
    "chown -R sparkuser:sparkuser /data1\n",
    "\n",
    "chown -R sparkuser:sparkuser /data2\n",
    "\n",
    "#usermod -aG sudo sparkuser\n",
    "echo 'sparkuser ALL=(ALL:ALL) NOPASSWD:ALL' | EDITOR='tee -a' visudo\n",
    "\n",
    "\n",
    "apt update\n",
    "\n",
    "apt install -y sudo locales wget tar tzdata git ccache cmake ninja-build build-essential llvm-11-dev clang-11 libiberty-dev libdwarf-dev libre2-dev libz-dev libssl-dev libboost-all-dev libcurl4-openssl-dev openjdk-8-jdk maven vim pip sysstat gcc-9 libjemalloc-dev\n",
    "\n",
    "pip install notebook==6.5.2\n",
    " \n",
    "pip install jupyter_highlight_selected_word\n",
    "\n",
    "pip install jupyter_contrib_nbextensions\n",
    "\n",
    "\n",
    "su sparkuser\n",
    "================================================sparkuser.sh===========================================================\n",
    "\n",
    "sed -i '5,9 s/^/# /' ~/.bashrc\n",
    "\n",
    "rm -rf ~/.ssh\n",
    "\n",
    "ssh-keygen\n",
    "\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "\n",
    "# for spr_sdpcloud\n",
    "echo \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDVvpYEAj3S4BRWB64yvpkm0pGLOqgm68NJdwmM1wYQoCSPOunLQnyEezGENRuMdSNRpJmbyxi3fUtG0gy+6VjDVhHbOxrShpm8WO9lykSuoIsqeevyku9lb/wZdF70NC+GVPa7G+CL7PZGoo7zfnGLkU48caF67CXAfGIMM8Jstz7csdSkbkEvvOLmlEeAcBxuSXKT1Fu1y9UATqPJCw1Tq2XVXYxlAMNdfUDE/1Rti3nsCoqiAtyQ1LUJPhp8Acacu5xoWIuo1wvzyXnblxw50g5r5PXTGVKablzFZ+yz8EvkJ8JoJflyFYjG8fLKt8392wG9IBnpdE3rQMVcqsP//D9tqn1Tl+n4Xr1+O7UBfQlgGi+nnwht5n5YWOTiLQw0QU+OWWRMeh/tTUaTQng6eigAXBH4SEvjoAtFkR4gWcu0LYnX/vOvfSquwnT5M43Y7p5NJ56S+qXeNdoOilNRZ04y1UeN1Fujo+jX8ztEHF/aFCv9prHIlDMdwbpf84c= sparkuser@984fee00a50c.jf.intel.com\" >> ~/.ssh/authorized_keys\n",
    "\n",
    "# for sr525\n",
    "echo \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD8P4D68bH4EuX9N0V7MKNVtRWZgkSv0hu2kF4Bsk75bqASdUeMzbbMJB4tZWtQQrwGK/7sWLB+fH8pRkRE2yjzvkD9quhSk4qVNgSMf9dzcbGgvfe9aB4WXq9oADNYifaJfY3HgdqlditpBTWN4crtuI5D0sSum+vzCIFDd2kVWiqPgl3LbbxuHVOOJiN47/oApb0fK808+JSfFeu4QFTSZZXJKglQ8CYy3uy/ji8C9WyvWwqDWamYVuJq35E3LtM3Tm5uNOc0APvsdpcOuBQ8/7H/Vlj44kdvbM6FsKqcN3zazxwjnWBwlgfIGyfWQuZ+7eVumKfDWvsx4/pRAs43 yuzhou@sr507\" >> ~/.ssh/authorized_keys\n",
    "\n",
    "sudo -s\n",
    "================================================sudo.sh===========================================================\n",
    "\n",
    "rm -rf /root/.ssh\n",
    "\n",
    "ssh-keygen\n",
    "\n",
    "cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys\n",
    "\n",
    "cat /home/sparkuser/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys\n",
    "\n",
    "\n",
    "su sparkuser\n",
    "================================================sparkuser2.sh===========================================================\n",
    "\n",
    "ssh -o StrictHostKeyChecking=no root@localhost ls\n",
    "\n",
    "ssh -o StrictHostKeyChecking=no root@127.0.0.1 ls\n",
    "\n",
    "ssh -o StrictHostKeyChecking=no root@`hostname` ls\n",
    "\n",
    "cd /home/sparkuser/.local/lib/ && rm -rf python*\n",
    "\n",
    "pip install --upgrade jsonschema\n",
    "\n",
    "pip install jsonschema[format]\n",
    "\n",
    "pip install sqlalchemy==1.4.46\n",
    "\n",
    "pip install papermill Black\n",
    "\n",
    "pip install NotebookScripter\n",
    "\n",
    "pip3 install findspark spylon-kernel matplotlib pandasql pyhdfs\n",
    "\n",
    "jupyter notebook --generate-config\n",
    "\n",
    "jupyter notebook password\n",
    "\n",
    "mkdir -p ~/.jupyter/custom/\n",
    "\n",
    "echo '.container { width:100% !important; }' >> ~/.jupyter/custom/custom.css\n",
    "\n",
    "echo 'div.output_stderr { background: #ffdd; display: none; }'  >> ~/.jupyter/custom/custom.css\n",
    "\n",
    "\n",
    "jupyter nbextension install --py jupyter_highlight_selected_word --user\n",
    "\n",
    "jupyter nbextension enable highlight_selected_word/main\n",
    "\n",
    "jupyter contrib nbextension install --user\n",
    "\n",
    "jupyter nbextension enable codefolding/main\n",
    "\n",
    "jupyter nbextension enable code_prettify/code_prettify\n",
    "\n",
    "jupyter nbextension enable codefolding/edit\n",
    "\n",
    "jupyter nbextension enable code_font_size/code_font_size\n",
    "\n",
    "jupyter nbextension enable collapsible_headings/main\n",
    "\n",
    "jupyter nbextension enable highlight_selected_word/main\n",
    "\n",
    "jupyter nbextension enable ipyparallel/main\n",
    "\n",
    "jupyter nbextension enable move_selected_cells/main\n",
    "\n",
    "jupyter nbextension enable nbTranslate/main\n",
    "\n",
    "jupyter nbextension enable scratchpad/main\n",
    "\n",
    "jupyter nbextension enable tree-filter/index\n",
    "\n",
    "jupyter nbextension enable comment-uncomment/main\n",
    "\n",
    "jupyter nbextension enable export_embedded/main\n",
    "\n",
    "jupyter nbextension enable hide_header/main\n",
    "\n",
    "jupyter nbextension enable highlighter/highlighter\n",
    "\n",
    "jupyter nbextension enable scroll_down/main\n",
    "\n",
    "jupyter nbextension enable snippets/main\n",
    "\n",
    "jupyter nbextension enable toc2/main\n",
    "\n",
    "jupyter nbextension enable varInspector/main\n",
    "\n",
    "jupyter nbextension enable codefolding/edit\n",
    "\n",
    "jupyter nbextension enable contrib_nbextensions_help_item/main\n",
    "\n",
    "jupyter nbextension enable freeze/main\n",
    "\n",
    "jupyter nbextension enable hide_input/main\n",
    "\n",
    "jupyter nbextension enable jupyter-js-widgets/extension\n",
    "\n",
    "jupyter nbextension enable nbextensions_configurator/tree_tab/main\n",
    "\n",
    "jupyter nbextension enable snippets_menu/main\n",
    "\n",
    "jupyter nbextension enable table_beautifier/main\n",
    "\n",
    "jupyter nbextension enable hide_input_all/main\n",
    "\n",
    "jupyter nbextension enable nbextensions_configurator/config_menu/main\n",
    "\n",
    "jupyter nbextension enable spellchecker/main\n",
    "\n",
    "jupyter nbextension enable toggle_all_line_numbers/main\n",
    "\n",
    "\n",
    "mkdir -p ~/ipython\n",
    "\n",
    "cd ~/ipython\n",
    "\n",
    "nohup jupyter notebook --ip=127.0.0.1 --port=8888 &\n",
    "\n",
    "\n",
    "#cd ~/gluten/ep/build-velox/build/velox_ep/scripts && ./setup-ubuntu.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize <font color=red> Run this section after note book restart! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datadir=['/data1','/data2','/data3','/data4']\n",
    "datadir=['/home/sparkuser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients=''' '''.split()\n",
    "clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "masterip=socket.gethostbyname(socket.gethostname())\n",
    "hostname=socket.gethostname()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'devrestricted-binweiyang'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclients=clients.copy()\n",
    "hclients.append(hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['devrestricted-binweiyang']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hclients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"sparkuser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sparkuser\n"
     ]
    }
   ],
   "source": [
    "%cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 62496\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cpu_num=os.cpu_count()\n",
    "\n",
    "meminfo = dict((i.split()[0].rstrip(':'),int(i.split()[1])) for i in open('/proc/meminfo').readlines())\n",
    "mem_mib = int(int(meminfo['MemTotal'])/1024)-1024\n",
    "print(cpu_num,mem_mib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparklocals=\",\".join([f'{l}/{user}/hdfs/yarn/local' for l in datadir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/sparkuser']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_arm:  False\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "is_arm = platform.machine() == 'aarch64'\n",
    "print(\"is_arm: \",is_arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cpu number:  8\n"
     ]
    }
   ],
   "source": [
    "totalcpu=!grep 'processor' /proc/cpuinfo | wc -l\n",
    "totalcpu=int(totalcpu[0])\n",
    "print(\"total cpu number: \", totalcpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitions number 16\n"
     ]
    }
   ],
   "source": [
    "# for gluten, suggest partition as 2x cpu#\n",
    "scaleFactor = 1000\n",
    "numPartitions = 2*totalcpu if len(clients)==0 else len(clients)*2*totalcpu\n",
    "print(\"partitions number\", numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir = \"/tpcds_sf1000_zstd\" # root directory of location to create data in.\n",
    "dataformat = \"parquet\" # data format of data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSourceCodec = \"zstd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total memory is  65045400 KB\n"
     ]
    }
   ],
   "source": [
    "num_executors = 8\n",
    "executor_cores=totalcpu/num_executors\n",
    "\n",
    "totalmemory = !cat /proc/meminfo | grep MemTotal | awk '{print $2}'\n",
    "totalmemory = int(totalmemory[0])\n",
    "print(\"total memory is \", totalmemory, \"KB\")\n",
    "executor_memory = (totalmemory - 3*1024*1024)/num_executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# partition worker node <font color=red> SKIP for single node </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## partition and mount disks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!lsblk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "blks=['/dev/nvme2n1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "if len(blks) != len(datadir) - 1:\n",
    "    print(\"blks length and datadir length doesn't match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for l in hclients:\n",
    "    for b in blks:\n",
    "        !ssh root@{l} \"echo -e \\\"g\\\\nw\\\\n\\\" | fdisk {b}\"\n",
    "        !ssh root@{l} \"echo -e \\\"n\\\\n\\\\n\\\\n\\\\nw\\\\n\\\" | fdisk {b}\"\n",
    "        !ssh root@{l} \"mkfs.ext4 {b}p1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!lsblk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for l in hclients:\n",
    "    for b, d in zip(blks, datadir[1:]):\n",
    "        !ssh root@{l} \"mkdir -p {d}\"\n",
    "        !ssh root@{l} \"mount {b}p1 {d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!mount -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## install python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install findspark\n",
    "\n",
    "!pip install pandas numpy\n",
    "\n",
    "!pip install matplotlib\n",
    "\n",
    "!pip install pandasql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# create user<font color=red> SKIP for single node </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for l in clients:\n",
    "    !ssh root@$l adduser {user}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for i in clients:\n",
    "    !ssh root@$i cp -r .ssh /home/{user}/\n",
    "    !ssh root@$i chown -R {user}:{user} /home/{user}/.ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Download Spark Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sparkuser\n"
     ]
    }
   ],
   "source": [
    "%cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-23 05:12:42--  https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
      "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
      "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 299360284 (285M) [application/x-gzip]\n",
      "Saving to: ‘spark-3.3.2-bin-hadoop3.tgz’\n",
      "\n",
      "spark-3.3.2-bin-had 100%[===================>] 285.49M  27.3MB/s    in 11s     \n",
      "\n",
      "2023-09-23 05:12:54 (25.7 MB/s) - ‘spark-3.3.2-bin-hadoop3.tgz’ saved [299360284/299360284]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#download both version\n",
    "\n",
    "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz > /dev/null 2>&1\n",
    "if is_arm:\n",
    "    !wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5-aarch64.tar.gz > /dev/null 2>&1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create directories clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cmd=\";\".join([f\"chown -R {user}:{user} \" + l for l in datadir])\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cmd=\";\".join([f\"rm -rf {l}/tmp; mkdir -p {l}/tmp\" for l in datadir])\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rm -rf /home/sparkuser/tmp; mkdir -p /home/sparkuser/tmp'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cmd=\";\".join([f\"mkdir -p {l}/{user}/hdfs/name; mkdir -p {l}/{user}/hdfs/data; mkdir -p {l}/{user}/yarn; mkdir -p {l}/{user}/yarn/local\" for l in datadir])\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p {datadir[0]}/{user}/hdfs/namesecondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sparkuser'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'hadoop': No such file or directory\r\n",
      "mv: cannot stat 'spark': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!mv -f hadoop hadoop.bak; mv -f spark spark.bak\n",
    "!tar zxvf hadoop-3.2.4.tar.gz > /dev/null 2>&1\n",
    "!tar -zxvf spark-3.3.2-bin-hadoop3.tgz > /dev/null 2>&1\n",
    "!sudo apt install openjdk-8-jdk > /dev/null 2>&1\n",
    "!ln -s hadoop-3.2.4 hadoop; ln -s spark-3.3.2-bin-hadoop3 spark\n",
    "if is_arm:\n",
    "    !tar zxvf hadoop-3.3.5-aarch64.tar.gz > /dev/null 2>&1\n",
    "    !cd hadoop && mv lib lib.bak && cp -rf ~/hadoop-3.3.5/lib ~/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Config bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cfg=f'''export HADOOP_HOME=~/hadoop\n",
    "export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n",
    "\n",
    "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
    "export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
    "\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "export SPARK_HOME=~/spark\n",
    "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if is_arm:\n",
    "    cfg += 'export CPU_TARGET=\"aarch64\"\\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64\\nexport PATH=$JAVA_HOME/bin:$PATH\\n'\n",
    "else:\n",
    "    cfg += 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\\nexport PATH=$JAVA_HOME/bin:$PATH\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"tmpcfg\",'w') as f:\n",
    "    f.writelines(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!cat ~/tmpcfg >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\n",
      "export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\n",
      "\r\n",
      "export PATH=$JAVA_HOME/bin:$PATH\r\n",
      "export SPARK_HOME=~/spark\r\n",
      "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH\r\n",
      "export PATH=$SPARK_HOME/bin:$PATH\r\n",
      "\r\n",
      "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\r\n",
      "export PATH=$JAVA_HOME/bin:$PATH\r\n"
     ]
    }
   ],
   "source": [
    "!tail ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# config Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!sudo apt install -y libiberty-dev libxml2-dev libkrb5-dev libgsasl7-dev libuuid1 uuid-dev > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## setup  short-circuit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!sudo mkdir -p /var/lib/hadoop-hdfs/\n",
    "!sudo chown {user}:{user} /var/lib/hadoop-hdfs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## enable security.authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "coresite='''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "  <property>\n",
    "      <name>fs.default.name</name>\n",
    "      <value>hdfs://{:s}:8020</value>\n",
    "      <final>true</final>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>hadoop.security.authentication</name>\n",
    "      <value>simple</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>hadoop.security.authorization</name>\n",
    "      <value>true</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "'''.format(hostname)\n",
    "\n",
    "with open('hadoop/etc/hadoop/core-site.xml','w') as f:\n",
    "    f.writelines(coresite)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop/etc/hadoop/core-site.xml $l:~/hadoop/etc/hadoop/core-site.xml >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## set IP check, note the command <font color=red>\", \"</font>.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'devrestricted-binweiyang'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hadooppolicy='''<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "\n",
    " Licensed to the Apache Software Foundation (ASF) under one\n",
    " or more contributor license agreements.  See the NOTICE file\n",
    " distributed with this work for additional information\n",
    " regarding copyright ownership.  The ASF licenses this file\n",
    " to you under the Apache License, Version 2.0 (the\n",
    " \"License\"); you may not use this file except in compliance\n",
    " with the License.  You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License.\n",
    "\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>security.service.authorization.default.hosts</name>\n",
    "    <value>{:s}</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>security.service.authorization.default.acl</name>\n",
    "    <value>{:s} {:s}</value>\n",
    "  </property>\n",
    "  \n",
    "  \n",
    "    <property>\n",
    "    <name>security.client.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ClientProtocol, which is used by user code\n",
    "    via the DistributedFileSystem.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.client.datanode.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ClientDatanodeProtocol, the client-to-datanode protocol\n",
    "    for block recovery.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.datanode.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for DatanodeProtocol, which is used by datanodes to\n",
    "    communicate with the namenode.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.inter.datanode.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for InterDatanodeProtocol, the inter-datanode protocol\n",
    "    for updating generation timestamp.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.namenode.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for NamenodeProtocol, the protocol used by the secondary\n",
    "    namenode to communicate with the namenode.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    " <property>\n",
    "    <name>security.admin.operations.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for AdminOperationsProtocol. Used for admin commands.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.refresh.user.mappings.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for RefreshUserMappingsProtocol. Used to refresh\n",
    "    users mappings. The ACL is a comma-separated list of user and\n",
    "    group names. The user and group list is separated by a blank. For\n",
    "    e.g. \"alice,bob users,wheel\".  A special value of \"*\" means all\n",
    "    users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.refresh.policy.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for RefreshAuthorizationPolicyProtocol, used by the\n",
    "    dfsadmin and mradmin commands to refresh the security policy in-effect.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.ha.service.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for HAService protocol used by HAAdmin to manage the\n",
    "      active and stand-by states of namenode.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.zkfc.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for access to the ZK Failover Controller\n",
    "    </description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.qjournal.service.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for QJournalProtocol, used by the NN to communicate with\n",
    "    JNs when using the QuorumJournalManager for edit logs.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.mrhs.client.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for HSClientProtocol, used by job clients to\n",
    "    communciate with the MR History Server job status etc.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <!-- YARN Protocols -->\n",
    "\n",
    "  <property>\n",
    "    <name>security.resourcetracker.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ResourceTrackerProtocol, used by the\n",
    "    ResourceManager and NodeManager to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.resourcemanager-administration.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ResourceManagerAdministrationProtocol, for admin commands.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.applicationclient.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ApplicationClientProtocol, used by the ResourceManager\n",
    "    and applications submission clients to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.applicationmaster.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ApplicationMasterProtocol, used by the ResourceManager\n",
    "    and ApplicationMasters to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.containermanagement.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ContainerManagementProtocol protocol, used by the NodeManager\n",
    "    and ApplicationMasters to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.resourcelocalizer.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ResourceLocalizer protocol, used by the NodeManager\n",
    "    and ResourceLocalizer to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.job.task.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for TaskUmbilicalProtocol, used by the map and reduce\n",
    "    tasks to communicate with the parent tasktracker.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.job.client.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for MRClientProtocol, used by job clients to\n",
    "    communciate with the MR ApplicationMaster to query job status etc.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.applicationhistory.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ApplicationHistoryProtocol, used by the timeline\n",
    "    server and the generic history service client to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "</configuration>\n",
    "'''.format((\", \").join(hclients),user,user)\n",
    "\n",
    "with open('hadoop/etc/hadoop/hadoop-policy.xml','w') as f:\n",
    "    f.writelines(hadooppolicy)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop/etc/hadoop/hadoop-policy.xml $l:~/hadoop/etc/hadoop/hadoop-policy.xml >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## hdfs config, set replication to 1 to cache all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hdfs_name=\",\".join([f'{l}/{user}/hdfs/name' for l in datadir])\n",
    "hdfs_data=\",\".join([f'{l}/{user}/hdfs/data' for l in datadir])\n",
    "\n",
    "hdfs_site=f'''<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.namenode.secondary.http-address</name>\n",
    "    <value>{hostname}:50090</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>{hdfs_name}</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "        <value>{hdfs_data}</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>dfs.namenode.checkpoint.dir</name>\n",
    "    <value>{datadir[0]}/{user}/hdfs/namesecondary</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.name.handler.count</name>\n",
    "    <value>100</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.blocksize</name>\n",
    "    <value>128m</value>\n",
    "</property>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "   <name>dfs.client.read.shortcircuit</name>\n",
    "   <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "   <name>dfs.domain.socket.path</name>\n",
    "   <value>/var/lib/hadoop-hdfs/dn_socket</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "'''\n",
    "\n",
    "\n",
    "with open('hadoop/etc/hadoop/hdfs-site.xml','w') as f:\n",
    "    f.writelines(hdfs_site)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop/etc/hadoop/hdfs-site.xml $l:~/hadoop/etc/hadoop/hdfs-site.xml >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## mapreduce config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mapreduce='''<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "\n",
    "     <property>\n",
    "         <name>mapreduce.job.maps</name>\n",
    "         <value>288</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>mapreduce.job.reduces</name>\n",
    "         <value>64</value>\n",
    "     </property>\n",
    "\n",
    "     <property>\n",
    "         <name>mapreduce.map.java.opts</name>\n",
    "         <value>-Xmx5120M -DpreferIPv4Stack=true</value>\n",
    "     </property>\n",
    "      <property>\n",
    "         <name>mapreduce.map.memory.mb</name>\n",
    "         <value>6144</value>\n",
    "         </property>\n",
    "\n",
    "     <property>\n",
    "         <name>mapreduce.reduce.java.opts</name>\n",
    "         <value>-Xmx5120M -DpreferIPv4Stack=true</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>mapreduce.reduce.memory.mb</name>\n",
    "         <value>6144</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>yarn.app.mapreduce.am.staging-dir</name>\n",
    "         <value>/user</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>mapreduce.task.io.sort.mb</name>\n",
    "         <value>2000</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>mapreduce.task.timeout</name>\n",
    "         <value>3600000</value>\n",
    "     </property>\n",
    "<!-- MapReduce Job History Server security configs -->\n",
    "<property>\n",
    "  <name>mapreduce.jobhistory.address</name>\n",
    "  <value>{:s}:10020</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "'''.format(hostname)\n",
    "\n",
    "\n",
    "with open('hadoop/etc/hadoop/mapred-site.xml','w') as f:\n",
    "    f.writelines(mapreduce)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop/etc/hadoop/mapred-site.xml $l:~/hadoop/etc/hadoop/mapred-site.xml >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## yarn config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sparkuser\n"
     ]
    }
   ],
   "source": [
    "%cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "yarn_site=f'''<?xml version=\"1.0\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "<configuration>\n",
    "  <property>\n",
    "      <name>yarn.resourcemanager.hostname</name>\n",
    "      <value>{hostname}</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.resourcemanager.address</name>\n",
    "      <value>{hostname}:8032</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "      <value>{mem_mib}</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
    "      <value>{cpu_num}</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.pmem-check-enabled</name>\n",
    "      <value>false</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "      <value>false</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
    "      <value>4.1</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.aux-services</name>\n",
    "      <value>mapreduce_shuffle,spark_shuffle</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "      <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "      <value>1024</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "      <value>{mem_mib}</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
    "      <value>1</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
    "      <value>{cpu_num}</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "      <name>yarn.log-aggregation-enable</name>\n",
    "      <value>false</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.log.retain-seconds</name>\n",
    "      <value>36000</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.delete.debug-delay-sec</name>\n",
    "      <value>3600</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.log.server.url</name>\n",
    "      <value>http://{hostname}:19888/jobhistory/logs/</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.log-dirs</name>\n",
    "      <value>/home/{user}/hadoop/logs/userlogs</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.local-dirs</name>\n",
    "      <value>{sparklocals}\n",
    "      </value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\n",
    "      <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "'''\n",
    "\n",
    "\n",
    "with open('hadoop/etc/hadoop/yarn-site.xml','w') as f:\n",
    "    f.writelines(yarn_site)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop/etc/hadoop/yarn-site.xml $l:~/hadoop/etc/hadoop/yarn-site.xml >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## hadoop-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#config java home\n",
    "if is_arm:\n",
    "    !echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64\" >> ~/hadoop/etc/hadoop/hadoop-env.sh\n",
    "else:\n",
    "    !echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\" >> ~/hadoop/etc/hadoop/hadoop-env.sh\n",
    "\n",
    "for l in clients:\n",
    "    !scp hadoop/etc/hadoop/hadoop-env.sh $l:~/hadoop/etc/hadoop/ >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## slave config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('hadoop/etc/hadoop/slaves','w') as f:\n",
    "    f.writelines(\"\\n\".join(hclients))\n",
    "\n",
    "for l in clients:\n",
    "    !scp hadoop/etc/hadoop/slaves $l:~/hadoop/etc/hadoop/ >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!cp spark/yarn/spark-3.3.2-yarn-shuffle.jar ~/hadoop/share/hadoop/common/lib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# config spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eventlog_dir=f'hdfs://{hostname}:8020/tmp/sparkEventLog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sparkuser\n"
     ]
    }
   ],
   "source": [
    "sparkconf=f'''\n",
    "spark.eventLog.enabled    true\n",
    "spark.eventLog.dir        {eventlog_dir}\n",
    "spark.history.fs.logDirectory    {eventlog_dir}\n",
    "'''\n",
    "\n",
    "%cd ~\n",
    "with open('spark/conf/spark-defaults.conf','w+') as f:\n",
    "    f.writelines(sparkconf)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp ~/spark/conf/spark-defaults.conf $l:~/spark/conf/spark-defaults.conf >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sparkuser/sparkuser/hdfs/yarn/local'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparklocals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sparkenv = f'export SPARK_LOCAL_DIRS={sparklocals}\\n'\n",
    "with open('/home/sparkuser/.bashrc', 'a+') as f:\n",
    "    f.writelines(sparkenv)\n",
    "for l in clients:\n",
    "    !scp ~/.bashrc $l:~/.bashrc >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\n",
      "\r\n",
      "export PATH=$JAVA_HOME/bin:$PATH\r\n",
      "export SPARK_HOME=~/spark\r\n",
      "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH\r\n",
      "export PATH=$SPARK_HOME/bin:$PATH\r\n",
      "\r\n",
      "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\r\n",
      "export PATH=$JAVA_HOME/bin:$PATH\r\n",
      "export SPARK_LOCAL_DIRS=/home/sparkuser/sparkuser/hdfs/yarn/local\r\n"
     ]
    }
   ],
   "source": [
    "!tail ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> SOURCE bashrc ; REBOOT JUPYTER ; Run section 3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start dfs, yarn, spark history server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute namenode is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs namenode\" instead.\n",
      "\n",
      "WARNING: /home/sparkuser/hadoop/logs does not exist. Creating.\n",
      "2023-09-23 05:57:22,161 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = devrestricted-binweiyang.ec2.pin220.com/10.9.235.144\n",
      "STARTUP_MSG:   args = [-format]\n",
      "STARTUP_MSG:   version = 3.2.4\n",
      "STARTUP_MSG:   classpath = /home/sparkuser/hadoop/etc/hadoop:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/hadoop-annotations-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/hadoop-auth-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-xml-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/netty-3.10.6.Final.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/zookeeper-3.4.14.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.35.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/curator-framework-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-server-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/reload4j-1.2.18.3.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/curator-client-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-text-1.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-util-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-http-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.35.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-lang3-3.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/json-smart-2.4.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-core-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/slf4j-api-1.7.35.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-security-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-io-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/spark-3.3.2-yarn-shuffle.jar:/home/sparkuser/hadoop/share/hadoop/common/hadoop-common-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/common/hadoop-common-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/hadoop-kms-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/hadoop-nfs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.18.3.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-text-1.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-submarine-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.4.jar\n",
      "STARTUP_MSG:   build = Unknown -r 7e5d9983b388e372fe640f21f048f2f2ae6e9eba; compiled by 'ubuntu' on 2022-07-12T11:58Z\n",
      "STARTUP_MSG:   java = 1.8.0_382\n",
      "************************************************************/\n",
      "2023-09-23 05:57:22,168 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-23 05:57:22,236 INFO namenode.NameNode: createNameNode [-format]\n",
      "2023-09-23 05:57:22,537 INFO common.Util: Assuming 'file' scheme for path /home/sparkuser/sparkuser/hdfs/name in configuration.\n",
      "2023-09-23 05:57:22,538 INFO common.Util: Assuming 'file' scheme for path /home/sparkuser/sparkuser/hdfs/name in configuration.\n",
      "Formatting using clusterid: CID-61d40b26-c743-4898-b29a-e5e60d2e2416\n",
      "2023-09-23 05:57:22,578 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2023-09-23 05:57:22,602 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2023-09-23 05:57:22,604 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2023-09-23 05:57:22,604 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2023-09-23 05:57:22,608 INFO namenode.FSNamesystem: fsOwner             = sparkuser (auth:SIMPLE)\n",
      "2023-09-23 05:57:22,608 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "2023-09-23 05:57:22,608 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "2023-09-23 05:57:22,608 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2023-09-23 05:57:22,652 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2023-09-23 05:57:22,662 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "2023-09-23 05:57:22,662 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2023-09-23 05:57:22,666 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2023-09-23 05:57:22,667 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Sep 23 05:57:22\n",
      "2023-09-23 05:57:22,668 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2023-09-23 05:57:22,668 INFO util.GSet: VM type       = 64-bit\n",
      "2023-09-23 05:57:22,669 INFO util.GSet: 2.0% max memory 13.8 GB = 282.3 MB\n",
      "2023-09-23 05:57:22,669 INFO util.GSet: capacity      = 2^25 = 33554432 entries\n",
      "2023-09-23 05:57:22,750 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
      "2023-09-23 05:57:22,750 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2023-09-23 05:57:22,756 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "2023-09-23 05:57:22,757 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2023-09-23 05:57:22,757 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2023-09-23 05:57:22,757 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
      "2023-09-23 05:57:22,757 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2023-09-23 05:57:22,757 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2023-09-23 05:57:22,757 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2023-09-23 05:57:22,757 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2023-09-23 05:57:22,757 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2023-09-23 05:57:22,757 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2023-09-23 05:57:22,977 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
      "2023-09-23 05:57:22,977 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
      "2023-09-23 05:57:22,977 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
      "2023-09-23 05:57:22,977 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
      "2023-09-23 05:57:22,988 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2023-09-23 05:57:22,988 INFO util.GSet: VM type       = 64-bit\n",
      "2023-09-23 05:57:22,988 INFO util.GSet: 1.0% max memory 13.8 GB = 141.2 MB\n",
      "2023-09-23 05:57:22,988 INFO util.GSet: capacity      = 2^24 = 16777216 entries\n",
      "2023-09-23 05:57:23,003 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "2023-09-23 05:57:23,003 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2023-09-23 05:57:23,003 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2023-09-23 05:57:23,004 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2023-09-23 05:57:23,008 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
      "2023-09-23 05:57:23,010 INFO snapshot.SnapshotManager: SkipList is disabled\n",
      "2023-09-23 05:57:23,013 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2023-09-23 05:57:23,013 INFO util.GSet: VM type       = 64-bit\n",
      "2023-09-23 05:57:23,014 INFO util.GSet: 0.25% max memory 13.8 GB = 35.3 MB\n",
      "2023-09-23 05:57:23,014 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
      "2023-09-23 05:57:23,022 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2023-09-23 05:57:23,022 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2023-09-23 05:57:23,022 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2023-09-23 05:57:23,025 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2023-09-23 05:57:23,025 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2023-09-23 05:57:23,026 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2023-09-23 05:57:23,026 INFO util.GSet: VM type       = 64-bit\n",
      "2023-09-23 05:57:23,027 INFO util.GSet: 0.029999999329447746% max memory 13.8 GB = 4.2 MB\n",
      "2023-09-23 05:57:23,027 INFO util.GSet: capacity      = 2^19 = 524288 entries\n",
      "2023-09-23 05:57:23,051 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1863449719-10.9.235.144-1695448643044\n",
      "2023-09-23 05:57:23,062 INFO common.Storage: Storage directory /home/sparkuser/sparkuser/hdfs/name has been successfully formatted.\n",
      "2023-09-23 05:57:23,082 INFO namenode.FSImageFormatProtobuf: Saving image file /home/sparkuser/sparkuser/hdfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2023-09-23 05:57:23,179 INFO namenode.FSImageFormatProtobuf: Image file /home/sparkuser/sparkuser/hdfs/name/current/fsimage.ckpt_0000000000000000000 of size 404 bytes saved in 0 seconds .\n",
      "2023-09-23 05:57:23,185 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2023-09-23 05:57:23,238 INFO namenode.FSNamesystem: Stopping services started for active state\n",
      "2023-09-23 05:57:23,238 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
      "2023-09-23 05:57:23,241 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
      "2023-09-23 05:57:23,241 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at devrestricted-binweiyang.ec2.pin220.com/10.9.235.144\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/bin/hadoop namenode -format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute datanode is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs datanode\" instead.\n",
      "\n",
      "2023-09-23 05:57:30,773 INFO datanode.DataNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting DataNode\n",
      "STARTUP_MSG:   host = devrestricted-binweiyang.ec2.pin220.com/10.9.235.144\n",
      "STARTUP_MSG:   args = [-format]\n",
      "STARTUP_MSG:   version = 3.2.4\n",
      "STARTUP_MSG:   classpath = /home/sparkuser/hadoop/etc/hadoop:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/hadoop-annotations-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/hadoop-auth-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-xml-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/netty-3.10.6.Final.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/zookeeper-3.4.14.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.35.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/curator-framework-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-server-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/reload4j-1.2.18.3.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/curator-client-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-text-1.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-util-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-http-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.35.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-lang3-3.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/json-smart-2.4.7.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jackson-core-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/slf4j-api-1.7.35.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-security-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/jetty-io-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/home/sparkuser/hadoop/share/hadoop/common/lib/spark-3.3.2-yarn-shuffle.jar:/home/sparkuser/hadoop/share/hadoop/common/hadoop-common-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/common/hadoop-common-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/hadoop-kms-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/common/hadoop-nfs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.18.3.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-text-1.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.43.v20210629.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4-tests.jar:/home/sparkuser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/home/sparkuser/hadoop/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-submarine-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.4.jar:/home/sparkuser/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.4.jar\n",
      "STARTUP_MSG:   build = Unknown -r 7e5d9983b388e372fe640f21f048f2f2ae6e9eba; compiled by 'ubuntu' on 2022-07-12T11:58Z\n",
      "STARTUP_MSG:   java = 1.8.0_382\n",
      "************************************************************/\n",
      "2023-09-23 05:57:30,780 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs datanode [-regular | -rollback | -rollingupgrade rollback ]\r\n",
      "    -regular                 : Normal DataNode startup (default).\r\n",
      "    -rollback                : Rollback a standard or rolling upgrade.\r\n",
      "    -rollingupgrade rollback : Rollback a rolling upgrade operation.\r\n",
      "  Refer to HDFS documentation for the difference between standard\r\n",
      "  and rolling upgrades.\r\n",
      "\r\n",
      "2023-09-23 05:57:30,869 WARN datanode.DataNode: Exiting Datanode\r\n",
      "2023-09-23 05:57:30,871 INFO util.ExitUtil: Exiting with status 1: ExitException\r\n",
      "2023-09-23 05:57:30,873 INFO datanode.DataNode: SHUTDOWN_MSG: \r\n",
      "/************************************************************\r\n",
      "SHUTDOWN_MSG: Shutting down DataNode at devrestricted-binweiyang.ec2.pin220.com/10.9.235.144\r\n",
      "************************************************************/\r\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/bin/hadoop datanode -format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for l in clients:\n",
    "    !ssh $l \"ps aux | grep java\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [devrestricted-binweiyang]\n",
      "pdsh@devrestricted-binweiyang: devrestricted-binweiyang: rcmd: socket: Permission denied\n",
      "Starting datanodes\n",
      "pdsh@devrestricted-binweiyang: localhost: rcmd: socket: Permission denied\n",
      "Starting secondary namenodes [devrestricted-binweiyang]\n",
      "pdsh@devrestricted-binweiyang: devrestricted-binweiyang: rcmd: socket: Permission denied\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#skip if we run on single node, we use standalone mode\n",
    "\n",
    "!~/hadoop/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!~/hadoop/sbin/stop-all.sh >/dev/null 2>&1\n",
    "!~/hadoop/sbin/start-all.sh >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!scp -o StrictHostKeyChecking=no /home/sparkuser/.ssh/known_hosts {masterip}:~/.ssh/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir -p /tmp/sparkEventLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~/spark && sbin/start-history-server.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !ssh root@$c \"apt install sysstat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## skip emon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://mlp-spark-dataset-bucket/sep_private_5_23_linux_12142003820f194.tar.bz2 /home/{user}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !scp /home/{user}/sep_private_5_40_linux_042722006ceeae3.tar.bz2 $c:~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !ssh {c} bunzip2 sep_private_5_40_linux_042722006ceeae3.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !ssh {c} tar xvf sep_private_5_40_linux_042722006ceeae3.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !ssh {c} ln -s sep_private_5_40_linux_042722006ceeae3 sep_installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !ssh {c} \"cd sep_installed; bunzip2 sep_private_5.40_linux_042722006ceeae3.tar.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !ssh {c} \"cd sep_installed; tar xvf sep_private_5.40_linux_042722006ceeae3.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !ssh {c} \"cd sep_installed/sepdk/src/; echo -e \\\"\\\\n\\\\n\\\\n\\\" | ./build-driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !ssh root@{c} \"/home/{user}/sep_installed/sepdk/src/insmod-sep -g {user}; \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for c in hclients:\n",
    "    !ssh {c} \"source /home/{user}/sep_installed/sep_vars.sh > /dev/null 2>&1; emon -v | head -n 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!echo \"source /home/sparkuser/sep_installed/sep_vars.sh > /dev/null 2>&1\" >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Inspect CPU Freq & HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if is_arm:\n",
    "    t = r'''\n",
    "    #include <stdlib.h>\n",
    "    #include <stdio.h>\n",
    "    #include <assert.h>\n",
    "    #include <sys/time.h>\n",
    "    #include <sched.h>\n",
    "    #include <sys/shm.h>\n",
    "    #include <errno.h>\n",
    "    #include <sys/mman.h>\n",
    "    #include <unistd.h>                        //used for parsing the command line arguments\n",
    "    #include <fcntl.h>                        //used for opening the memory device file\n",
    "    #include <math.h>                        //used for rounding functions\n",
    "    #include <signal.h>\n",
    "    #include <linux/types.h>\n",
    "    #include <stdint.h>\n",
    "\n",
    "    static inline uint64_t GetTickCount()\n",
    "    {//Return ns counts\n",
    "        struct timeval tp;\n",
    "        gettimeofday(&tp,NULL);\n",
    "        return tp.tv_sec*1000+tp.tv_usec/1000;\n",
    "    }\n",
    "\n",
    "    uint64_t CNT=CNT_DEF;\n",
    "\n",
    "    int main()\n",
    "    {\n",
    "\n",
    "      uint64_t start, end;\n",
    "      start=end=GetTickCount();\n",
    "\n",
    "        asm  volatile (\n",
    "          \"1:\\n\"\n",
    "          \"SUBS %0,%0,#1\\n\"\n",
    "          \"bne 1b\\n\"\n",
    "          ::\"r\"(CNT)\n",
    "          );\n",
    "\n",
    "      end=GetTickCount();\n",
    "\n",
    "      printf(\" total time = %lu, freq = %lu \\n\", end-start, CNT/(end-start)/1000);\n",
    "\n",
    "      return 0;\n",
    "    }\n",
    "    '''\n",
    "else:\n",
    "    t=r'''\n",
    "    #include <stdlib.h>\n",
    "    #include <stdio.h>\n",
    "    #include <assert.h>\n",
    "    #include <sys/time.h>\n",
    "    #include <sched.h>\n",
    "    #include <sys/shm.h>\n",
    "    #include <errno.h>\n",
    "    #include <sys/mman.h>\n",
    "    #include <unistd.h>                        //used for parsing the command line arguments\n",
    "    #include <fcntl.h>                        //used for opening the memory device file\n",
    "    #include <math.h>                        //used for rounding functions\n",
    "    #include <signal.h>\n",
    "    #include <linux/types.h>\n",
    "    #include <stdint.h>\n",
    "\n",
    "    static inline uint64_t GetTickCount()\n",
    "    {//Return ns counts\n",
    "        struct timeval tp;\n",
    "        gettimeofday(&tp,NULL);\n",
    "        return tp.tv_sec*1000+tp.tv_usec/1000;\n",
    "    }\n",
    "\n",
    "    uint64_t CNT=CNT_DEF;\n",
    "\n",
    "    int main()\n",
    "    {\n",
    "\n",
    "      uint64_t start, end;\n",
    "      start=end=GetTickCount();\n",
    "\n",
    "        asm  volatile (\n",
    "          \"1:\\n\"\n",
    "          \"dec %0\\n\"\n",
    "          \"jnz 1b\\n\"\n",
    "          ::\"r\"(CNT)\n",
    "          );\n",
    "\n",
    "      end=GetTickCount();\n",
    "\n",
    "      printf(\" total time = %lu, freq = %lu \\n\", end-start, CNT/(end-start)/1000);\n",
    "\n",
    "      return 0;\n",
    "    }\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%cd ~\n",
    "with open(\"t.c\", 'w') as f:\n",
    "    f.writelines(t)\n",
    "!gcc -O3 -DCNT_DEF=10000000000LL -o t t.c; gcc -O3 -DCNT_DEF=1000000000000LL -o t.delay t.c;\n",
    "!for j in `seq 1 $(nproc)`; do echo -n $j; (for i in `seq 1 $j`; do taskset -c $i ./t.delay & done); sleep 1; ./t; killall t.delay; sleep 2; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clear yarn cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "clearcachecmd=\";\".join([f\"rm -rf {l}/{user}/hdfs/yarn/local/usercache/*\" for l in datadir])\n",
    "for c in hclients:\n",
    "    !ssh {c} {clearcachecmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install gluten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!rm -rf ~/gluten\n",
    "!git clone https://github.com/oap-project/gluten.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/gluten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh localhost \"cd ~/gluten/dev && nohup ./buildbundle-veloxbe.sh --enable_hdfs=ON >build.log 2>&1 &\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ~/gluten/package/target/ | grep bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "%cd ep/build-velox/src\n",
    "!./get_velox.sh --enable_hdfs=ON\n",
    "!./build_velox.sh --enable_hdfs=ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!which java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# %cd ~/gluten/ep/build-arrow/src/\n",
    "# ./get_arrow.sh\n",
    "# ./build_arrow.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "%cd ~/gluten/cpp\n",
    "!mkdir build\n",
    "%cd build\n",
    "!cmake -DBUILD_VELOX_BACKEND=ON -DENABLE_HDFS=ON ..\n",
    "!make -j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "%cd ~/gluten\n",
    "!mvn clean package -Pbackends-velox -Pspark-3.3 -Pfull-scala-compiler -DskipTests -Dcheckstyle.skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> \n",
    "Copy databricks package from another machine:\n",
    "\n",
    "on sr525 execute\n",
    "\n",
    "!scp -r -P 22 -o \"ProxyCommand nc --proxy-type socks5 --proxy child-prc.intel.com:1080 %h %p\" /home/yuzhou/PAUS/gcp_cloud/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar sparkuser@{aws_gluten_ip}:~/\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!{os.environ['SPARK_HOME']}/sbin/stop-slave.sh\n",
    "!{os.environ['SPARK_HOME']}/sbin/stop-master.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{os.environ['SPARK_HOME']}/sbin/start-master.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{os.environ['SPARK_HOME']}/sbin/start-worker.sh spark://{hostname}:7077 -c {cpu_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{os.environ['HADOOP_HOME']}/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{os.environ['SPARK_HOME']}/sbin/start-history-server.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone https://github.com/databricks/tpch-dbgen.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~/tpch-dbgen && git checkout 0469309147b42abac8857fa61b4cf69a6d3128a8 && make clean && make OS=LINUX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/gluten/backends-velox/workload/tpch/gen_data/parquet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify parameters\n",
    "print(scaleFactor)\n",
    "print(numPartitions)\n",
    "print(dataformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scala=f'''import com.databricks.spark.sql.perf.tpch._\n",
    "\n",
    "\n",
    "val scaleFactor = \"{scaleFactor}\" // scaleFactor defines the size of the dataset to generate (in GB).\n",
    "val numPartitions = {numPartitions}  // how many dsdgen partitions to run - number of input tasks.\n",
    "\n",
    "val format = \"{dataformat}\" // valid spark format like parquet \"parquet\".\n",
    "val rootDir = \"/tpch_sf1000_zstd\" // root directory of location to create data in.\n",
    "val dbgenDir = \"/home/{user}/tpch-dbgen\" // location of dbgen\n",
    "\n",
    "val tables = new TPCHTables(spark.sqlContext,\n",
    "    dbgenDir = dbgenDir,\n",
    "    scaleFactor = scaleFactor,\n",
    "    useDoubleForDecimal = false, // true to replace DecimalType with DoubleType\n",
    "    useStringForDate = false) // true to replace DateType with StringType\n",
    "\n",
    "\n",
    "tables.genData(\n",
    "    location = rootDir,\n",
    "    format = format,\n",
    "    overwrite = true, // overwrite the data that is already there\n",
    "    partitionTables = false, // do not create the partitioned fact tables\n",
    "    clusterByPartitionColumns = false, // shuffle to get partitions coalesced into single files.\n",
    "    filterOutNullPartitionValues = false, // true to filter out the partition with NULL key value\n",
    "    tableFilter = \"\", // \"\" means generate all tables\n",
    "    numPartitions = numPartitions) // how many dsdgen partitions to run - number of input tasks.\n",
    "'''\n",
    "\n",
    "with open(\"tpch_datagen_parquet.scala\",\"w\") as f:\n",
    "    f.writelines(scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify parameters\n",
    "print(num_executors)\n",
    "print(executor_memory)\n",
    "print(executor_cores)\n",
    "print(numPartitions)\n",
    "print(dataSourceCodec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpch_datagen_parquet=f'''\n",
    "cat tpch_datagen_parquet.scala | {os.environ['SPARK_HOME']}/bin/spark-shell \\\n",
    "  --num-executors {int(num_executors)} \\\n",
    "  --name tpch_gen_parquet \\\n",
    "  --executor-memory {int(executor_memory)}k \\\n",
    "  --executor-cores {int(executor_cores)} \\\n",
    "  --master spark://{hostname}:7077 \\\n",
    "  --driver-memory 10g \\\n",
    "  --conf spark.executor.memoryOverhead=1g \\\n",
    "  --conf spark.sql.broadcastTimeout=4800 \\\n",
    "  --conf spark.driver.maxResultSize=4g \\\n",
    "  --conf spark.sql.shuffle.partitions={numPartitions} \\\n",
    "  --conf spark.sql.parquet.compression.codec={dataSourceCodec} \\\n",
    "  --conf spark.network.timeout 800s\n",
    "  --conf spark.executor.heartbeatInterval 200s\n",
    "  --jars /home/{user}/ipython/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\n",
    "'''\n",
    "\n",
    "with open(\"tpch_datagen_parquet.sh\",\"w\") as f:\n",
    "    f.writelines(tpch_datagen_parquet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash tpch_datagen_parquet.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -du -h /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -du -h /tpch_sf1000_zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone https://github.com/databricks/tpcds-kit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~/tpcds-kit/tools && make clean && make OS=LINUX CC=gcc-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/gluten/backends-velox/workload/tpcds/gen_data/parquet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scala=f'''import com.databricks.spark.sql.perf.tpcds._\n",
    "\n",
    "\n",
    "val scaleFactor = \"{scaleFactor}\" // scaleFactor defines the size of the dataset to generate (in GB).\n",
    "val numPartitions = {numPartitions}  // how many dsdgen partitions to run - number of input tasks.\n",
    "\n",
    "val format = \"{dataformat}\" // valid spark format like parquet \"parquet\".\n",
    "val rootDir = \"/tpcds_sf1000_zstd\" // root directory of location to create data in.\n",
    "val dsdgenDir = \"/home/{user}/tpcds-kit/tools/\" // location of dbgen\n",
    "\n",
    "val tables = new TPCDSTables(spark.sqlContext,\n",
    "    dsdgenDir = dsdgenDir,\n",
    "    scaleFactor = scaleFactor,\n",
    "    useDoubleForDecimal = false, // true to replace DecimalType with DoubleType\n",
    "    useStringForDate = false) // true to replace DateType with StringType\n",
    "\n",
    "\n",
    "tables.genData(\n",
    "    location = rootDir,\n",
    "    format = format,\n",
    "    overwrite = true, // overwrite the data that is already there\n",
    "    partitionTables = true, // create the partitioned fact tables\n",
    "    clusterByPartitionColumns = true, // shuffle to get partitions coalesced into single files.\n",
    "    filterOutNullPartitionValues = false, // true to filter out the partition with NULL key value\n",
    "    tableFilter = \"\", // \"\" means generate all tables\n",
    "    numPartitions = numPartitions) // how many dsdgen partitions to run - number of input tasks.\n",
    "'''\n",
    "\n",
    "with open(\"tpcds_datagen_parquet.scala\",\"w\") as f:\n",
    "    f.writelines(scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpcds_datagen_parquet=f'''\n",
    "cat tpcds_datagen_parquet.scala | {os.environ['SPARK_HOME']}/bin/spark-shell \\\n",
    "  --num-executors {int(num_executors)} \\\n",
    "  --name tpch_gen_parquet \\\n",
    "  --executor-memory {int(executor_memory)}k \\\n",
    "  --executor-cores {int(executor_cores)} \\\n",
    "  --master spark://{hostname}:7077 \\\n",
    "  --driver-memory 10g \\\n",
    "  --conf spark.executor.memoryOverhead=1g \\\n",
    "  --conf spark.sql.broadcastTimeout=4800 \\\n",
    "  --conf spark.driver.maxResultSize=4g \\\n",
    "  --conf spark.sql.shuffle.partitions={numPartitions} \\\n",
    "  --conf spark.sql.parquet.compression.codec={dataSourceCodec} \\\n",
    "  --conf spark.network.timeout 800s\n",
    "  --conf spark.executor.heartbeatInterval 200s\n",
    "  --jars /home/{user}/ipython/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\n",
    "'''\n",
    "\n",
    "with open(\"tpcds_datagen_parquet.sh\",\"w\") as f:\n",
    "    f.writelines(tpcds_datagen_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash tpcds_datagen_parquet.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run TPC Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
