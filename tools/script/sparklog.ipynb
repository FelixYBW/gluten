{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413971f1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9581005b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0773c7f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T00:01:17.510155Z",
     "start_time": "2023-08-21T23:59:03.796927Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/lib/spark-3.2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155dbbaa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, DateType,\n",
    "    TimestampType, StringType, LongType, IntegerType, DoubleType,FloatType)\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0237b18c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import builtins\n",
    "from itertools import chain\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b295cda3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "import re\n",
    "import collections\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas\n",
    "import numpy\n",
    "import time\n",
    "from pandasql import sqldf\n",
    "import html\n",
    "\n",
    "pandas.options.display.max_rows=50\n",
    "pandas.options.display.max_columns=200\n",
    "pandas.options.display.float_format = '{:,}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c234e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e494b349",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib import colors\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.sans-serif'] =  'Courier New'\n",
    "rcParams['font.family'] = 'Courier New'\n",
    "rcParams['font.size'] = '12'\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display,HTML\n",
    "import threading\n",
    "import collections\n",
    "\n",
    "from IPython.display import display\n",
    "import time\n",
    "import threading\n",
    "import gzip\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, DateType,\n",
    "    TimestampType, StringType, LongType, IntegerType, DoubleType,FloatType)\n",
    "from pyspark.sql.functions import to_date, floor\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import lit\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "\n",
    "import re\n",
    "import math\n",
    "from functools import reduce\n",
    "import json\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a671cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = \"./jupyter_env/bin/python\"\n",
    "#os.environ['PYTHONPATH']=\"/usr/lib/spark-3.2/python/lib/py4j-0.10.9.2-src.zip:/usr/lib/spark-3.2/python/lib/pyspark.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba35bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ca32e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7b3aa5e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ddc091a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T00:02:04.250099Z",
     "start_time": "2023-08-22T00:02:04.215043Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    def __init__(self,file):\n",
    "        self.file=file\n",
    "        self.starttime=0\n",
    "        self.df=None\n",
    "    \n",
    "    def load_data(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_trace_view_list(self,id=0, **kwargs):\n",
    "        if self.df==None:\n",
    "            self.load_data()\n",
    "        trace_events=[]\n",
    "        node=kwargs.get('node',\"node\")\n",
    "        trace_events.append(json.dumps({\"name\": \"process_name\",\"ph\": \"M\",\"pid\":id,\"tid\":0,\"args\":{\"name\":\" \"+node}}))\n",
    "        return trace_events\n",
    "   \n",
    "    def generate_trace_view(self, trace_output, **kwargs):\n",
    "        traces=[]\n",
    "        traces.extend(self.generate_trace_view_list(0,**kwargs))\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ],\n",
    "            \"displayTimeUnit\": \"ns\"\n",
    "        }'''\n",
    "\n",
    "        if(\"home\" in trace_output):\n",
    "            outputfolder=trace_output\n",
    "            appidx=trace_output.split(\"/\")[-1]\n",
    "        else:\n",
    "            outputfolder='/home/yuzhou/trace_result/'+trace_output+'.json'\n",
    "            appidx=trace_output\n",
    "        with open(outputfolder, 'w') as outfile: \n",
    "            outfile.write(output)\n",
    "        \n",
    "        (print(\"<a href=http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+appidx+\".json>http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+appidx+\".json</a>\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6670bb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# sar analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "72dfb799",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def splits(x):\n",
    "    fi=[]\n",
    "    for l in x:\n",
    "        li=re.split(r'\\s+',l)\n",
    "        for j in range(len(li),118):\n",
    "            li.append('')\n",
    "        fi.append(li)\n",
    "    return iter(fi)\n",
    "\n",
    "class Sar_analysis(Analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def load_data(self):\n",
    "        sardata=sc.textFile(self.file)\n",
    "        sardf=sardata.mapPartitions(splits).toDF()\n",
    "        sardf=sardf.where(\"_1<>'Average:'\")\n",
    "        \n",
    "        colstart=1;\n",
    "        ampm=sardf.where(\"_2='AM' or _2='PM'\").count()\n",
    "        if ampm==0:\n",
    "            for i in range(len(sardf.columns),1,-1):\n",
    "                sardf=sardf.withColumnRenamed(f'_{i}',f'_{i+1}')\n",
    "            self.timeformat='yyyy-MM-dd HH:mm:ss '\n",
    "            sardf=sardf.withColumn('_2',F.lit(''))\n",
    "            #print('no PM/AM')\n",
    "            colstart=1\n",
    "        else:\n",
    "            self.timeformat='yyyy-MM-dd hh:mm:ss a'\n",
    "            colstart=2\n",
    "            #print('with PM/AM')\n",
    "        \n",
    "        f=fs.open(self.file)\n",
    "        t=f.readline()\n",
    "        t=f.readline()\n",
    "        while len(t)==1:\n",
    "            t=f.readline()\n",
    "        cols=t.decode('ascii')\n",
    "        li=re.split(r'\\s+',cols)\n",
    "        ci=3;\n",
    "        for c in li[colstart:]:\n",
    "            sardf=sardf.withColumnRenamed(f\"_{ci}\",c)\n",
    "            ci=ci+1\n",
    "            \n",
    "        sardf=sardf.where(F.col(li[-2])!=li[-2]).where(F.col(\"_1\")!=F.lit(\"Linux\"))        \n",
    "        \n",
    "        sardf.cache()\n",
    "        self.df=sardf\n",
    "        \n",
    "        self.sarversion=\"\"\n",
    "        paths=os.path.split(self.file)\n",
    "        if fs.exists(paths[0]+\"/sarv.txt\"):\n",
    "            with fs.open(paths[0]+\"/sarv.txt\") as f:\n",
    "                allcnt = f.read().decode('ascii')\n",
    "                #print(allcnt)\n",
    "                self.sarversion=allcnt.split(\"\\n\")[0].split(\" \")[2]\n",
    "        \n",
    "        return sardf\n",
    "\n",
    "    def col_df(self,cond,colname,args,slaver_id=0, thread_id=0):\n",
    "        sardf=self.df\n",
    "        starttime=self.starttime\n",
    "        cpudf=sardf.where(cond)\n",
    "        #cpudf.select(F.date_format(F.from_unixtime(F.lit(starttime/1000)), 'yyyy-MM-dd HH:mm:ss').alias('starttime'),'_1').show(1)\n",
    "\n",
    "        cpudf=cpudf.withColumn('time',F.unix_timestamp(F.concat_ws(' ',F.date_format(F.from_unixtime(F.lit(starttime/1000)), 'yyyy-MM-dd'),F.col('_1'),F.col('_2')),self.timeformat))\n",
    "\n",
    "        cols=cpudf.columns\n",
    "                \n",
    "        cpudf=cpudf.groupBy('time').agg(\n",
    "            F.sum(F.when(F.col(cols[1]).rlike('^\\d+(\\.\\d+)*$'),F.col(cols[1]).astype(FloatType())).otherwise(0)).alias(cols[1]),\n",
    "            F.sum(F.when(F.col(cols[2]).rlike('^\\d+(\\.\\d+)*$'),F.col(cols[2]).astype(FloatType())).otherwise(0)).alias(cols[2]),\n",
    "            *[F.sum(F.col(c)).alias(c) for c in cols[3:] if not c.startswith(\"_\") and c!=\"\" and c!=\"time\"]\n",
    "        )\n",
    "        \n",
    "        traces=cpudf.orderBy(F.col(\"time\")).select(\n",
    "                F.lit(thread_id).alias('tid'),\n",
    "                (F.expr(\"time*1000\")-F.lit(self.starttime)).astype(IntegerType()).alias('ts'),\n",
    "                F.lit(slaver_id).alias('pid'),\n",
    "                F.lit('C').alias('ph'),\n",
    "                F.lit(colname).alias('name'),\n",
    "                args(cpudf).alias('args')\n",
    "            ).toJSON().collect()\n",
    "        return traces\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        return trace_events\n",
    "\n",
    "    def get_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        starttime=kwargs.get(\"starttime\")\n",
    "        endtime=kwargs.get(\"endtime\")\n",
    "        print(starttime,endtime,self.file)\n",
    "        if starttime >0 and endtime >0:\n",
    "            starttime=int(starttime/1000)\n",
    "            endtime=int(endtime/1000)\n",
    "            self.df=self.df.withColumn('rtime',F.unix_timestamp(F.concat_ws(' ',F.date_format(F.from_unixtime(F.lit(starttime)), 'yyyy-MM-dd'),F.col('_1'),F.col('_2')),self.timeformat))\n",
    "            dfx=self.df.where(f\"rtime between {starttime} and {endtime}\")\n",
    "            if dfx.count()==0:\n",
    "                print(\"no profile in the period: \", starttime,\": \",endtime,\". \")\n",
    "            else:\n",
    "                self.df=self.df.where(f\"rtime between {starttime} and {endtime}\")        \n",
    "            \n",
    "class Sar_cpu_analysis(Sar_analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Sar_analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Sar_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        self.df=self.df.withColumn(\"%iowait\",F.when(F.col(\"%iowait\")>100,F.lit(100)).otherwise(F.col(\"%iowait\")))\n",
    "        \n",
    "        trace_events.extend(self.col_df(\"CPU='all'\",             \"all cpu%\",    lambda l: F.struct(\n",
    "                                                                                                    F.floor(F.col('%user').astype(FloatType())).alias('user'),\n",
    "                                                                                                    F.floor(F.col('%system').astype(FloatType())).alias('system'),\n",
    "                                                                                                    F.floor(F.col('%iowait').astype(FloatType())).alias('iowait')\n",
    "                                                                                                    ),                            id, 0))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":0,\"args\":{\"sort_index \":0}}))\n",
    "        \n",
    "        return trace_events    \n",
    "    def get_stat(sar_cpu,**kwargs):\n",
    "        Sar_analysis.get_stat(sar_cpu,**kwargs)\n",
    "        \n",
    "        cpuutil=sar_cpu.df.where(\"CPU='all'\").groupBy(\"_1\").agg(*[F.mean(F.col(l).astype(FloatType())).alias(l) for l in [\"%user\",\"%system\",\"%iowait\"]]).orderBy(\"_1\")\n",
    "        cnt=cpuutil.count()\n",
    "        user_morethan_90=cpuutil.where(\"`%user`>0.9\").count()\n",
    "        kernel_morethan_10=cpuutil.where(\"`%system`>0.1\").count()\n",
    "        iowait_morethan_10=cpuutil.where(\"`%iowait`>0.1\").count()\n",
    "        out=[['%user>90%',user_morethan_90/cnt],['%kernel>10%',kernel_morethan_10/cnt],[\"%iowait>10%\",iowait_morethan_10/cnt]]\n",
    "        avgutil=cpuutil.agg(*[F.mean(l).alias(l) for l in [\"%user\",\"%system\",\"%iowait\"]]).collect()\n",
    "        out.extend([[\"avg \" + l,avgutil[0][l]] for l in [\"%user\",\"%system\",\"%iowait\"]])\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_cpu.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "class Sar_mem_analysis(Sar_analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Sar_analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def load_data(self):\n",
    "        Sar_analysis.load_data(self)\n",
    "        sarv=[int(l) for l in self.sarversion.split(\".\")]\n",
    "        if sarv[0]>=12 and sarv[1]>=2:\n",
    "            self.df=self.df.withColumn(\"kbrealused\",F.col(\"kbmemused\"))\n",
    "        else:\n",
    "            # sar 10.1.5, sar 11.6.1\n",
    "            self.df=self.df.withColumn(\"kbrealused\",F.col(\"kbmemused\")-F.col(\"kbcached\")-F.col(\"kbbuffers\"))\n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Sar_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        \n",
    "        trace_events.extend(self.col_df(F.col('kbmemfree').rlike('^\\d+$'),\"mem % \",      lambda l: F.struct(F.floor(l['kbcached']*l['%memused']/l['kbmemused']).alias('cached'),  # kbcached / (kbmemfree+kbmemused)\n",
    "                                                                                                       F.floor(l['kbbuffers']*l['%memused']/l['kbmemused']).alias('buffered'),# kbbuffers / (kbmemfree+kbmemused)\n",
    "                                                                                                       F.floor(l['kbrealused']*l['%memused']/l['kbmemused']).alias('used')), # (%memused- kbcached-kbbuffers )/  (kbmemfree+kbmemused)\n",
    "                                          id,1))\n",
    "        #trace_events.extend(self.col_df(self.df._3.rlike('^\\d+$'),\"mem cmt % \",  lambda l: F.struct(F.floor(l._8*F.lit(100)/(l._3+l._4)).alias('commit/phy'),\n",
    "        #                                                                                                   F.floor(l._10-l._8*F.lit(100)/(l._3+l._4)).alias('commit/all')),                                                             id))\n",
    "        trace_events.extend(self.col_df(F.col('kbmemfree').rlike('^\\d+$'),\"pagecache % \",      lambda l: F.struct(F.floor((l['kbcached']-l['kbdirty'])*l['%memused']/l['kbmemused']).alias('clean'), \n",
    "                                                                                                       F.floor(l['kbdirty']*l['%memused']/l['kbmemused']).alias('dirty')),\n",
    "                                          id,2))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":1,\"args\":{\"sort_index \":1}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":2,\"args\":{\"sort_index \":2}}))\n",
    "        return trace_events    \n",
    "    def get_stat(sar_mem,**kwargs):\n",
    "        Sar_analysis.get_stat(sar_mem,**kwargs)\n",
    "        \n",
    "        memutil=sar_mem.df.where(F.col('kbmemfree').rlike('^\\d+$')).select(F.floor(F.col('kbcached').astype(FloatType())/1024/1024).alias('cached'),  \n",
    "                                                                                   F.floor(F.col('kbbuffers').astype(FloatType())/1024/1024).alias('buffered'),\n",
    "                                                                                   F.floor(F.col('kbrealused').astype(FloatType())/1024/1024).alias('used'),\n",
    "                                                                                   F.floor(F.col('kbdirty').astype(FloatType())/1024/1024).alias('dirty'))\n",
    "        memsum=memutil.summary().toPandas()\n",
    "        memsum=memsum.set_index(\"summary\")\n",
    "        out=[\n",
    "            [[l + ' mean',float(memsum[l][\"mean\"])],\n",
    "            [l + ' 75%',float(memsum[l][\"75%\"])],\n",
    "            [l + ' max',float(memsum[l][\"max\"])]] for l in [\"cached\",\"used\",\"dirty\"]]\n",
    "        out=[*out[0],*out[1],*out[2]]\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_mem.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "class Sar_PageCache_analysis(Sar_analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Sar_analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def load_data(self):\n",
    "        Sar_analysis.load_data(self)\n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Sar_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        \n",
    "        trace_events.extend(self.col_df(F.col('pgpgin/s').rlike('^\\d'),\"page inout\",      lambda l: F.struct(\n",
    "                                                                                                       F.floor(l['pgpgin/s']/1024).alias('in'),\n",
    "                                                                                                       F.floor(l['pgpgout/s']/1024).alias('out')),\n",
    "                                          id,11))\n",
    "        trace_events.extend(self.col_df(F.col('pgpgin/s').rlike('^\\d'),\"faults\",      lambda l: F.struct(F.floor((l['majflt/s'])).alias('major'), \n",
    "                                                                                                       F.floor(l['fault/s']-l['majflt/s']).alias('minor')),\n",
    "                                          id,12))\n",
    "        trace_events.extend(self.col_df(F.col('pgpgin/s').rlike('^\\d'),\"page free\",      lambda l: F.struct(F.floor((l['pgfree/s']*4/1024)).alias('free')),\n",
    "                                          id,13))\n",
    "        trace_events.extend(self.col_df(F.col('pgpgin/s').rlike('^\\d'),\"scan\",      lambda l: F.struct(F.floor((l['pgscank/s'])*4/1024).alias('kernel'), \n",
    "                                                                                                       F.floor(l['pgscand/s']*4/1024).alias('app')),\n",
    "                                          id,14))\n",
    "        trace_events.extend(self.col_df(F.col('pgpgin/s').rlike('^\\d'),\"vmeff\",      lambda l: F.struct(F.floor((l['%vmeff'])).alias('steal')),\n",
    "                                          id,15))\n",
    "        \n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":11,\"args\":{\"sort_index \":11}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":12,\"args\":{\"sort_index \":12}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":13,\"args\":{\"sort_index \":13}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":14,\"args\":{\"sort_index \":14}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":15,\"args\":{\"sort_index \":15}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":16,\"args\":{\"sort_index \":16}}))\n",
    "        return trace_events    \n",
    "    def get_stat(sar_mem,**kwargs):\n",
    "        Sar_analysis.get_stat(sar_mem,**kwargs)\n",
    "        \n",
    "        memutil=sar_mem.df.where(F.col('pgpgin/s').rlike('^\\d')).select(F.floor(F.col('pgpgin/s').astype(FloatType())/1024).alias('pgin'),  \n",
    "                                                                                   F.floor(F.col('pgpgout/s').astype(FloatType())/1024).alias('pgout'),\n",
    "                                                                                   F.floor(F.col('fault/s').astype(FloatType())-F.col('majflt/s').astype(FloatType())).alias('fault')\n",
    "                                                                                   )\n",
    "        memsum=memutil.summary().toPandas()\n",
    "        memsum=memsum.set_index(\"summary\")\n",
    "        out=[\n",
    "            [[l + ' mean',float(memsum[l][\"mean\"])],\n",
    "            [l + ' 75%',float(memsum[l][\"75%\"])],\n",
    "            [l + ' max',float(memsum[l][\"max\"])]] for l in [\"pgin\",\"pgout\",\"fault\"]]\n",
    "        out=[*out[0],*out[1],*out[2]]\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_mem.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "    \n",
    "class Sar_disk_analysis(Sar_analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Sar_analysis.__init__(self,sar_file)\n",
    "        \n",
    "    def load_data(self):\n",
    "        Sar_analysis.load_data(self)\n",
    "        \n",
    "        self.df=self.df.withColumn(\"%util\",F.col(\"%util\").astype(IntegerType()))\n",
    "        #used_disk=self.df.groupBy(\"DEV\").agg(F.max(F.col(\"%util\")).alias(\"max_util\"),F.mean(\"%util\").alias(\"avg_util\")).where(F.col(\"max_util\")>10).collect()\n",
    "        #self.df=self.df.where(F.col(\"DEV\").isin([l['DEV'] for l in used_disk]))\n",
    "        #print(\"used disks with its max util% and avg util% are: \")\n",
    "        #display([(l['DEV'],l[\"max_util\"],l[\"avg_util\"]) for l in used_disk])\n",
    "        \n",
    "        if \"rd_sec/s\" in self.df.columns:\n",
    "            self.df=self.df.withColumn(\"rkB/s\",F.expr('cast(`rd_sec/s` as float)*512/1024'))\n",
    "        if \"wr_sec/s\" in self.df.columns:\n",
    "            self.df=self.df.withColumn(\"wkB/s\",F.expr('cast(`wr_sec/s` as float)*512/1024'))\n",
    "        \n",
    "        if \"areq-sz\" in self.df.columns:\n",
    "            self.df=self.df.withColumnRenamed(\"areq-sz\",\"avgrq-sz\")\n",
    "        if \"aqu-sz\" in self.df.columns:\n",
    "            self.df=self.df.withColumnRenamed(\"aqu-sz\",\"avgqu-sz\")\n",
    "            \n",
    "        if \"rkB/s\" in self.df.columns:\n",
    "            self.df=self.df.withColumn(\"rkB/s\",F.expr('cast(`rkB/s` as float)/1024'))\n",
    "        if \"wkB/s\" in self.df.columns:\n",
    "            self.df=self.df.withColumn(\"wkB/s\",F.expr('cast(`wkB/s` as float)/1024'))\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Sar_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "\n",
    "        disk_prefix=kwargs.get('disk_prefix',\"\")\n",
    "        \n",
    "        if type(disk_prefix)==str:\n",
    "            diskfilter = \"DEV like '\"+disk_prefix+\"%'\"\n",
    "        elif type(disk_prefix)==list:\n",
    "            diskfilter = \"DEV in (\"+\",\".join(disk_prefix)+\")\"\n",
    "        else:\n",
    "            diskfilter = \"DEV like '%'\"\n",
    "\n",
    "        print(diskfilter)\n",
    "        devcnt=self.df.where(diskfilter).select(\"DEV\").distinct().count()\n",
    "        \n",
    "        trace_events.extend(self.col_df(diskfilter,      \"disk b/w\",       lambda l: F.struct(\n",
    "                                                                                                            F.floor(F.col(\"rKB/s\")).alias('read'),\n",
    "                                                                                                            F.floor(F.col(\"wKB/s\")).alias('write')),id, 3))\n",
    "        trace_events.extend(self.col_df(diskfilter,      \"disk%\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"%util\")/F.lit(devcnt)).alias('%util')),id, 4))\n",
    "        trace_events.extend(self.col_df(diskfilter,      \"req size\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"avgrq-sz\")/F.lit(devcnt)).alias('avgrq-sz')),id, 5))\n",
    "        trace_events.extend(self.col_df(diskfilter,      \"queue size\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"avgqu-sz\")/F.lit(512*devcnt/1024)).alias('avgqu-sz')),id, 6))\n",
    "        trace_events.extend(self.col_df(diskfilter,      \"await\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"await\")/F.lit(devcnt)).alias('await')),id,7))\n",
    "        \n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":3,\"args\":{\"sort_index \":3}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":4,\"args\":{\"sort_index \":4}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":5,\"args\":{\"sort_index \":5}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":6,\"args\":{\"sort_index \":6}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":7,\"args\":{\"sort_index \":7}}))\n",
    "        return trace_events    \n",
    "\n",
    "    def get_stat(sar_disk,**kwargs):\n",
    "        Sar_analysis.get_stat(sar_disk,**kwargs)\n",
    "        disk_prefix=kwargs.get('disk_prefix',\"\")\n",
    "        \n",
    "        if type(disk_prefix)==str:\n",
    "            diskfilter = \"DEV like '\"+disk_prefix+\"%'\"\n",
    "        elif type(disk_prefix)==list:\n",
    "            diskfilter = \"DEV in (\"+\",\".join(disk_prefix)+\")\"\n",
    "        else:\n",
    "            diskfilter = \"DEV like '%'\"\n",
    "\n",
    "        diskutil=sar_disk.df.where(diskfilter).groupBy(\"_1\").agg(F.mean(F.col(\"%util\").astype(FloatType())).alias(\"%util\")).orderBy(\"_1\")\n",
    "        totalcnt=diskutil.count()\n",
    "        time_morethan_90=diskutil.where(F.col(\"%util\")>90).count()/totalcnt\n",
    "        avgutil=diskutil.agg(F.mean(\"%util\")).collect()\n",
    "        out=[[\"avg disk util\",avgutil[0][\"avg(%util)\"]],\n",
    "            [\"time more than 90%\", time_morethan_90]]\n",
    "        diskbw=sar_disk.df.where(diskfilter).groupBy(\"_1\").agg(F.sum(F.col(\"rKB/s\")).alias(\"rd_bw\"),F.sum(F.col(\"wKB/s\")).alias(\"wr_bw\"))\n",
    "        bw=diskbw.agg(F.sum(\"rd_bw\").alias(\"total read\"),F.sum(\"wr_bw\").alias(\"total write\"),F.mean(\"rd_bw\").alias(\"read bw\"),F.mean(\"wr_bw\").alias(\"write bw\"),F.max(\"rd_bw\").alias(\"max read\"),F.max(\"wr_bw\").alias(\"max write\")).collect()\n",
    "        maxread=bw[0][\"max read\"]\n",
    "        maxwrite=bw[0][\"max write\"]\n",
    "        rdstat, wrstat = diskbw.stat.approxQuantile(['rd_bw','wr_bw'],[0.75,0.95,0.99],0.0)\n",
    "        time_rd_morethan_95 = diskbw.where(F.col(\"rd_bw\")>rdstat[1]).count()/totalcnt\n",
    "        time_wr_morethan_95 = diskbw.where(F.col(\"wr_bw\")>rdstat[1]).count()/totalcnt\n",
    "        out.append(['total read (G)' , bw[0][\"total read\"]/1024])\n",
    "        out.append(['total write (G)', bw[0][\"total write\"]/1024])\n",
    "        out.append(['avg read bw (MB/s)', bw[0][\"read bw\"]])\n",
    "        out.append(['avg write bw (MB/s)', bw[0][\"write bw\"]])\n",
    "        out.append(['read bw %75', rdstat[0]])\n",
    "        out.append(['read bw %95', rdstat[1]])\n",
    "        out.append(['read bw max', rdstat[2]])\n",
    "        out.append(['time_rd_morethan_95', time_rd_morethan_95])\n",
    "        out.append(['write bw %75', wrstat[0]])\n",
    "        out.append(['write bw %95', wrstat[1]])\n",
    "        out.append(['write bw max', wrstat[2]])\n",
    "        out.append(['time_wr_morethan_95', time_wr_morethan_95])\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_disk.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "class Sar_nic_analysis(Sar_analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Sar_analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Sar_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        nicfilter=\"\"\n",
    "        if 'nic_prefix' in kwargs.keys():\n",
    "            nicfilter= \"IFACE in (\" + \",\".join(kwargs.get('nic_prefix',[\"'eth3'\",\"'enp24s0f1'\"])) + \")\"\n",
    "        else:\n",
    "            nicfilter= \"IFACE != 'lo'\"\n",
    "        \n",
    "        trace_events.extend(self.col_df(nicfilter,       \"eth \",        lambda l: F.struct(F.floor(F.expr('cast(`rxkB/s` as float)/1024')).alias('rxmb/s'),F.floor(F.expr('cast(`txkB/s` as float)/1024')).alias('txmb/s')),                id, 8))\n",
    "        trace_events.extend(self.col_df(\"_3 like 'ib%'\",        \"ib \",        lambda l: F.struct(F.floor(F.expr('cast(`rxkB/s` as float)/1024')).alias('rxmb/s'),F.floor(F.expr('cast(`txkB/s` as float)/1024')).alias('txmb/s')),                id, 9))\n",
    "        trace_events.extend(self.col_df(\"_3 = 'lo'\",            \"lo \",         lambda l: F.struct(F.floor(F.expr('cast (`rxkB/s` as float)/1024')).alias('rxmb/s'),F.floor(F.expr('cast (`txkB/s` as float)/1024')).alias('txmb/s')),              id, 10))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":8,\"args\":{\"sort_index \":8}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":9,\"args\":{\"sort_index \":9}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":10,\"args\":{\"sort_index \":10}}))\n",
    "        return trace_events  \n",
    "    \n",
    "    def get_stat(sar_nic,**kwargs):\n",
    "        Sar_analysis.get_stat(sar_nic,**kwargs)\n",
    "        nicfilter=\"\"\n",
    "        \n",
    "        if 'nic_prefix' in kwargs.keys():\n",
    "            nicfilter= \"IFACE in (\" + \",\".join(kwargs.get('nic_prefix',[\"'eth3'\",\"'enp24s0f1'\"])) + \")\"\n",
    "        else:\n",
    "            nicfilter= \"IFACE != 'lo'\"\n",
    "            \n",
    "        nicbw=sar_nic.df.where(nicfilter).groupBy(\"_1\").agg(F.sum(F.col(\"rxkB/s\").astype(FloatType())/1024).alias(\"rx MB/s\")).orderBy(\"_1\")\n",
    "        if nicbw.count()==0:\n",
    "            out=[[\"rx MB/s 75%\",0],[\"rx MB/s 95%\",0],[\"rx MB/s 99%\",0]]\n",
    "        else:\n",
    "            out=nicbw.stat.approxQuantile(['rx MB/s'],[0.75,0.95,0.99],0.0)[0]\n",
    "            out=[[\"rx MB/s 75%\",out[0]],[\"rx MB/s 95%\",out[1]],[\"rx MB/s 99%\",out[2]]]\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_nic.file.split(\"/\")[-2]]\n",
    "        return pdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f7f85",
   "metadata": {},
   "source": [
    "# App_Log_Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f825594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_gradient(s, m, M, cmap='PuBu', low=0, high=0):\n",
    "    from matplotlib import colors\n",
    "    rng = M - m\n",
    "    norm = colors.Normalize(m - (rng * low),\n",
    "                            M + (rng * high))\n",
    "    normed = norm(s.values)\n",
    "    c = [colors.rgb2hex(x) for x in plt.cm.get_cmap(cmap)(normed)]\n",
    "    return ['background-color: {:s}'.format(color) for color in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0af62ff5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T00:02:16.135410Z",
     "start_time": "2023-08-22T00:02:15.309228Z"
    },
    "code_folding": [
     39,
     45,
     203,
     419,
     456,
     569,
     725,
     891,
     969,
     1069,
     1103,
     1145,
     1151,
     1162,
     1258,
     1263,
     1271,
     1276,
     1356,
     1363,
     1364,
     1387,
     1610,
     1667,
     1682,
     1719,
     1752,
     1786
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "@udf(\"long\")\n",
    "def isfinish_udf(s):\n",
    "    import json\n",
    "    s=json.loads(s)\n",
    "    def isfinish(root):\n",
    "        if \"isFinalPlan=false\" in root['simpleString'] or root['children'] is None:\n",
    "            return 0\n",
    "        for c in root[\"children\"]:\n",
    "            if isfinish(c)==0:\n",
    "                return 0\n",
    "        return 1\n",
    "    if len(s)>0:\n",
    "        return isfinish(s[0])\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "@pandas_udf(\"taskid long, start long, dur long, name string\", PandasUDFType.GROUPED_MAP)\n",
    "def time_breakdown(pdf):\n",
    "    ltime=pdf['Launch Time'][0]+2\n",
    "    pdf['start']=0\n",
    "    pdf['dur']=0\n",
    "    outpdf=[]\n",
    "    ratio=(pdf[\"Finish Time\"][0]-pdf[\"Launch Time\"][0])/pdf[\"Update\"].sum()\n",
    "    ratio=1 if ratio>1 else ratio\n",
    "    for idx,l in pdf.iterrows():\n",
    "        if(l[\"Update\"]*ratio>1):\n",
    "            outpdf.append([l[\"Task ID\"],ltime,int(l[\"Update\"]*ratio),l[\"mname\"]])\n",
    "            ltime=ltime+int(l[\"Update\"]*ratio)\n",
    "    if len(outpdf)>0:\n",
    "        return pandas.DataFrame(outpdf)\n",
    "    else:\n",
    "        return pandas.DataFrame({'taskid': pandas.Series([], dtype='long'),\n",
    "                   'start': pandas.Series([], dtype='long'),\n",
    "                   'dur': pandas.Series([], dtype='long'),\n",
    "                   'name': pandas.Series([], dtype='str'),\n",
    "                                })\n",
    "    \n",
    "class App_Log_Analysis(Analysis):\n",
    "    def __init__(self, file, jobids):\n",
    "        Analysis.__init__(self,file)\n",
    "        self.jobids=[] if jobids is None else [str(l) for l in jobids]\n",
    "        self.df=None\n",
    "        self.pids=[]\n",
    "        \n",
    "    def load_data(self):\n",
    "        print(\"load data \", self.file)\n",
    "        jobids=self.jobids\n",
    "        df=spark.read.json(self.file)\n",
    "        \n",
    "        if 'App ID' in df.columns:\n",
    "            self.appid=df.where(\"`App ID` is not null\").collect()[0][\"App ID\"]\n",
    "        else:\n",
    "            self.appid=\"Application-00000000\"\n",
    "                \n",
    "        if df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates'\").count()>0:\n",
    "            self.dfacc=df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates'\").select(F.col(\"executionId\").alias(\"queryid\"),F.explode(\"accumUpdates\"))\n",
    "        else:\n",
    "            self.dfacc = None\n",
    "            \n",
    "        if \"sparkPlanInfo\" in df.columns:\n",
    "            self.queryplans=df.where(\"(Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart' or Event='org.apache.spark.sql.execution.ui.SparkListenerSQLAdaptiveExecutionUpdate') \\\n",
    "                                  and (sparkPlanInfo.nodeName!='AdaptiveSparkPlan' or sparkPlanInfo.simpleString='AdaptiveSparkPlan isFinalPlan=true') \").select(F.col(\"executionId\").alias(\"queryid\"),'physicalPlanDescription',\"sparkPlanInfo.*\")\n",
    "        else:\n",
    "            self.queryplans=None\n",
    "        \n",
    "        seen = set()\n",
    "        \n",
    "        if self.queryplans is not None:\n",
    "            self.queryplans=self.queryplans.where(isfinish_udf(F.to_json(\"children\"))==1)\n",
    "        \n",
    "            self.allmetrics=[]\n",
    "            if self.queryplans.count() > 0:\n",
    "                metrics=self.queryplans.collect()\n",
    "                def get_metric(root):\n",
    "                    for l in root[\"metrics\"]:\n",
    "                        if l['accumulatorId'] not in seen:\n",
    "                            seen.add(l['accumulatorId'])\n",
    "                            self.allmetrics.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"]])\n",
    "                    if root['children'] is not None:\n",
    "                        for c in root[\"children\"]:\n",
    "                            get_metric(c)\n",
    "                for c in metrics:\n",
    "                    get_metric(c)\n",
    "        \n",
    "            amsdf=spark.createDataFrame(self.allmetrics)\n",
    "            amsdf=amsdf.withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"type\").withColumnRenamed(\"_3\",\"Name\").withColumnRenamed(\"_4\",\"nodeName\")\n",
    "        \n",
    "        \n",
    "        if self.dfacc is not None:\n",
    "            self.dfacc=self.dfacc.select(\"queryid\",(F.col(\"col\")[0]).alias(\"ID\"),(F.col(\"col\")[1]).alias(\"Update\")).join(amsdf,on=[\"ID\"])\n",
    "        \n",
    "        if self.queryplans is not None:\n",
    "            self.metricscollect=[l for l in self.allmetrics if l[1] in ['nsTiming','timing'] and (l[2].startswith(\"time of \") or l[2].startswith(\"time to \") or l[2].startswith(\"scan time\") or l[2].startswith(\"shuffle write time\") or l[2].startswith(\"time to spill\") or l[2].startswith(\"task commit time\")) \n",
    "                                 and l[2] not in(\"totaltime to collect batch\", \"time of scan\", \"time of input iterator\") ]\n",
    "        \n",
    "        #config=df.where(\"event='SparkListenerJobStart' and Properties.`spark.executor.cores` is not null\").select(\"Properties.*\").limit(1).collect()\n",
    "        config=df.select(\"`Spark Properties`.*\").where(\"`spark.app.id` is not null\").limit(1).collect()\n",
    "    \n",
    "        configdic=config[0].asDict()\n",
    "        self.parallelism=int(configdic['spark.sql.shuffle.partitions']) if 'spark.sql.shuffle.partitions' in configdic else 1\n",
    "        self.executor_cores=int(configdic['spark.executor.cores']) if 'spark.executor.cores' in configdic else 1\n",
    "        self.executor_instances=int(configdic['spark.executor.instances']) if 'spark.executor.instances' in configdic else 1\n",
    "        self.taskcpus= int(configdic['spark.task.cpus'])if 'spark.task.cpus' in configdic else 1\n",
    "        self.batchsize= int(configdic['spark.gluten.sql.columnar.maxBatchSize'])if 'spark.gluten.sql.columnar.maxBatchSize' in configdic else 4096\n",
    "        \n",
    "        self.realexecutors = df.where(~F.isnull(F.col(\"Executor ID\"))).select(\"Executor ID\").distinct().count()\n",
    "        \n",
    "        execstart = df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart'\").select(\"executionId\",\"time\")\n",
    "        execend = df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd'\").select(\"executionId\",\"time\")\n",
    "        execstart=execstart.withColumnRenamed(\"time\",\"query_starttime\").withColumnRenamed(\"executionId\",\"queryid\")\n",
    "        execend=execend.withColumnRenamed(\"time\",\"query_endtime\").withColumnRenamed(\"executionId\",\"queryid\")\n",
    "        exectime = execstart.join(execend,on=[\"queryid\"])\n",
    "\n",
    "        if \"spark.sql.execution.id\" in df.where(\"Event='SparkListenerJobStart'\").select(\"Properties.*\").columns:\n",
    "            df_jobstart=df.where(\"Event='SparkListenerJobStart'\").select(\"Job ID\",\"Submission Time\",F.col(\"Properties.`spark.sql.execution.id`\").alias(\"queryid\"),\"Stage IDs\")\n",
    "        else:\n",
    "            df_jobstart=df.where(\"Event='SparkListenerJobStart'\").select(\"Job ID\",\"Submission Time\",F.lit(0).alias(\"queryid\"),\"Stage IDs\")\n",
    "        \n",
    "        df_jobend=df.where(\"Event='SparkListenerJobEnd'\").select(\"`Job ID`\",\"Completion Time\")\n",
    "        df_job=df_jobstart.join(df_jobend,\"Job ID\")\n",
    "        df_job=df_job.withColumnRenamed(\"Submission Time\",\"job_start_time\")\n",
    "        df_job=df_job.withColumnRenamed(\"Completion Time\",\"job_stop_time\")\n",
    "        self.df_job=df_job\n",
    "        \n",
    "        jobstage=df_job.select(\"*\",F.explode(\"Stage IDs\").alias(\"Stage ID\"))\n",
    "        task=df.where(\"(Event='SparkListenerTaskEnd' or Event='SparkListenerTaskStart') \").select(\"Event\",\"Stage ID\",\"task info.*\",\"task metrics.*\")\n",
    "        \n",
    "        self.failed_stages = [str(l['Stage ID']) for l in task.where(\"Failed='true'\").select(\"Stage ID\").distinct().collect()]\n",
    "        \n",
    "        self.speculativetask = task.where(\"speculative = 'true'\").count()\n",
    "        self.speculativekilledtask = task.where(\"speculative = true and killed='true'\").count()\n",
    "        self.speculativestage = task.where(\"speculative = true and killed='true'\").select(\"`Stage ID`\").distinct().count()\n",
    "        \n",
    "        validtsk = task.where(\"Event = 'SparkListenerTaskEnd' and (Failed<>'true' or killed<>'true')\").select(\"`Task ID`\")\n",
    "        task=task.join(validtsk,on='Task ID',how='inner')\n",
    "        \n",
    "        taskjob=task.\\\n",
    "            select(\"Host\",\"`Event`\",\"`Launch Time`\",\"`Executor ID`\",\"`Task ID`\",\"`Finish Time`\",\n",
    "                    \"`Stage ID`\",\"`Input Metrics`.`Bytes Read`\",\"`Disk Bytes Spilled`\",\"`Memory Bytes Spilled`\",\"`Shuffle Read Metrics`.`Local Bytes Read`\",\"`Shuffle Read Metrics`.`Remote Bytes Read`\",\n",
    "                   \"`Shuffle Write Metrics`.`Shuffle Bytes Written`\",\"`Executor Deserialize Time`\",\"`Shuffle Read Metrics`.`Fetch Wait Time`\",\"`Executor Run Time`\",\"`Shuffle Write Metrics`.`Shuffle Write Time`\",\n",
    "                   \"`Result Serialization Time`\",\"`Getting Result Time`\",\"`JVM GC Time`\",\"`Executor CPU Time`\",\"Accumulables\",\"Peak Execution Memory\",\n",
    "                    F.when(task['Finish Time']==0,task['Launch Time']).otherwise(task['Finish Time']).alias('eventtime')\n",
    "        ).join(jobstage,\"Stage ID\").where(\"`Finish Time` is null or `Finish Time` <=job_stop_time+5\")\n",
    "        \n",
    "        taskjob = taskjob.join(exectime,on=['queryid'],how='left')\n",
    "        \n",
    "        self.df=taskjob\n",
    "        \n",
    "        if len(jobids)>0:\n",
    "            self.df=self.df.where('`Job ID` in ({:s})'.format(','.join(jobids)))\n",
    "        \n",
    "        queryids=self.df.select(F.col(\"queryid\").astype(IntegerType())).distinct().where(\"queryid is not null\").orderBy(\"queryid\").toPandas()\n",
    "        \n",
    "        self.query_num=len(queryids)\n",
    "        if self.query_num>0:\n",
    "            queryidx=queryids.reset_index()\n",
    "            queryidx['index']=queryidx['index']+1\n",
    "            #tpcds query\n",
    "            if self.query_num==103:\n",
    "                queryidx['index']=queryidx['index'].map(tpcds_query_map)\n",
    "            qidx=spark.createDataFrame(queryidx)\n",
    "            qidx=qidx.withColumnRenamed(\"index\",\"real_queryid\")\n",
    "            self.df=self.df.join(qidx,on=\"queryid\",how=\"left\")\n",
    "            if self.dfacc is not None:\n",
    "                self.dfacc=self.dfacc.join(qidx,on=\"queryid\",how='left')\n",
    "\n",
    "            if self.queryplans:\n",
    "                self.queryplans=self.queryplans.join(qidx,\"queryid\",how=\"right\")\n",
    "        \n",
    "        self.df=self.df.fillna(0)\n",
    "        self.df=self.df.withColumn('Executor ID',F.when(F.col(\"Executor ID\")==\"driver\",1).otherwise(F.col(\"Executor ID\")))\n",
    "        self.df.cache()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##############################\n",
    "        \n",
    "        dfx=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"Stage ID\",\"Launch Time\",\"Finish Time\",\"Task ID\")\n",
    "        dfxpds=dfx.toPandas()\n",
    "        dfxpds.columns=[l.replace(\" \",\"_\") for l in dfxpds.columns]\n",
    "        #dfxpds_ods=sqldf('''select * from dfxpds order by finish_time desc''')\n",
    "        dfxpds_ods=dfxpds.sort_values(by=\"Finish_Time\",ascending=False,ignore_index=True)\n",
    "        criticaltasks=[]\n",
    "        idx=0\n",
    "        prefinish=0\n",
    "        launchtime=dfxpds_ods[\"Launch_Time\"][0]\n",
    "        criticaltasks.append([dfxpds_ods[\"Task_ID\"][0],launchtime,dfxpds_ods[\"Finish_Time\"][0]])\n",
    "        total_row=len(dfxpds_ods)\n",
    "\n",
    "        while True:\n",
    "            while idx<total_row:\n",
    "                if dfxpds_ods[\"Finish_Time\"][idx]-2<launchtime:\n",
    "                    break\n",
    "                idx=idx+1\n",
    "            else:\n",
    "                break\n",
    "            cur_finish=dfxpds_ods[\"Finish_Time\"][idx]\n",
    "            cur_finish=launchtime-1 if cur_finish>=launchtime else cur_finish\n",
    "            launchtime=dfxpds_ods[\"Launch_Time\"][idx]\n",
    "            criticaltasks.append([dfxpds_ods[\"Task_ID\"][idx],launchtime,cur_finish])\n",
    "        self.criticaltasks=criticaltasks\n",
    "\n",
    "    def get_physical_plan(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        queryid=kwargs.get('queryid',None)\n",
    "        shownops=kwargs.get(\"shownops\",['ArrowRowToColumnarExec','ColumnarToRow','RowToArrowColumnar',\n",
    "                                        'VeloxNativeColumnarToRowExec','ArrowColumnarToRow','Filter','HashAggregate','Project','SortAggregate','SortMergeJoin','window'])\n",
    "        \n",
    "        desensitization=kwargs.get('desensitization',True)\n",
    "        \n",
    "        def get_fields(colss):\n",
    "            lvls=0\n",
    "            colns=[]\n",
    "            ks=\"\"\n",
    "            for c in colss:\n",
    "                if c==\",\" and lvls==0:\n",
    "                    colns.append(ks)\n",
    "                    ks=\"\"\n",
    "                    continue\n",
    "                if c==\" \" and ks==\"\":\n",
    "                    continue\n",
    "                if c==\"(\":\n",
    "                    lvls+=1\n",
    "                if c==\")\":\n",
    "                    lvls-=1\n",
    "                ks+=c\n",
    "            if ks!=\"\":\n",
    "                colns.append(ks)\n",
    "            return colns\n",
    "        \n",
    "        def get_column_names(s, opname, resultname, prefix, columns, funcs):\n",
    "            p=re.search(r\" \"+opname+\" \",s[0])\n",
    "            if p:\n",
    "                for v in s[1].split(\"\\n\"):\n",
    "                    if v.startswith(resultname):\n",
    "                        cols=re.search(\"\\[([^0-9].+)\\]\",v)\n",
    "                        if cols:\n",
    "                            colss=cols.group(1)\n",
    "                            colns=get_fields(colss)\n",
    "                            if opname+str(len(columns)) not in funcs:\n",
    "                                funcs[opname+str(len(columns))]=[]\n",
    "                            funcs[opname+str(len(columns))].extend(colns)\n",
    "                            for c in colns:\n",
    "                                if \" AS \" in c:\n",
    "                                    c=re.sub(\"#\\d+L*\",\"\",c)\n",
    "                                    colname=re.search(r\" AS (.+)\",c).group(1)\n",
    "                                    if colname not in columns:\n",
    "                                        columns[colname]=prefix\n",
    "        \n",
    "        plans=appals.queryplans.select('real_queryid','physicalPlanDescription').collect() if queryid is None else appals.queryplans.where(f\"real_queryid='{queryid}'\").select(\"physicalPlanDescription\").collect()\n",
    "        \n",
    "        for pr in range(0,len(plans)):\n",
    "            plan=plans[pr]['physicalPlanDescription']\n",
    "            nodes={}\n",
    "            lines=plan.split(\"\\n\")\n",
    "            for idx in range(0,len(lines)):\n",
    "                l=lines[idx]\n",
    "                if l=='+- == Final Plan ==':\n",
    "                    while l!='+- == Initial Plan ==':\n",
    "                        idx+=1\n",
    "                        l=lines[idx]\n",
    "                        if not l.endswith(\")\"):\n",
    "                            break\n",
    "                        idv=re.search(\"\\(\\d+\\)$\",l).group(0)\n",
    "                        nodes[idv]=[l]\n",
    "                if l==\"== Physical Plan ==\":\n",
    "                    while not lines[idx+1].startswith(\"(\"):\n",
    "                        idx+=1\n",
    "                        l=lines[idx]\n",
    "                        if not l.endswith(\")\"):\n",
    "                            break\n",
    "                        idv=re.search(\"\\(\\d+\\)$\",l).group(0)\n",
    "                        nodes[idv]=[l]\n",
    "                        \n",
    "                if l.startswith(\"(\"):\n",
    "                    idv=re.search(\"^\\(\\d+\\)\",l).group(0)\n",
    "                    if idv in nodes:\n",
    "                        desc=\"\"\n",
    "                        while l.strip()!=\"\":\n",
    "                            desc+=l+\"\\n\"\n",
    "                            idx+=1\n",
    "                            l=lines[idx]\n",
    "                        desc=re.sub(r\"#\\d+L*\",r\"\",desc)\n",
    "                        desc=re.sub(r\"= [^)]+\",r\"=\",desc)\n",
    "                        desc=re.sub(r\"IN \\([^)]\\)\",r\"IN ()\",desc)\n",
    "                        desc=re.sub(r\"In\\([^)]\\)\",r\"In()\",desc)\n",
    "                        desc=re.sub(r\"EqualTo\\(([^,]+),[^)]+\\)\",r\"EqualTo(\\1,)\",desc)\n",
    "                        desc=re.sub(r\"搜索广告\",r\"xxx\",desc)\n",
    "                        ## add all keyword replace here\n",
    "                        nodes[idv].append(desc)\n",
    "            tables={}\n",
    "            columns={}\n",
    "            functions={}\n",
    "            for s in nodes.values():\n",
    "                p=re.search(r\"Scan arrow [^.]*\\.([^ ]+)\",s[0])\n",
    "                if p:\n",
    "                    tn=p.group(1)\n",
    "                    if not tn in tables:\n",
    "                        tables[tn]=\"table\"\n",
    "                    if desensitization:\n",
    "                        s[0]=s[0].replace(tn,tables[tn])\n",
    "                        s[1]=s[1].replace(tn,tables[tn])\n",
    "                    colsv=[]\n",
    "                    schema=[]\n",
    "                    for v in s[1].split(\"\\n\"):\n",
    "                        if v.startswith(\"ReadSchema\"):\n",
    "                            cols=re.search(\"<(.*)>\",v)\n",
    "                            if cols:\n",
    "                                colss=cols.group(1).split(\",\")\n",
    "                                for c in colss:\n",
    "                                    cts=c.split(\":\")\n",
    "                                    ct=cts[0]\n",
    "                                    if not ct in columns:\n",
    "                                        if len(cts)==2:\n",
    "                                            cts[1]=cts[1]\n",
    "                                            columns[ct]=cts[1]+\"_\"\n",
    "                                        else:\n",
    "                                            columns[ct]=\"c_\"\n",
    "                        if v.startswith(\"Location\") and desensitization:\n",
    "                            s[1]=s[1].replace(v+\"\\n\",\"\")\n",
    "                            \n",
    "                get_column_names(s, \"Project\", \"Output\", \"proj_\", columns, functions)\n",
    "                get_column_names(s, \"HashAggregate\", \"Results\", \"shagg_\", columns, functions)\n",
    "                get_column_names(s, \"SortAggregate\", \"Results\", \"stagg_\", columns, functions)\n",
    "                get_column_names(s, \"ColumnarConditionProject\", \"Arguments\", \"cproj_\", columns, functions)\n",
    "                get_column_names(s, \"ColumnarHashAggregate\", \"Results\", \"cshagg_\", columns, functions)\n",
    "                get_column_names(s, \"Window\", \"Arguments\", \"window_\", columns, functions)\n",
    "\n",
    "            keys=[]\n",
    "            ckeys=list(columns.keys())\n",
    "            for l in range(0,len(ckeys)):\n",
    "                k1=ckeys[l]\n",
    "                for k in range(0,len(keys)):\n",
    "                    if keys[k] in k1:\n",
    "                        keys.insert(k,k1)\n",
    "                        break\n",
    "                else:\n",
    "                    keys.append(k1)\n",
    "                \n",
    "            for s in nodes.values():\n",
    "                s[1]=html.escape(s[1])\n",
    "                if desensitization:\n",
    "                    for c in keys:\n",
    "                        v=columns[c]\n",
    "                        if v.startswith(\"array\") or v.startswith(\"map\") or v.startswith(\"struct\"):\n",
    "                            s[1]=re.sub(c, '<span style=\"color:red;background-color:yellow\">'+html.escape(v)+\"</span>\",s[1])\n",
    "                        else:\n",
    "                            s[1]=re.sub(c, \"<font color=#33cc33>\"+html.escape(v)+\"</font>\",s[1])\n",
    "\n",
    "\n",
    "            htmls=['''<table style=\"table-layout:fixed;max-width: 100%;\">''']\n",
    "            qid=pr+1 if queryid is None else queryid\n",
    "            htmls.append(f\"<tr><td colspan=2>{qid}</td></tr>\")\n",
    "            for l in nodes.values():\n",
    "                if shownops is not None:\n",
    "                    for k in shownops:\n",
    "                        if \" \"+k+\" \" in l[0]:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                htmls.append(\"<tr>\")\n",
    "                htmls.append('<td width=33%><div align=\"left\" style=\"font-family:Courier New;overflow-wrap: anywhere\">')\n",
    "                htmls.append(l[0].replace(\" \",\"_\")\n",
    "                             .replace(\"ColumnarToRow\",\"<font color=blue>ColumnarToRow</font>\")\n",
    "                             .replace(\"RowToArrowColumnar\",\"<font color=blue>RowToArrowColumnar</font>\")\n",
    "                             .replace(\"ArrowColumnarToRow\",\"<font color=blue>ArrowColumnarToRow</font>\")\n",
    "                             .replace(\"ArrowRowToColumnar\",\"<font color=blue>ArrowRowToColumnar</font>\")\n",
    "                             .replace(\"VeloxNativeColumnarToRowExec\",\"<font color=blue>VeloxNativeColumnarToRowExec</font>\")\n",
    "                            )\n",
    "                htmls.append(\"</div></td>\")\n",
    "                htmls.append('<td width=66%><div align=\"left\" style=\"font-family:Courier New;overflow-wrap: anywhere\">')\n",
    "                ls=l[1].split(\"\\n\")\n",
    "                lsx=[]\n",
    "                for t in ls:\n",
    "                    cols=re.search(\"\\[([^0-9].+)\\]\",t)\n",
    "                    if cols:\n",
    "                        colss=cols.group(1)\n",
    "                        colns=get_fields(colss)\n",
    "                        t=re.sub(\"\\[([^0-9].+)\\]\",\"\",t)\n",
    "                        t+=\"[\"+'<span style=\"background-color:#ededed;\">;</span>'.join(colns)+\"]\"                        \n",
    "                    if \":\" in t:\n",
    "                        lsx.append(re.sub(r'^([^:]+:)',r'<font color=blue>\\1</font>',t))\n",
    "                    else:\n",
    "                        lsx.append(t)\n",
    "                htmls.append(\"<br>\".join(lsx))\n",
    "                htmls.append(\"</div></td>\")\n",
    "                htmls.append(\"</tr>\")\n",
    "            htmls.append(\"</table>\")\n",
    "            display(HTML(\"\\n\".join(htmls)))\n",
    "            \n",
    "            for k, v in functions.items():\n",
    "                functions[k]=[l for l in v if \"(\" in l]\n",
    "            for f in functions.values():\n",
    "                for idx in range(0,len(f)):\n",
    "                    for c in keys:\n",
    "                        v=columns[c]\n",
    "                        if v.startswith(\"array\") or v.startswith(\"map\") or v.startswith(\"struct\"):\n",
    "                            f[idx]=re.sub(c, '<span style=\"color:red;background-color:yellow\">'+html.escape(v)+\"</span>\",f[idx])\n",
    "                        else:\n",
    "                            f[idx]=re.sub(c, \"<font color=#33cc33>\"+html.escape(v)+\"</font>\",f[idx])\n",
    "            funchtml=\"<table>\"\n",
    "            for k,v in functions.items():\n",
    "                if shownops is not None:\n",
    "                    for ks in shownops:\n",
    "                        if \" \"+ks+\" \" in k:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                funchtml+=\"<tr><td width=10%>\"+k+'</td><td width=90%><table stype=\"width:100%;table-layout:fixed\">'\n",
    "                for f in v:\n",
    "                    funchtml+='<tr><td width=100% ><div align=\"left\" style=\"font-family:Courier New\">'+f+\"</div></td></tr>\"\n",
    "                funchtml+=\"</table></td></tr>\"\n",
    "            funchtml+=\"</table>\"    \n",
    "            display(HTML(funchtml))\n",
    "        \n",
    "        return plans\n",
    "        \n",
    "    def get_physical_allnodes(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        queryid=None\n",
    "        \n",
    "        plans=appals.queryplans.select('real_queryid','physicalPlanDescription').collect() if queryid is None else appals.queryplans.where(f\"real_queryid='{queryid}'\").select(\"physicalPlanDescription\").collect()\n",
    "        \n",
    "        allnodes={}\n",
    "        for pr in range(0,len(plans)):\n",
    "            plan=plans[pr]['physicalPlanDescription']\n",
    "            allnodes[pr]={}\n",
    "            nodes=allnodes[pr]\n",
    "            if plan is None:\n",
    "                continue\n",
    "            lines=plan.split(\"\\n\")\n",
    "            for idx in range(0,len(lines)):\n",
    "                l=lines[idx]\n",
    "                if l=='+- == Final Plan ==':\n",
    "                    while l!='+- == Initial Plan ==':\n",
    "                        idx+=1\n",
    "                        l=lines[idx]\n",
    "                        if not l.endswith(\")\"):\n",
    "                            break\n",
    "                        idv=re.search(\"\\(\\d+\\)$\",l).group(0)\n",
    "                        nodes[idv]=[l]\n",
    "                if l.startswith(\"(\"):\n",
    "                    idv=re.search(\"^\\(\\d+\\)\",l).group(0)\n",
    "                    if idv in nodes:\n",
    "                        desc=\"\"\n",
    "                        while l!=\"\":\n",
    "                            desc+=l+\"\\n\"\n",
    "                            idx+=1\n",
    "                            l=lines[idx]\n",
    "                        nodes[idv].append(desc)\n",
    "        return allnodes\n",
    "        \n",
    "        \n",
    "    def get_basic_state(appals):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        display(HTML(\"<a href=http://https://monarch-dev.pinadmin.com/gateway/monarch-dev-015/sparkhistory/history/\"+appals.appid+\"/1/stages/>http://https://monarch-dev.pinadmin.com/gateway/monarch-dev-015/sparkhistory/history/\"+appals.appid+\"/1/stages/</a>\"))\n",
    "        \n",
    "        errorcolor=\"#000000\" if appals.executor_instances == appals.realexecutors else \"#c0392b\"\n",
    "        \n",
    "        qtime=appals.get_query_time(plot=False)\n",
    "        sums=qtime.sum()\n",
    "        \n",
    "        #total_rchar,total_wchar,total_read_bytes,total_write_bytes,total_cancelled_write_bytes = getexecutor_stat(appals.file[:-len(\"app.log\")])\n",
    "        \n",
    "        if len(appals.failed_stages)>0:\n",
    "            failure=\"<br>\".join([\"query: \" + str(l[\"real_queryid\"])+\"|stage: \" + str(l[\"Stage ID\"]) for l in appals.df.where(\"`Stage ID` in (\"+\",\".join(appals.failed_stages)+\")\").select(\"real_queryid\",\"Stage ID\").distinct().collect()])\n",
    "        else:\n",
    "            failure=\"\"\n",
    "        display(HTML(f'''\n",
    "        <table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width:500px\">\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">appid</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.appid}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">executor.instances</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.executor_instances}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">executor.cores</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.executor_cores}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">shuffle.partitions</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{(appals.parallelism)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">batch size</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{(appals.batchsize):,}</strong></span></td>\n",
    "                </tr>                \n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">real executors</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:{errorcolor}\"><strong>{(appals.realexecutors)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Failed Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:{errorcolor}\"><strong>{(failure)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Speculative Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#87b00c\"><strong>{(appals.speculativetask)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Speculative Killed Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#87b00c\"><strong>{(appals.speculativekilledtask)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Speculative Stage</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#87b00c\"><strong>{(appals.speculativestage)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">runtime</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['runtime'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">disk spilled</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['disk spilled'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">memspilled</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['memspilled'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">local_read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['local_read'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">remote_read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['remote_read'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">shuffle_write</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['shuffle_write'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">task run time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['run_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">ser_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['ser_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">f_wait_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['f_wait_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">gc_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['gc_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">input read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['input read'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">acc_task_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['acc_task_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "\n",
    "        '''))        \n",
    "   \n",
    "        \n",
    "    def generate_trace_view_list_exec(self,id=0,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,**kwargs)\n",
    "        showcpu=kwargs.get('showcpu',False)\n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        \n",
    "        showdf=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        \n",
    "        events=showdf.toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=self.starttime\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=showdf.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "        \n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                self.pids.append(pid)\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=[tsk,stime]\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=[tsk,stime]\n",
    "                #print(\"task {:d} tid is {:s}.{:d}\".format(tsk,pid,t))\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "\n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]) :\n",
    "                    if tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2:\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "                    print(f'applog start time: {starttime}')\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['job id'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "\n",
    "                des_time=l['Executor Deserialize Time']\n",
    "                read_time=l['Fetch Wait Time']\n",
    "                exec_time=l['Executor Run Time']\n",
    "                write_time=math.floor(l['Shuffle Write Time']/1000000)\n",
    "                ser_time=l['Result Serialization Time']\n",
    "                getrst_time=l['Getting Result Time']\n",
    "                durtime=fintime-sstime-starttime;\n",
    "\n",
    "                times=[0,des_time,read_time,exec_time,write_time,ser_time,getrst_time]\n",
    "                time_names=['sched delay','deserialize time','read time','executor time','write time','serialize time','result time']\n",
    "                evttime=reduce((lambda x, y: x + y),times)\n",
    "                if evttime>durtime:\n",
    "                    times=[math.floor(l*1.0*durtime/evttime) for l in times]\n",
    "                else:\n",
    "                    times[0]=durtime-evttime\n",
    "\n",
    "                esstime=sstime\n",
    "                for idx in range(0,len(times)):\n",
    "                    if times[idx]>0:\n",
    "                        trace_events.append({\n",
    "                             'tid':pid+int(t),\n",
    "                             'ts':esstime,\n",
    "                             'dur':times[idx],                \n",
    "                             'pid':pid,\n",
    "                             'ph':'X',\n",
    "                             'name':time_names[idx]})\n",
    "                        if idx==3:\n",
    "                            trace_events.append({\n",
    "                                 'tid':pid+int(t),\n",
    "                                 'ts':esstime,\n",
    "                                 'dur':l['JVM GC Time'],\n",
    "                                 'pid':pid,\n",
    "                                 'ph':'X',\n",
    "                                 'name':'GC Time'})\n",
    "                            if showcpu:\n",
    "                                trace_events.append({\n",
    "                                     'tid':pid+int(t),\n",
    "                                     'ts':esstime,\n",
    "                                     'pid':pid,\n",
    "                                     'ph':'C',\n",
    "                                     'name':'cpu% {:d}'.format(pid+int(t)),\n",
    "                                     'args':{'value':l['Executor CPU Time']/1000000.0/times[idx]}})\n",
    "                                trace_events.append({\n",
    "                                     'tid':pid+int(t),\n",
    "                                     'ts':esstime+times[idx],\n",
    "                                     'pid':pid,\n",
    "                                     'ph':'C',\n",
    "                                     'name':'cpu% {:d}'.format(pid+int(t)),\n",
    "                                     'args':{'value':0}})\n",
    "                        esstime=esstime+times[idx]\n",
    "        self.starttime=starttime\n",
    "        return [json.dumps(l) for l in trace_events]\n",
    "\n",
    "    def generate_trace_view_list(self,id=0,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,**kwargs)\n",
    "        showcpu=kwargs.get('showcpu',False)\n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        \n",
    "        showdf=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        \n",
    "        showdf=showdf.orderBy([\"eventtime\", \"Finish Time\"], ascending=[1, 0])\n",
    "        \n",
    "        events=showdf.drop(\"Accumulables\").toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=self.starttime\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=showdf.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "        \n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        tskmap={}\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                self.pids.append(pid)\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=[tsk,stime]\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=[tsk,stime]\n",
    "                #print(f\"task {tsk} tid is {pid}.{t}\")\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "                \n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]) :\n",
    "                    if tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2:\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "                    print(f'applog start time: {starttime}')\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['Job ID'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "                tskmap[tsk]={'pid':pid,'tid':pid+int(t)}\n",
    "\n",
    "        self.starttime=starttime\n",
    "        self.tskmap=tskmap\n",
    "        output=[json.dumps(l) for l in trace_events]\n",
    "        \n",
    "        df=self.df\n",
    "        \n",
    "        if showcpu and len(self.metricscollect)>0:\n",
    "            metricscollect=self.metricscollect\n",
    "            metrics_explode=df.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "            m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "            metric_name_df = spark.createDataFrame(metricscollect)\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "\n",
    "            met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "            met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "            met_df=met_df.where(\"Update>1\")\n",
    "\n",
    "            metdfx=met_df.groupBy(\"Task ID\",\"elapsedtime\").agg(F.sum(\"Update\").alias(\"totalCnt\"))\n",
    "            taskratio=metdfx.withColumn(\"ratio\",F.when(F.col(\"totalCnt\")<F.col(\"elapsedtime\"),1).otherwise(F.col(\"elapsedtime\")/F.col(\"totalCnt\"))).select(\"Task ID\",\"ratio\")\n",
    "            met_df=met_df.join(taskratio,on=\"Task ID\")\n",
    "            met_df=met_df.withColumn(\"Update\",F.col(\"Update\")*F.col(\"ratio\"))\n",
    "\n",
    "            w = (Window.partitionBy('Task ID').orderBy(F.desc(\"Update\")).rangeBetween(Window.unboundedPreceding, 0))\n",
    "            met_df=met_df.withColumn('cum_sum', F.sum('Update').over(w))\n",
    "\n",
    "            met_df=met_df.withColumn(\"starttime\",F.col(\"Launch Time\")+F.col(\"cum_sum\")-F.col(\"Update\"))\n",
    "\n",
    "            tskmapdf = spark.createDataFrame(pandas.DataFrame(self.tskmap).T.reset_index())\n",
    "            met_df=met_df.join(tskmapdf,on=[met_df[\"Task ID\"]==tskmapdf[\"index\"]])\n",
    "\n",
    "            rstdf=met_df.select(\n",
    "                F.col(\"tid\"),\n",
    "                F.round(F.col(\"starttime\")-self.starttime,0).alias(\"ts\"),\n",
    "                F.round(F.col(\"Update\"),0).alias(\"dur\"),\n",
    "                F.col(\"pid\"),\n",
    "                F.lit(\"X\").alias(\"ph\"),\n",
    "                F.col(\"mname\").alias(\"name\")\n",
    "            ).where(F.col(\"ts\").isNotNull()).orderBy('ts')\n",
    "\n",
    "            output.extend(rstdf.toJSON().collect())\n",
    "\n",
    "            qtime=df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"real_queryid\").agg(F.min(\"Finish Time\").alias(\"time\"))\n",
    "            output.extend(qtime.select(\n",
    "                F.lit(\"i\").alias(\"ph\"),\n",
    "                (F.col(\"time\")-starttime).alias('ts'),\n",
    "                F.lit(0).alias(\"pid\"),\n",
    "                F.lit(0).alias(\"tid\"),\n",
    "                F.lit(\"p\").alias(\"s\")\n",
    "            ).toJSON().collect())\n",
    "        \n",
    "        self.starttime=starttime\n",
    "        \n",
    "        if kwargs.get(\"show_criticalshow_time_metric_path\",True):\n",
    "            output.extend(self.generate_critical_patch_traceview(hostid-1))\n",
    "        \n",
    "        return output        \n",
    "\n",
    "    def generate_critical_patch_traceview(self,pid):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        traces=[]\n",
    "        df=self.df.where(\"Event='SparkListenerTaskEnd' and real_queryid is not null\")\n",
    "        criticaltasks=self.criticaltasks\n",
    "        cripds=pandas.DataFrame(criticaltasks)\n",
    "        cripds.columns=['task_id',\"launch\",\"finish\"]\n",
    "        cridf=spark.createDataFrame(cripds)\n",
    "        df_ctsk=df.join(cridf,on=[F.col(\"task_id\")==F.col(\"Task ID\")],how=\"inner\")\n",
    "        traces.extend(df_ctsk.select(F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"launch\")-F.lit(self.starttime)+1).alias(\"ts\"),\n",
    "                      (F.col(\"finish\")-F.col(\"launch\")-1).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.concat(F.lit(\"stg\"),F.col(\"Stage ID\")).alias(\"name\"),\n",
    "                      F.struct(\n",
    "                          F.col(\"Task ID\").alias('taskid'),\n",
    "                          F.col(\"Executor ID\").astype(IntegerType()).alias('exec_id'),\n",
    "                          F.col(\"Host\").alias(\"host\"),\n",
    "                          ).alias(\"args\")\n",
    "                        ).toJSON().collect())\n",
    "        traces.extend(df.groupBy(\"real_queryid\").agg(F.max(\"Finish Time\").alias(\"finish\"),F.min(\"Launch Time\").alias(\"launch\")).select(\n",
    "                        F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"launch\")-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                      (F.col(\"finish\")-F.col(\"launch\")).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.concat(F.lit(\"qry\"),F.col(\"real_queryid\")).alias(\"name\")).toJSON().collect())\n",
    "\n",
    "\n",
    "        metricscollect=self.metricscollect\n",
    "\n",
    "        metrics_explode=df_ctsk.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "        m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "        metric_name_df = spark.createDataFrame(metricscollect)\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_4\",\"node\")\n",
    "\n",
    "        metric_name_df=metric_name_df.where(\"mname <> 'totaltime to collect batch' and mname <> 'totaltime of scan'\")\n",
    "\n",
    "        met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "        met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "        \n",
    "        #pandas UDF doesn't work. hang\n",
    "        #tmbk=met_df.groupBy('Task ID').apply(time_breakdown)\n",
    "        \n",
    "        w=Window.partitionBy('Task ID')\n",
    "        met_df1=met_df.withColumn(\"sum_update\",F.sum(\"Update\").over(w))\n",
    "        met_df2=met_df1.withColumn(\"ratio\",(F.col(\"Finish Time\")-F.col(\"Launch Time\")-2)/F.col(\"sum_update\"))\n",
    "        met_df3=met_df2.withColumn(\"ratio\",F.when(F.col(\"ratio\")>1,1).otherwise(F.col(\"ratio\")))\n",
    "        met_df4=met_df3.withColumn(\"update_ratio\",F.floor(F.col(\"ratio\")*F.col(\"Update\")))\n",
    "        met_df5=met_df4.where(F.col(\"update_ratio\")>2)\n",
    "        w = (Window.partitionBy('Task ID').orderBy(F.desc(\"update_ratio\")).rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    "        met_df6=met_df5.withColumn('ltime_dur', F.sum('update_ratio').over(w))\n",
    "        met_df8=met_df6.withColumn(\"ltime\",F.col(\"ltime_dur\")+F.col(\"Launch Time\")-F.col(\"update_ratio\"))\n",
    "\n",
    "        tmbk=met_df8.withColumn(\"taskid\",F.col(\"Task ID\")).withColumn(\"start\",F.col(\"ltime\")+F.lit(1)).withColumn(\"dur\",F.col(\"update_ratio\")-F.lit(1)).withColumn(\"name\",F.col(\"mname\"))\n",
    "        \n",
    "        \n",
    "        traces.extend(tmbk.select(\n",
    "                        F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"start\")-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                      (F.col(\"dur\")).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.col(\"name\").alias(\"name\")).toJSON().collect())\n",
    "        traces.append(json.dumps({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"critical path\"}\n",
    "                      }))\n",
    "        return traces    \n",
    "    \n",
    "    def show_Stage_histogram(apps,stageid,bincount):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        inputsize = apps.df.where(\"`Stage ID`={:d}\".format(stageid)).select(\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"Task ID\") \\\n",
    "                      .agg((F.sum(\"Update\")).alias(\"input read\"))\n",
    "\n",
    "\n",
    "        stage37=apps.df.where(\"`Stage ID`={:d} and event='SparkListenerTaskEnd'\".format(stageid) )\\\n",
    "                        .join(inputsize,on=[\"Task ID\"],how=\"left\")\\\n",
    "                        .fillna(0) \\\n",
    "                        .select(F.col('Host'), \n",
    "                                F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),\n",
    "                                F.round((F.col('`input read`')+F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input'))\n",
    "        stage37=stage37.cache()\n",
    "        hist_elapsedtime=stage37.select('elapsedtime').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        hist_input=stage37.select('input').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        fig, axs = plt.subplots(figsize=(30, 5),nrows=1, ncols=2)\n",
    "        ax=axs[0]\n",
    "        binSides, binCounts = hist_elapsedtime\n",
    "        binSides=[builtins.round(l,2) for l in binSides]\n",
    "\n",
    "        N = len(binCounts)\n",
    "        ind = numpy.arange(N)\n",
    "        width = 0.5\n",
    "\n",
    "        rects1 = ax.bar(ind+0.5, binCounts, width, color='b')\n",
    "\n",
    "        ax.set_ylabel('Frequencies')\n",
    "        ax.set_title('stage{:d} elapsed time breakdown'.format(stageid))\n",
    "        ax.set_xticks(numpy.arange(N+1))\n",
    "        ax.set_xticklabels(binSides)\n",
    "\n",
    "        ax=axs[1]\n",
    "        binSides, binCounts = hist_input\n",
    "        binSides=[builtins.round(l,2) for l in binSides]\n",
    "\n",
    "        N = len(binCounts)\n",
    "        ind = numpy.arange(N)\n",
    "        width = 0.5\n",
    "        rects1 = ax.bar(ind+0.5, binCounts, width, color='b')\n",
    "\n",
    "        ax.set_ylabel('Frequencies')\n",
    "        ax.set_title('stage{:d} input data breakdown'.format(stageid))\n",
    "        ax.set_xticks(numpy.arange(N+1))\n",
    "        ax.set_xticklabels(binSides)\n",
    "\n",
    "        out=stage37\n",
    "        outpds=out.toPandas()\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=3, sharey=False,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1, 1]})\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "        groups= outpds.groupby('Host')\n",
    "        for name, group in groups:\n",
    "            axs[0].plot(group.input, group.elapsedtime, marker='o', linestyle='', ms=5, label=name)\n",
    "        axs[0].set_xlabel('input size (MB)')\n",
    "        axs[0].set_ylabel('elapsed time (s)')\n",
    "\n",
    "        #axs[0].legend()\n",
    "\n",
    "        axs[0].get_shared_y_axes().join(axs[0], axs[1])\n",
    "\n",
    "        sns.violinplot(y='elapsedtime', x='Host', data=outpds,palette=['g'],ax=axs[1])\n",
    "        axs[1].set_xticklabels([])\n",
    "        sns.violinplot(y='input', x='Host', data=outpds,palette=['g'],ax=axs[2])\n",
    "        axs[2].set_xticklabels([])\n",
    "\n",
    "        #ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(''))\n",
    "        #ax.yaxis.set_major_formatter(mtick.FormatStrFormatter(''))\n",
    "\n",
    "        if False:\n",
    "            out=stage37\n",
    "            vecAssembler = VectorAssembler(inputCols=[\"input\",'elapsedtime'], outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "            new_df = vecAssembler.transform(out)\n",
    "            kmeans = KMeans(k=2, seed=1)  # 2 clusters here\n",
    "            model = kmeans.fit(new_df.select('features'))\n",
    "            transformed = model.transform(new_df)\n",
    "\n",
    "\n",
    "            outpds=transformed.select('Host','elapsedtime','input','prediction').toPandas()\n",
    "\n",
    "            fig, axs = plt.subplots(nrows=1, ncols=2, sharey=False,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1]})\n",
    "            plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "            groups= outpds.groupby('prediction')\n",
    "            for name, group in groups:\n",
    "                axs[0].plot(group.input, group.elapsedtime, marker='o', linestyle='', ms=5, label=name)\n",
    "            axs[0].legend()\n",
    "\n",
    "            bars=transformed.where('prediction=1').groupBy(\"Host\").count().toPandas()\n",
    "\n",
    "            axs[1].bar(bars['Host'], bars['count'], 0.4, color='coral')\n",
    "            axs[1].set_title('cluster=1')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def show_Stages_hist(apps,**kwargs):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        bincount=kwargs.get(\"bincount\",15)\n",
    "        threshold=kwargs.get(\"threshold\",0.9)\n",
    "        \n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        df=apps.df.where(F.col(\"real_queryid\").isin(query)) if query else apps.df\n",
    "        \n",
    "        totaltime=df.where(\"event='SparkListenerTaskEnd'\" ).agg(F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time')).collect()[0]['total_time']\n",
    "        stage_time=df.where(\"event='SparkListenerTaskEnd'\" ).groupBy('`Stage ID`').agg(F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time')).orderBy('total_time', ascending=False).toPandas()\n",
    "        stage_time['acc_total'] = stage_time['total_time'].cumsum()/totaltime\n",
    "        stage_time=stage_time.reset_index()\n",
    "        fig, ax = plt.subplots(figsize=(30, 5))\n",
    "\n",
    "        rects1 = ax.plot(stage_time['index'],stage_time['acc_total'],'b.-')\n",
    "        ax.set_xticks(stage_time['index'])\n",
    "        ax.set_xticklabels(stage_time['Stage ID'])\n",
    "        ax.set_xlabel('stage')\n",
    "        ax.grid(which='major', axis='x')\n",
    "        plt.show()\n",
    "        shownstage=[]\n",
    "        for x in stage_time.index:\n",
    "            if stage_time['acc_total'][x]<=threshold:\n",
    "                shownstage.append(stage_time['Stage ID'][x])\n",
    "            else:\n",
    "                shownstage.append(stage_time['Stage ID'][x])\n",
    "                break\n",
    "        for row in shownstage:\n",
    "            apps.show_Stage_histogram(row,bincount) \n",
    "            \n",
    "    def get_hottest_stages(apps,**kwargs):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        bincount=kwargs.get(\"bincount\",15)\n",
    "        threshold=kwargs.get(\"threshold\",0.9)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        df=apps.df.where(F.col(\"real_queryid\").isin(query)) if query else apps.df.where(\"queryid is not NULL\")\n",
    "\n",
    "        stage_time=df.where(\"event='SparkListenerTaskEnd'\" ).groupBy('`Stage ID`','Job ID','real_queryid').agg(\n",
    "            F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time'),\n",
    "            F.stddev(F.col('Finish Time')/1000-F.col('Launch Time')/1000).alias('stdev_time'),\n",
    "            F.count(\"*\").alias(\"cnt\"),\n",
    "            F.first('queryid').astype(IntegerType()).alias('queryid')\n",
    "            )\\\n",
    "            .select('`Stage ID`','Job ID','real_queryid','queryid',\n",
    "                    (F.col(\"total_time\")/1000/(F.when(F.col(\"cnt\")>F.lit(apps.executor_instances*apps.executor_cores/apps.taskcpus),F.lit(apps.executor_instances*apps.executor_cores/apps.taskcpus)).otherwise(F.col(\"cnt\")))).alias(\"total_time\"),\n",
    "                    F.col(\"stdev_time\")\n",
    "                   ).orderBy('total_time', ascending=False).toPandas()\n",
    "\n",
    "        totaltime=stage_time['total_time'].sum()\n",
    "        stage_time['acc_total'] = stage_time['total_time'].cumsum()/totaltime\n",
    "        stage_time['total'] = stage_time['total_time']/totaltime\n",
    "        stage_time=stage_time.reset_index()\n",
    "\n",
    "        shownstage=stage_time.loc[stage_time['total'] >=0.05]\n",
    "        shownstage['stg']=shownstage['real_queryid'].astype(str)+'_'+shownstage['Job ID'].astype(str)+'_'+shownstage['Stage ID'].astype(str)\n",
    "        if plot:\n",
    "            totals=shownstage.T.loc[['total']]\n",
    "            totals.columns=list(shownstage['stg'])\n",
    "            totals.plot.bar(stacked=True,figsize=(30,8))\n",
    "            display(stage_time)\n",
    "            for stg in shownstage['Stage ID']:\n",
    "                apps.show_Stage_histogram(stg,15)\n",
    "                apps.show_time_metric(stageids=[stg])\n",
    "                \n",
    "        return stage_time\n",
    "\n",
    "    def scatter_elapsetime_input(apps,stageid):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        stage37=apps.df.where(\"`Stage ID`={:d} and event='SparkListenerTaskEnd'\".format(stageid) ).select(F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),F.round((F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input')).toPandas()\n",
    "        stage37.plot.scatter('input','elapsedtime',figsize=(30, 5))\n",
    "\n",
    "    def get_critical_path_stages(self):     \n",
    "        df=self.df.where(\"Event='SparkListenerTaskEnd'\")\n",
    "        criticaltasks=self.criticaltasks\n",
    "        cripds=pandas.DataFrame(criticaltasks)\n",
    "        cripds.columns=['task_id',\"launch\",\"finish\"]\n",
    "        cridf=spark.createDataFrame(cripds)\n",
    "        df_ctsk=df.join(cridf,on=[F.col(\"task_id\")==F.col(\"Task ID\")],how=\"inner\")\n",
    "        df_ctsk=df_ctsk.withColumn(\"elapsed\",(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000)\n",
    "        return df_ctsk.where(\"elapsed>10\").orderBy(F.desc(\"elapsed\")).select(\"real_queryid\",F.round(\"elapsed\",2).alias(\"elapsed\"),\"Host\",\"executor ID\",\"Stage ID\",\"Task ID\",F.round(F.col(\"Bytes Read\")/1000000,0).alias(\"file read\"),F.round((F.col(\"Local Bytes Read\")+F.col(\"Remote Bytes Read\"))/1000000,0).alias(\"shuffle read\")).toPandas()\n",
    "        \n",
    "    def show_time_metric(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        taskids=kwargs.get(\"taskids\",None)\n",
    "        stageids=kwargs.get(\"stageids\",None)\n",
    "        \n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        \n",
    "        showexecutor=kwargs.get(\"showexecutor\",True) if not (taskids or stageids)else False\n",
    "        queryid = query[0] if query else 0\n",
    "        \n",
    "        df=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        df=df.where(F.col(\"real_queryid\").isin(query)) if query else df.where(\"queryid is not NULL\")\n",
    "\n",
    "        df=df.where(F.col(\"Task ID\").isin(taskids)) if taskids else df\n",
    "        df=df.where(F.col(\"Stage ID\").isin(stageids)) if stageids else df\n",
    "\n",
    "        exec_cores=1 if taskids else self.executor_cores\n",
    "        execs=1 if taskids else self.realexecutors\n",
    "\n",
    "        metricscollect=self.metricscollect\n",
    "\n",
    "        metrics_explode=df.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "        m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "        metric_name_df = spark.createDataFrame(metricscollect)\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_4\",\"node\")\n",
    "\n",
    "        runtime=metrics_explode.agg(F.round(F.max(\"Finish Time\")/1000-F.min(\"Launch Time\")/1000,2).alias(\"runtime\")).collect()[0][\"runtime\"]\n",
    "\n",
    "        met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "        met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "        outpdf=met_df.groupBy(\"`Executor ID`\",\"mname\").sum(\"Update\").orderBy(\"Executor ID\").toPandas()\n",
    "\n",
    "        met_time_cnt=df.where(\"Event='SparkListenerTaskEnd'\")\n",
    "        exectime=met_time_cnt.groupBy(\"Executor ID\").agg((F.max(\"Finish Time\")-F.min(\"Launch Time\")).alias(\"totaltime\"),F.sum(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"tasktime\"))\n",
    "\n",
    "        totaltime_query=met_time_cnt.groupBy(\"real_queryid\").agg((F.max(\"Finish Time\")-F.min(\"Launch Time\")).alias(\"totaltime\")).agg(F.sum(\"totaltime\").alias(\"totaltime\")).collect()\n",
    "        totaltime_query=totaltime_query[0][\"totaltime\"]\n",
    "        \n",
    "        pdf=exectime.toPandas()\n",
    "        exeids=set(outpdf['Executor ID'])\n",
    "        outpdfs=[outpdf[outpdf[\"Executor ID\"]==l] for l in exeids]\n",
    "        tasktime=pdf.set_index(\"Executor ID\").to_dict()['tasktime']\n",
    "\n",
    "        def comb(l,r):\n",
    "            execid=list(r['Executor ID'])[0]\n",
    "            lp=r[['mname','sum(Update)']]\n",
    "            lp.columns=[\"mname\",\"val_\"+execid]\n",
    "            idle=totaltime_query*exec_cores-tasktime[execid]\n",
    "            nocount=tasktime[execid]-sum(lp[\"val_\"+execid])\n",
    "            if idle<0:\n",
    "                idle=0\n",
    "            if nocount<0:\n",
    "                nocount=0\n",
    "            lp=lp.append([{\"mname\":\"idle\",\"val_\"+execid:idle}])\n",
    "            lp=lp.append([{\"mname\":\"not_counted\",\"val_\"+execid:nocount}])\n",
    "            if l is not None:\n",
    "                return pandas.merge(lp, l,on=[\"mname\"],how='outer')\n",
    "            else:\n",
    "                return lp\n",
    "\n",
    "        rstpdf=None\n",
    "        for l in outpdfs[0:]:\n",
    "            rstpdf=comb(rstpdf,l)\n",
    "            \n",
    "        for l in [l for l in rstpdf.columns if l!=\"mname\"]:\n",
    "            rstpdf[l]=rstpdf[l]/1000/exec_cores\n",
    "    \n",
    "        rstpdf=rstpdf.sort_values(by=\"val_\"+list(exeids)[0],axis=0,ascending=False)\n",
    "        if showexecutor and plot:\n",
    "            rstpdf.set_index(\"mname\").T.plot.bar(stacked=True,figsize=(30,8))\n",
    "        pdf_sum=pandas.DataFrame(rstpdf.set_index(\"mname\").T.sum())\n",
    "        totaltime=totaltime_query/1000\n",
    "        pdf_sum[0]=pdf_sum[0]/(execs)\n",
    "        pdf_sum[0][\"idle\"]=(totaltime_query-sum(tasktime.values())/execs/exec_cores)/1000\n",
    "        pdf_sum=pdf_sum.sort_values(by=0,axis=0,ascending=False)\n",
    "        pdf_sum=pdf_sum.T\n",
    "        pdf_sum.columns=[\"{:>2.0f}%_{:s}\".format(pdf_sum[l][0]/totaltime*100,l) for l in pdf_sum.columns]\n",
    "        matplotlib.rcParams['font.sans-serif'] = \"monospace\"\n",
    "        matplotlib.rcParams['font.family'] = \"monospace\"\n",
    "        import matplotlib.font_manager as font_manager\n",
    "        if plot:\n",
    "            ax=pdf_sum.plot.bar(stacked=True,figsize=(30,8))\n",
    "            font = font_manager.FontProperties(family='monospace',\n",
    "                                               style='normal', size=14)\n",
    "            ax.legend(prop=font,loc=4)\n",
    "            if stageids:\n",
    "                stageidstr=\",\".join([str(l) for l in stageids])\n",
    "            else:\n",
    "                stageidstr=\"All\"\n",
    "            plt.title(\"{:s} q{:d} executors={:d} cores_per_executor={:d} stage={:s} parallelism={:d} sumtime={:.0f} runtime={:.0f}\".format(\n",
    "                self.file.split(\"/\")[-1],queryid,self.realexecutors,self.executor_cores,stageidstr,self.parallelism,totaltime,runtime),fontdict={'fontsize':24})\n",
    "        return pdf_sum\n",
    "\n",
    "    def show_critical_path_time_breakdown(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        return self.show_time_metric(taskids=[l[0].item() for l in self.criticaltasks])\n",
    "    \n",
    "    def get_spark_config(self):\n",
    "        df=spark.read.json(self.file)\n",
    "        self.appid=df.where(\"`App ID` is not null\").collect()[0][\"App ID\"]\n",
    "        pandas.set_option('display.max_rows', None)\n",
    "        pandas.set_option('display.max_columns', None)\n",
    "        pandas.set_option('display.max_colwidth', 100000)\n",
    "        return df.select(\"Properties.*\").where(\"`spark.app.id` is not null\").limit(1).toPandas().T\n",
    "    \n",
    "    def get_app_name(self):\n",
    "        cfg=self.get_spark_config()\n",
    "        display(HTML(\"<font size=5 color=red>\" + cfg.loc[cfg.index=='spark.app.name'][0][0]+\"</font>\"))\n",
    "        \n",
    "        \n",
    "    def get_query_time(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        showtable=kwargs.get(\"showtable\",True)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "           \n",
    "        df=self.df.where(F.col(\"real_queryid\").isin(queryid)) if queryid else self.df.where(\"queryid is not NULL\")\n",
    "        \n",
    "            \n",
    "        stages=df.select(\"real_queryid\",\"Stage ID\").distinct().orderBy(\"Stage ID\").groupBy(\"real_queryid\").agg(F.collect_list(\"Stage ID\").alias(\"stages\")).orderBy(\"real_queryid\")\n",
    "        runtimeacc=df.where(\"Event='SparkListenerTaskEnd'\") \\\n",
    "                      .groupBy(\"real_queryid\") \\\n",
    "                      .agg(F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,2).alias(\"acc_task_time\"))\n",
    "        inputsize = df.select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"real_queryid\") \\\n",
    "                      .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read\")).orderBy(\"real_queryid\")\n",
    "        if self.dfacc is not None:\n",
    "            inputsizev1 = self.dfacc.where(\"Name='size of files read'\").groupBy(\"real_queryid\").agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read v1\")).orderBy(\"real_queryid\")\n",
    "            inputsize=inputsize.join(inputsizev1,on=\"real_queryid\",how=\"outer\")\n",
    "            inputsize=inputsize.withColumn(\"input read\",F.coalesce(F.col(\"input read\"),F.col(\"input read v1\"))).drop(\"input read v1\")\n",
    "        \n",
    "        outputrows = df.select(\"real_queryid\",\"Stage ID\",\"Stage ID\",F.explode(\"Accumulables\"))\\\n",
    "                        .select(\"real_queryid\",\"Stage ID\",\"Stage ID\",\"col.*\")\\\n",
    "                        .where(\"Name='number of output rows'\")\\\n",
    "                        .groupBy(\"real_queryid\")\\\n",
    "                        .agg(F.round(F.sum(\"Update\")/1000000000,2).alias(\"output rows\"))\n",
    "        \n",
    "        stages=runtimeacc.join(stages,on=\"real_queryid\",how=\"right\")\n",
    "        stages=inputsize.join(stages,on=\"real_queryid\",how=\"right\")\n",
    "        stages=stages.join(outputrows,on='real_queryid',how=\"left\")\n",
    "        \n",
    "        out=df.groupBy(\"real_queryid\").agg(\n",
    "            F.round(F.max(\"query_endtime\")/1000-F.min(\"query_starttime\")/1000,2).alias(\"runtime\"),\n",
    "            F.round(F.sum(\"Disk Bytes Spilled\")/1024/1024/1024,2).alias(\"disk spilled\"),\n",
    "            F.round(F.sum(\"Memory Bytes Spilled\")/1024/1024/1024,2).alias(\"memspilled\"),\n",
    "            F.round(F.sum(\"Local Bytes Read\")/1024/1024/1024,2).alias(\"local_read\"),\n",
    "            F.round(F.sum(\"Remote Bytes Read\")/1024/1024/1024,2).alias(\"remote_read\"),\n",
    "            F.round(F.sum(\"Shuffle Bytes Written\")/1024/1024/1024,2).alias(\"shuffle_write\"),\n",
    "            F.round(F.sum(\"Executor Deserialize Time\")/1000/self.parallelism,2).alias(\"deser_time\"),\n",
    "            F.round(F.sum(\"Executor Run Time\")/1000/self.parallelism,2).alias(\"run_time\"),\n",
    "            F.round(F.sum(\"Result Serialization Time\")/1000/self.parallelism,2).alias(\"ser_time\"),\n",
    "            F.round(F.sum(\"Fetch Wait Time\")/1000/self.parallelism,2).alias(\"f_wait_time\"),\n",
    "            F.round(F.sum(\"JVM GC Time\")/1000/self.parallelism,2).alias(\"gc_time\"),\n",
    "            F.round(F.max(\"Peak Execution Memory\")/1000000000*self.executor_instances*self.executor_cores,2).alias(\"peak_mem\"),\n",
    "            F.max(\"queryid\").alias(\"queryid\")\n",
    "            ).join(stages,\"real_queryid\",how=\"left\").orderBy(\"real_queryid\").toPandas().set_index(\"real_queryid\")\n",
    "        out[\"executors\"]=self.executor_instances\n",
    "        out[\"core/exec\"]=self.executor_cores\n",
    "        out[\"task.cpus\"]=self.taskcpus\n",
    "        out['parallelism']=self.parallelism\n",
    "        \n",
    "        if not showtable:\n",
    "            return out\n",
    "\n",
    "        def highlight_greater(x):\n",
    "            m1 = x['acc_task_time'] / x['runtime'] * 100\n",
    "            m2 = x['run_time'] / x['runtime'] * 100\n",
    "            m3 = x['f_wait_time'] / x['runtime'] * 100\n",
    "            \n",
    "\n",
    "            df1 = pandas.DataFrame('', index=x.index, columns=x.columns)\n",
    "\n",
    "            df1['acc_task_time'] = m1.apply(lambda x: 'background-image: linear-gradient(to right,#5fba7d {:f}%,white {:f}%)'.format(x,x))\n",
    "            df1['run_time'] = m2.apply(lambda x: 'background-image: linear-gradient(to right,#5fba7d {:f}%,white {:f}%)'.format(x,x))\n",
    "            df1['f_wait_time'] = m3.apply(lambda x: 'background-image: linear-gradient(to right,#d65f5f {:f}%,white {:f}%)'.format(x,x))\n",
    "            return df1\n",
    "\n",
    "\n",
    "        cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "        if plot:\n",
    "            display(out.style.apply(highlight_greater, axis=None).background_gradient(cmap=cm,subset=['input read', 'shuffle_write']))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_query_time_metric(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        querids=self.df.select(\"queryid\").distinct().collect()\n",
    "        for idx,q in enumerate([l[\"queryid\"] for l in querids]):\n",
    "            self.show_time_metric(query=[q,],showexecutor=False)\n",
    "            \n",
    "    def getOperatorCount(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        df=spark.read.json(self.file)\n",
    "        queryids=self.df.select(F.col(\"queryid\").astype(LongType()),F.col(\"real_queryid\")).distinct().orderBy(\"real_queryid\")\n",
    "        queryplans=self.queryplans.collect()\n",
    "        list_queryid=[l.real_queryid for l in queryids.collect()]\n",
    "\n",
    "        def get_child(execid,node):\n",
    "            #wholestagetransformer not counted\n",
    "            if node['nodeName'] is not None and not node['nodeName'].startswith(\"WholeStageCodegenTransformer\"):\n",
    "                if node[\"nodeName\"] not in qps:\n",
    "                    qps[node[\"nodeName\"]]={l:0 for l in list_queryid}\n",
    "                qps[node[\"nodeName\"]][execid]=qps[node[\"nodeName\"]][execid]+1\n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    get_child(execid,c)\n",
    "\n",
    "        qps={}\n",
    "        for c in queryplans:\n",
    "            get_child(c['real_queryid'],c)\n",
    "\n",
    "        return pandas.DataFrame(qps).T.sort_index(axis=0)        \n",
    "    \n",
    "    def get_query_plan(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        stageid=kwargs.get(\"stageid\",None)\n",
    "        \n",
    "        outputstage=kwargs.get(\"outputstage\",None)\n",
    "        \n",
    "        show_plan_only=kwargs.get(\"show_plan_only\",False)\n",
    "        show_simple_string=kwargs.get(\"show_simple_string\",False)\n",
    "\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        colors=[\"#{:02x}{:02x}{:02x}\".format(int(l[0]*255),int(l[1]*255),int(l[2]*255)) for l in matplotlib.cm.get_cmap('tab20').colors]\n",
    "        \n",
    "        if queryid is not None:\n",
    "            if type(queryid)==int or type(queryid)==str:\n",
    "                queryid = [queryid,]\n",
    "            shown_stageid = [l[\"Stage ID\"] for l in self.df.where(F.col(\"real_queryid\").isin(queryid)).select(\"Stage ID\").distinct().collect()]\n",
    "        if stageid is not None:\n",
    "            if type(stageid)==int:\n",
    "                shown_stageid = [stageid,]\n",
    "            elif type(stageid)==list:\n",
    "                shown_stageid = stageid\n",
    "            queryid = [l[\"real_queryid\"] for l in self.df.where(F.col(\"`Stage ID`\").isin(shown_stageid)).select(\"real_queryid\").limit(1).collect()]\n",
    "\n",
    "\n",
    "        queryplans=[]\n",
    "        queryplans = self.queryplans.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"real_queryid\").collect() if queryid else self.queryplans.orderBy(\"real_queryid\").collect()\n",
    "        dfmetric=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"queryid\",\"real_queryid\",\"Stage ID\",\"Job ID\",F.explode(\"Accumulables\").alias(\"metric\")).select(\"*\",\"metric.*\").select(\"Stage ID\",\"ID\",\"Update\").groupBy(\"ID\",\"Stage ID\").agg(F.round(F.sum(\"Update\"),1).alias(\"value\"),F.round(F.stddev(\"Update\"),1).alias(\"stdev\")).collect()\n",
    "        accid2stageid={l.ID:(l[\"Stage ID\"],l[\"value\"],l[\"stdev\"]) for l in dfmetric}\n",
    "\n",
    "        stagetime=self.df.where((F.col(\"real_queryid\").isin(queryid))).where(F.col(\"Event\")=='SparkListenerTaskEnd').groupBy(\"Stage ID\").agg(\n",
    "            F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,1).alias(\"elapsed time\"),\n",
    "            F.round(F.stddev(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000,1).alias(\"time stdev\"),\n",
    "            F.count(F.col(\"Task ID\")).alias(\"partitions\")\n",
    "            ).orderBy(F.desc(\"elapsed time\")).collect()\n",
    "\n",
    "        apptotaltime=reduce(lambda x,y: x+y['elapsed time'], stagetime,0)\n",
    "        if apptotaltime==0:\n",
    "            display(HTML(\"<font size=4 color=red>Error, totaltime is 0 </font>\"))\n",
    "            apptotaltime=1\n",
    "            return \"\"\n",
    "\n",
    "        stagemap={l[\"Stage ID\"]:l[\"elapsed time\"] for l in stagetime}\n",
    "        stage_time_stdev_map={l[\"Stage ID\"]:l[\"time stdev\"] for l in stagetime}\n",
    "        stagepartmap={l[\"Stage ID\"]:l[\"partitions\"] for l in stagetime}\n",
    "\n",
    "        keystage=[]\n",
    "        keystagetime=[]\n",
    "        subtotal=0\n",
    "        for s in stagetime:\n",
    "            subtotal=subtotal+s['elapsed time']\n",
    "            keystage.append(s['Stage ID'])\n",
    "            keystagetime.append(s['elapsed time'])\n",
    "            if subtotal/apptotaltime>0.9:\n",
    "                break\n",
    "        keystagetime=[\"{:02x}{:02x}\".format(int(255*l/keystagetime[0]),255-int(255*l/keystagetime[0])) for l in keystagetime if keystagetime[0]>0]\n",
    "        keystagemap=dict(zip(keystage,keystagetime))\n",
    "        outstr=[]\n",
    "        def print_plan(real_queryid,level,node,parent_stageid):\n",
    "            stageid = accid2stageid[int(node[\"metrics\"][0][\"accumulatorId\"])][0]  if node[\"metrics\"] is not None and len(node[\"metrics\"])>0 and node[\"metrics\"][0][\"accumulatorId\"] in accid2stageid else parent_stageid\n",
    "\n",
    "            if stageid in shown_stageid:\n",
    "                fontcolor=f\"color:#{keystagemap[stageid]}00;font-weight:bold\" if stageid in keystagemap else \"color:#000000\"\n",
    "                stagetime=0 if stageid not in stagemap else stagemap[stageid]\n",
    "                stageParts=0 if stageid not in stagepartmap else stagepartmap[stageid]\n",
    "\n",
    "                input_rowcntstr=\"\"\n",
    "                output_rowcntstr=\"\"\n",
    "                timename={}\n",
    "                input_columnarbatch=\"\"\n",
    "                output_columnarbatch=\"\"\n",
    "                output_row_batch=\"\"\n",
    "                other_metric_name={}\n",
    "\n",
    "                outputrows=0\n",
    "                outputbatches=0\n",
    "                if node[\"metrics\"] is not None:\n",
    "                    for m in node[\"metrics\"]:\n",
    "\n",
    "                        if m[\"accumulatorId\"] not in accid2stageid:\n",
    "                            continue\n",
    "                        \n",
    "                        if m[\"name\"].endswith(\"block wall nanos\") or m['name'].endswith(\"cpu nanos\"):\n",
    "                            continue\n",
    "                            \n",
    "                        \n",
    "                        value=accid2stageid[m[\"accumulatorId\"]][1]\n",
    "                        stdev_value=accid2stageid[m[\"accumulatorId\"]][2]\n",
    "                        stdev_value=0 if stdev_value is None else stdev_value\n",
    "                        if m[\"metricType\"] in ['nsTiming','timing']:\n",
    "                            totaltime=value/1000 if  m[\"metricType\"] == 'timing' else value/1000000000\n",
    "                            stdev_value=stdev_value/1000 if  m[\"metricType\"] == 'timing' else stdev_value/1000000000\n",
    "                            \n",
    "                            timeratio= 0  if stagetime==0 else totaltime/self.executor_instances/self.executor_cores*self.taskcpus/stagetime*100\n",
    "                            timeratio_query = totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100\n",
    "                            if timeratio > 10 or timeratio_query>10:\n",
    "                                timename[m[\"name\"]]=\"<font style='background-color:#ffff42'>{:,.2f}s ({:.1f}%, {:.1f}%, {:,.2f})</font>\".format(totaltime,timeratio, totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100,stdev_value)\n",
    "                            else:\n",
    "                                timename[m[\"name\"]]=\"{:,.2f}s ({:.1f}%, {:.1f}%, {:,.2f})\".format(totaltime,timeratio, totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100,stdev_value)\n",
    "                        elif m[\"name\"] in [\"number of output rows\",\"number of final output rows\"]:\n",
    "                            output_rowcntstr=\"{:,.1f}\".format(value/1000/1000)+\" M\"\n",
    "                            outputrows=value\n",
    "                        elif m[\"name\"] in [\"number of output columnar batches\",\"number of output batches\",\"output_batches\", \"number of output vectors\",\"number of final output vectors\", \"records read\"]: \n",
    "                            # records reads is the output of shuffle\n",
    "                            output_columnarbatch=\"{:,d}\".format(int(value))\n",
    "                            outputbatches=value\n",
    "                        elif m[\"name\"]==\"number of input rows\":\n",
    "                            input_rowcntstr=\"{:,.1f}\".format(value/1000/1000)+\" M\"\n",
    "                        elif m[\"name\"] in [\"number of input batches\",\"input_batches\",\"number of input vectors\"]:\n",
    "                            input_columnarbatch=\"{:,d}\".format(int(value))\n",
    "                        else:\n",
    "                            if value>1000000000:\n",
    "                                other_metric_name[m[\"name\"]]=\"{:,.1f} G ({:,.1f})\".format(value/1000000000,stdev_value/1000000000)\n",
    "                            elif value>1000000:\n",
    "                                other_metric_name[m[\"name\"]]=\"{:,.1f} M ({:,.1f})\".format(value/1000000,stdev_value/1000000)\n",
    "                            elif value>1000:\n",
    "                                other_metric_name[m[\"name\"]]=\"{:,.1f} K ({:,.1f})\".format(value/1000,stdev_value/1000)\n",
    "                            else:\n",
    "                                other_metric_name[m[\"name\"]]=\"{:,d} ({:,.1f})\".format(int(value),stdev_value)\n",
    "\n",
    "\n",
    "                if outputrows>0 and outputbatches>0:\n",
    "                    output_row_batch=\"{:,d}\".format(int(outputrows/outputbatches))\n",
    "\n",
    "\n",
    "                fontcolor=f\"color:#{keystagemap[stageid]}00;font-weight:bold\" if stageid in keystage else \"color:#000000\"\n",
    "                stagetime=0 if stageid not in stagemap else stagemap[stageid]\n",
    "                stage_time_stdev=0 if stageid not in stage_time_stdev_map else stage_time_stdev_map[stageid]\n",
    "                \n",
    "                nodenamestr=node[\"nodeName\"]\n",
    "                if nodenamestr is None:\n",
    "                    nodenamestr=\"\"\n",
    "                if nodenamestr in ['ColumnarToRow','RowToArrowColumnar','ArrowColumnarToRow','ArrowRowToColumnarExec','GlutenColumnarToRowExec','GlutenRowToArrowColumnar']:\n",
    "                    nodename='<span style=\"color: green; background-color: #ffff42\">'+nodenamestr+'</span>'\n",
    "                else:\n",
    "                    nodename=nodenamestr\n",
    "                if outputstage is not None:\n",
    "                    outputstage.append({\"queryid\":real_queryid,\"stageid\":stageid,\"stagetime\":stagetime,\"stageParts\":stageParts,\"nodename\":nodenamestr,\"output_rowcnt\":outputrows,\"nodename_level\":\" \".join([\"|_\" for l in range(0,level)]) + \" \" + nodenamestr})\n",
    "                if not show_plan_only:\n",
    "                    nodestr= \" \".join([\"|_\" for l in range(0,level)]) + \" \" + nodename\n",
    "                    if show_simple_string :\n",
    "                        simstr=node['simpleString']\n",
    "                        nodestr = nodestr + \"<br>\\n\" +  simstr                                                                 \n",
    "                    \n",
    "                    timenametable='<table  style=\"width:100%\">\\n'\n",
    "                    \n",
    "                    timenameSort=list(timename)\n",
    "                    \n",
    "                    for nameidx in sorted(timename):\n",
    "                        timenametable+=f\"<tr><td>{nameidx}</td><td>{timename[nameidx]}</td></tr>\"\n",
    "                    timenametable+=\"</table>\\n\"\n",
    "                    \n",
    "                    \n",
    "                    othertable='<table style=\"width:100%\">\\n'\n",
    "                    for nameidx in sorted(other_metric_name):\n",
    "                        othertable+=f\"<tr><td>{nameidx}</td><td>{other_metric_name[nameidx]}</td></tr>\"\n",
    "                    othertable+=\"</table>\\n\"\n",
    "                    \n",
    "                    outstr.append(f\"<tr><td style='{fontcolor}'>{stageid}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stagetime}({stage_time_stdev}) </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stageParts} </td>\"+\n",
    "                                  f\"<td style='text-align:left; background-color:{colors[stageid % 20]}'>\" + nodestr + f\"</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {input_rowcntstr} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {input_columnarbatch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_rowcntstr} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_columnarbatch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_row_batch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}' colspan=2> {timenametable} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}' colspan=2> {othertable} </td>\"+\n",
    "                                  \"</tr>\")\n",
    "                else:\n",
    "                    outstr.append(f\"<tr><td style='{fontcolor}'>{stageid}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stagetime} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stageParts} </td>\"+\n",
    "                                  f\"<td style='text-align:left; background-color:{colors[stageid % 20]}'>\" + \" \".join([\"|_\" for l in range(0,level)]) + \" \" + nodename + f\"</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_rowcntstr} </td></tr>\")\n",
    "                    \n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    print_plan(real_queryid, level+1,c,stageid)\n",
    "\n",
    "        for c in queryplans:\n",
    "            outstr.append(\"<font color=red size=4>\"+str(c['real_queryid'])+\"</font><table>\")\n",
    "            if not show_plan_only:\n",
    "                outstr.append('''<tr>\n",
    "                                    <td>stage id</td>\n",
    "                                    <td>stage time</td>\n",
    "                                    <td>partions</td>\n",
    "                                    <td>operator</td>\n",
    "                                    <td>input rows</td>\n",
    "                                    <td>input batches</td>\n",
    "                                    <td>output rows</td>\n",
    "                                    <td>output batches</td>\n",
    "                                    <td>output rows/batch</td>\n",
    "                                    <td width=150>time metric name</td>\n",
    "                                    <td width=200>time(%stage,%total,stdev)</td>\n",
    "                                    <td width=150>other metric name</td>\n",
    "                                    <td width=130>value(stdev)</td>\n",
    "                                </tr>''')\n",
    "            else:\n",
    "                outstr.append('''<tr>\n",
    "                                    <td>stage id</td>\n",
    "                                    <td>stage time</td>\n",
    "                                    <td>partions</td>\n",
    "                                    <td>operator</td>\n",
    "                                    <td>output rows</td>\n",
    "                                </tr>''')\n",
    "\n",
    "            print_plan(c['real_queryid'],0,c,0)\n",
    "            outstr.append(\"</table>\")\n",
    "        if plot:\n",
    "            display(HTML(\" \".join(outstr)))\n",
    "        return \" \".join(outstr)\n",
    "    \n",
    "    def get_metric_output_rowcnt(self, **kwargs):\n",
    "        return self.get_metric_rowcnt(\"number of output rows\",**kwargs)\n",
    "        \n",
    "    def get_metric_input_rowcnt(self, **kwargs):\n",
    "        return self.get_metric_rowcnt(\"number of input rows\",**kwargs)\n",
    "        \n",
    "    def get_metric_rowcnt(self,rowname, **kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        stageid=kwargs.get(\"stageid\",None)\n",
    "        show_task=kwargs.get(\"show_task\",False)\n",
    "        \n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "            \n",
    "        if stageid and type(stageid)==int:\n",
    "            stageid = [stageid,]\n",
    "            \n",
    "        queryplans = self.queryplans.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"real_queryid\").collect() if queryid else self.queryplans.orderBy(\"real_queryid\").collect()\n",
    "        qps=[]\n",
    "\n",
    "        rownames=rowname if type(rowname)==list else [rowname,]\n",
    "        def get_child(execid,node):\n",
    "            if node['metrics'] is not None:\n",
    "                outputrows=[x for x in node[\"metrics\"] if \"name\" in x and x[\"name\"] in rownames]\n",
    "                if len(outputrows)>0:\n",
    "                    qps.append([node[\"nodeName\"],execid,outputrows[0]['accumulatorId']])\n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    get_child(execid,c)\n",
    "        for c in queryplans:\n",
    "            get_child(c['real_queryid'],c)\n",
    "\n",
    "        if len(qps)==0:\n",
    "            print(\"Metric \",rowname,\" is not found. \")\n",
    "            return None\n",
    "        stagetime=self.df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"Stage ID\").agg(F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,2).alias(\"stage time\"))\n",
    "        dfmetric=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"queryid\",\"real_queryid\",\"Stage ID\",\"Job ID\",F.explode(\"Accumulables\").alias(\"metric\")).select(\"*\",\"metric.*\").drop(\"metric\")\n",
    "        numrowmetric=spark.createDataFrame(qps)\n",
    "        numrowmetric=numrowmetric.withColumnRenamed(\"_1\",\"metric\").withColumnRenamed(\"_2\",\"real_queryid\").withColumnRenamed(\"_3\",\"metricid\")\n",
    "        dfmetric_rowcnt=dfmetric.join(numrowmetric.drop(\"real_queryid\"),on=[F.col(\"metricid\")==F.col(\"ID\")],how=\"right\")\n",
    "        if show_task:\n",
    "            stagemetric=dfmetric_rowcnt.join(stagetime,\"Stage ID\")\n",
    "        else:\n",
    "            stagemetric=dfmetric_rowcnt.groupBy(\"queryid\",\"real_queryid\",\"Job ID\",\"Stage ID\",\"metricid\").agg(F.round(F.sum(\"Update\")/1000000,2).alias(\"total_row\"),F.max(\"metric\").alias(\"nodename\")).join(stagetime,\"Stage ID\")\n",
    "\n",
    "        if queryid:\n",
    "            if stageid:\n",
    "                return stagemetric.where(F.col(\"real_queryid\").isin(queryid) & F.col(\"Stage ID\").isin(stageid)).orderBy(\"Stage ID\")\n",
    "            else:\n",
    "                return stagemetric.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"Stage ID\")\n",
    "        else:\n",
    "            noderow=stagemetric.groupBy(\"real_queryid\",\"nodename\").agg(F.round(F.sum(\"total_row\"),2).alias(\"total_row\")).orderBy(\"nodename\").collect()\n",
    "            out={}\n",
    "            qids=set([r.real_queryid for r in noderow])\n",
    "            for r in noderow:\n",
    "                if r.nodename not in out:\n",
    "                    out[r.nodename]={c:0 for c in qids}\n",
    "                out[r.nodename][r.real_queryid]=r.total_row\n",
    "            return pandas.DataFrame(out).T.sort_index(axis=0)\n",
    "    \n",
    "    def get_query_info(self,queryid):\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> time stat info </b></font>\",))\n",
    "        tmp=self.get_query_time(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> stage stat info </b></font>\",))\n",
    "        display(self.get_stage_stat(queryid=queryid))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> query plan </b></font>\",))\n",
    "        self.get_query_plan(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> stage hist info </b></font>\",))\n",
    "        self.show_Stages_hist(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> time info </b></font>\",))\n",
    "        display(self.show_time_metric(queryid=queryid))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator and rowcount </b></font>\",))\n",
    "        display(self.get_metric_input_rowcnt(queryid=queryid))\n",
    "        display(self.get_metric_output_rowcnt(queryid=queryid))\n",
    "        \n",
    "    def get_app_info(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        display(HTML(f\"<font color=red size=7 face='Courier New'><b> {self.appid} </b></font>\",))\n",
    "        display(HTML(f\"<a href=http://sr525:18080/history/{self.appid}>http://sr525:18080/history/{self.appid}</a>\"))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> query time </b></font>\",))\n",
    "        tmp=self.get_query_time(**kwargs)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator count </b></font>\",))\n",
    "        pdf=self.getOperatorCount()\n",
    "        display(pdf.style.apply(background_gradient,\n",
    "               cmap='OrRd',\n",
    "               m=pdf.min().min(),\n",
    "               M=pdf.max().max(),\n",
    "               low=0,\n",
    "               high=1))\n",
    "        \n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator input row count </b></font>\",))\n",
    "        pdf=self.get_metric_input_rowcnt(**kwargs)\n",
    "        if pdf is not None:\n",
    "            display(pdf.style.apply(background_gradient,\n",
    "                   cmap='OrRd',\n",
    "                   m=pdf.min().min(),\n",
    "                   M=pdf.max().max(),\n",
    "                   low=0,\n",
    "                   high=1))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator output row count </b></font>\",))\n",
    "        pdf=self.get_metric_output_rowcnt(**kwargs)\n",
    "        if pdf is not None:\n",
    "            display(pdf.style.apply(background_gradient,\n",
    "                   cmap='OrRd',\n",
    "                   m=pdf.min().min(),\n",
    "                   M=pdf.max().max(),\n",
    "                   low=0,\n",
    "                   high=1))\n",
    "        self.show_time_metric(**kwargs)\n",
    "        \n",
    "    def get_stage_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "\n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "            \n",
    "        df=self.df.where(F.col(\"real_queryid\").isin(queryid)).where(F.col(\"Event\")=='SparkListenerTaskEnd')\n",
    "        \n",
    "        inputsize = df.select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"Stage ID\") \\\n",
    "                      .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read\"))\n",
    "        \n",
    "        return df.groupBy(\"Job ID\",\"Stage ID\").agg(\n",
    "            F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,1).alias(\"elapsed time\"),\n",
    "            F.round(F.sum(F.col(\"Disk Bytes Spilled\"))/1024/1024/1024,1).alias(\"disk spilled\"),\n",
    "            F.round(F.sum(F.col(\"Memory Bytes Spilled\"))/1024/1024/1024,1).alias(\"mem spilled\"),\n",
    "            F.round(F.sum(F.col(\"Local Bytes Read\"))/1024/1024/1024,1).alias(\"local read\"),\n",
    "            F.round(F.sum(F.col(\"Remote Bytes Read\"))/1024/1024/1024,1).alias(\"remote read\"),\n",
    "            F.round(F.sum(F.col(\"Shuffle Bytes Written\"))/1024/1024/1024,1).alias(\"shuffle write\"),\n",
    "            F.round(F.sum(F.col(\"Executor Deserialize Time\"))/1000,1).alias(\"deseri time\"),\n",
    "            F.round(F.sum(F.col(\"Fetch Wait Time\"))/1000,1).alias(\"fetch wait time\"),\n",
    "            F.round(F.sum(F.col(\"Shuffle Write Time\"))/1000000000,1).alias(\"shuffle write time\"),\n",
    "            F.round(F.sum(F.col(\"Result Serialization Time\"))/1000,1).alias(\"seri time\"),\n",
    "            F.round(F.sum(F.col(\"Getting Result Time\"))/1000,1).alias(\"get result time\"),\n",
    "            F.round(F.sum(F.col(\"JVM GC Time\"))/1000,1).alias(\"gc time\"),\n",
    "            F.round(F.sum(F.col(\"Executor CPU Time\"))/1000000000,1).alias(\"exe cpu time\")    \n",
    "            ).join(inputsize,on=[\"Stage ID\"],how=\"left\").orderBy(\"Stage ID\").toPandas()\n",
    "    \n",
    "    def get_metrics_by_node(self,node_name):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        \n",
    "        if type(node_name)==str:\n",
    "            node_name=[node_name]\n",
    "        metrics=self.queryplans.collect()\n",
    "        coalesce=[]\n",
    "        metricsid=[0]\n",
    "        def get_metric(root):\n",
    "            if root['nodeName'] in node_name:\n",
    "                metricsid[0]=metricsid[0]+1\n",
    "                for l in root[\"metrics\"]:\n",
    "                    coalesce.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"],metricsid[0]])\n",
    "            if root[\"children\"] is not None:\n",
    "                for c in root[\"children\"]:\n",
    "                    get_metric(c)\n",
    "        for c in metrics:\n",
    "            get_metric(c)\n",
    "\n",
    "        df=self.df.select(\"queryid\",\"real_queryid\",'Stage ID','Task ID','Job ID',F.explode(\"Accumulables\"))\n",
    "        df=df.select(\"*\",\"col.*\")\n",
    "        metricdf=spark.createDataFrame(coalesce)\n",
    "        metricdf=metricdf.withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"Unit\").withColumnRenamed(\"_3\",\"metricName\").withColumnRenamed(\"_4\",\"nodeName\").withColumnRenamed(\"_5\",\"nodeID\")\n",
    "        df=df.join(metricdf,on=[\"ID\"],how=\"right\")\n",
    "        shufflemetric=set(l[2] for l in coalesce)\n",
    "        metricdfs=[df.where(F.col(\"Name\")==l).groupBy(\"real_queryid\",\"nodeID\",\"Stage ID\").agg(F.stddev(\"Update\").alias(l+\"_stddev\"),F.mean(\"Update\").alias(l+\"_mean\"),F.mean(\"Update\").alias(l) if l.startswith(\"avg\") else F.sum(\"Update\").alias(l)) for l in shufflemetric]\n",
    "        \n",
    "        stagetimedf=self.df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"Stage ID\").agg(F.count(\"*\").alias(\"partnum\"),F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000,2).alias(\"ElapsedTime\"))\n",
    "        \n",
    "        nodemetric=reduce(lambda x,y: x.join(y, on=['nodeID',\"Stage ID\",\"real_queryid\"],how=\"full\"),metricdfs)\n",
    "        return nodemetric.join(stagetimedf,on=\"Stage ID\")\n",
    "    \n",
    "    \n",
    "    def get_coalesce_batch_row_cnt(self,**kwargs):\n",
    "        stagesum=self.get_metrics_by_node(\"CoalesceBatches\")\n",
    "        \n",
    "        pandas.options.display.float_format = '{:,}'.format\n",
    "        \n",
    "        stagesum=stagesum.withColumnRenamed(\"number of output rows\",\"rows\")\n",
    "        \n",
    "        coalescedf = stagesum.orderBy(\"real_queryid\",'Stage ID').where(\"rows>4000\").toPandas()\n",
    "        \n",
    "        coalescedf[\"row/input_batch\"] = coalescedf[\"rows\"]/coalescedf[\"input_batches\"]\n",
    "        coalescedf[\"row/out_batch\"] = coalescedf[\"rows\"]/coalescedf[\"output_batches\"]\n",
    "        coalescedf['stage']=coalescedf[\"real_queryid\"].astype(str)+\"_\"+coalescedf['Stage ID'].astype(str)\n",
    "        \n",
    "        ax=coalescedf.plot(y=[\"row/input_batch\",\"row/out_batch\"],figsize=(30,8),style=\"-*\")\n",
    "        coalescedf.plot(ax=ax,y=['rows'],secondary_y=['rows'],style=\"k_\")\n",
    "        self.print_real_queryid(ax,coalescedf)\n",
    "        \n",
    "        return coalescedf\n",
    "    \n",
    "    def print_real_queryid(self,ax,dataset):\n",
    "        ax.axes.get_xaxis().set_ticks([])\n",
    "\n",
    "        ymin, ymax = ax.get_ybound()\n",
    "\n",
    "        real_queryid=list(dataset['real_queryid'])\n",
    "        s=real_queryid[0]\n",
    "        lastx=0\n",
    "        for idx,v in enumerate(real_queryid):\n",
    "            if v!=s:\n",
    "                xmin = xmax = idx-1+0.5\n",
    "                l = mlines.Line2D([xmin,xmax], [ymin,ymax],color=\"green\")\n",
    "                ax.add_line(l)\n",
    "                ax.text(lastx+(xmin-lastx)/2-0.25,ymin-(ymax-ymin)/20,f\"{s}\",size=20)\n",
    "                s=v\n",
    "                lastx=xmin\n",
    "\n",
    "    def get_shuffle_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        shufflesize=kwargs.get(\"shuffle_size\",1000000)\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        if queryid is not None:\n",
    "            if type(queryid) is str or type(queryid) is int:\n",
    "                queryid=[queryid,]\n",
    "\n",
    "        exchangedf=self.get_metrics_by_node([\"ColumnarExchange\",\"ColumnarExchangeAdaptor\"])\n",
    "        exchangedf.cache()\n",
    "        exchangedf.count()\n",
    "        mapdf=exchangedf.where(\"`totaltime to split` is not null\").select(\"nodeID\",F.col(\"Stage ID\").alias(\"map_stageid\"),\"real_queryid\",F.floor(F.col(\"totaltime to split\")/F.col(\"totaltime to split_mean\")).alias(\"map_partnum\"),\"totaltime to compress\",\"totaltime to split\",\"shuffle write time\",\"totaltime to spill\",'shuffle records written','data size','shuffle bytes written','shuffle bytes written_mean','shuffle bytes written_stddev','shuffle bytes spilled','number of input rows','number of input batches')\n",
    "        reducerdf=exchangedf.where(\"`totaltime to split` is null\").select(\"nodeID\",F.col(\"Stage ID\").alias(\"reducer_stageid\"),\"real_queryid\",'local blocks read','local bytes read',F.floor(F.col(\"records read\")/F.col(\"records read_mean\")).alias(\"reducer_partnum\"),(F.col('avg read batch num rows')/10).alias(\"avg read batch num rows\"),'remote bytes read','records read','remote blocks read',(F.col(\"number of output rows\")/F.col(\"records read\")).alias(\"avg rows per split recordbatch\"))\n",
    "        shuffledf=mapdf.join(reducerdf,on=[\"nodeID\",\"real_queryid\"],how=\"full\")\n",
    "        if queryid is not None:\n",
    "            shuffledf=shuffledf.where(F.col(\"real_queryid\").isin(queryid))\n",
    "        shuffle_pdf=shuffledf.where(\"`shuffle bytes written`>1000000\").orderBy(\"real_queryid\",\"map_stageid\",\"nodeID\").toPandas()\n",
    "        shuffle_pdf[\"shuffle bytes written\"]=shuffle_pdf[\"shuffle bytes written\"]/1000000000\n",
    "        shuffle_pdf[\"data size\"]=shuffle_pdf[\"data size\"]/1000000000\n",
    "        shuffle_pdf[\"shuffle bytes written_mean\"]=shuffle_pdf[\"shuffle bytes written_mean\"]/1000000\n",
    "        shuffle_pdf[\"shuffle bytes written_stddev\"]=shuffle_pdf[\"shuffle bytes written_stddev\"]/1000000\n",
    "        ax=shuffle_pdf.plot(y=[\"avg read batch num rows\",'avg rows per split recordbatch'],figsize=(30,8),style=\"-*\",title=\"average batch size after split\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf[\"split_ratio\"]=shuffle_pdf[\"records read\"]/shuffle_pdf['number of input batches']\n",
    "        ax=shuffle_pdf.plot(y=[\"split_ratio\",\"records read\"],secondary_y=[\"records read\"],figsize=(30,8),style=\"-*\",title=\"Split Ratio\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf[\"compress_ratio\"]=shuffle_pdf[\"data size\"]/shuffle_pdf['shuffle bytes written']\n",
    "        ax=shuffle_pdf.plot(y=[\"shuffle bytes written\",\"compress_ratio\"],secondary_y=[\"compress_ratio\"],figsize=(30,8),style=\"-*\",title=\"compress ratio\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shufflewritepdf=shuffle_pdf\n",
    "        ax=shufflewritepdf.plot.bar(y=[\"shuffle write time\",\"totaltime to spill\",\"totaltime to compress\",\"totaltime to split\"],stacked=True,figsize=(30,8),title=\"split time + shuffle write time vs. shuffle bytes written\")\n",
    "        ax=shufflewritepdf.plot(ax=ax,y=[\"shuffle bytes written\"],secondary_y=[\"shuffle bytes written\"],style=\"-*\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        shuffle_pdf['avg input batch size']=shuffle_pdf[\"number of input rows\"]/shuffle_pdf[\"number of input batches\"]\n",
    "        ax=shuffle_pdf.plot(y=[\"avg input batch size\"],figsize=(30,8),style=\"b-*\",title=\"average input batch size\")\n",
    "        ax=shuffle_pdf.plot.bar(ax=ax,y=['number of input rows'],secondary_y=True)\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        \n",
    "        metrics=self.queryplans.collect()\n",
    "        coalesce=[]\n",
    "        metricsid=[0]\n",
    "        def get_metric(root):\n",
    "            if root['nodeName'] in [\"ColumnarExchange\",\"ColumnarExchangeAdaptor\"]:\n",
    "                metricsid[0]=metricsid[0]+1\n",
    "                for l in root[\"metrics\"]:\n",
    "                    coalesce.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"],metricsid[0],root[\"simpleString\"]])\n",
    "            if root[\"children\"] is not None:\n",
    "                for c in root[\"children\"]:\n",
    "                    get_metric(c)\n",
    "        for c in metrics:\n",
    "            get_metric(c)\n",
    "\n",
    "        tps={}\n",
    "        for r in coalesce:\n",
    "            rx=re.search(r\"\\[OUTPUT\\] List\\((.*)\\)\",r[5])\n",
    "            if rx:\n",
    "                if r[4] not in tps:\n",
    "                    tps[r[4]]={}\n",
    "                    fds=rx.group(1).split(\", \")\n",
    "                    for f in fds:\n",
    "                        if f.endswith(\"Type\"):\n",
    "                            tp=re.search(r\":(.+Type)\",f).group(1)\n",
    "                            if tp not in tps[r[4]]:\n",
    "                                tps[r[4]][tp]=1\n",
    "                            else:\n",
    "                                tps[r[4]][tp]+=1\n",
    "        if len(tps)>0:\n",
    "            typedf=pandas.DataFrame(tps).T.reset_index()\n",
    "            typedf=typedf.fillna(0)\n",
    "            shuffle_pdf=pandas.merge(shuffle_pdf,typedf,left_on=\"nodeID\",right_on=\"index\")\n",
    "            shufflewritepdf=shuffle_pdf\n",
    "            ax=shufflewritepdf.plot.bar(y=[\"number of input rows\"],stacked=True,figsize=(30,8),title=\"rows vs. shuffle data type\")\n",
    "            ax=shufflewritepdf.plot(ax=ax,y=list(typedf.columns[1:]),secondary_y=list(typedf.columns[1:]),style=\"-o\")\n",
    "            self.print_real_queryid(ax,shufflewritepdf)\n",
    "            ax=shufflewritepdf.plot.bar(y=[\"totaltime to split\"],stacked=True,figsize=(30,8),title=\"split time vs. shuffle data type\")\n",
    "            ax=shufflewritepdf.plot(ax=ax,y=list(typedf.columns[1:]),secondary_y=list(typedf.columns[1:]),style=\"-o\")\n",
    "            self.print_real_queryid(ax,shufflewritepdf)\n",
    "\n",
    "        \n",
    "        \n",
    "        shufflewritepdf.plot(x=\"shuffle bytes written\",y=[\"shuffle write time\",\"totaltime to split\"],figsize=(30,8),style=\"*\")\n",
    "        shufflewritepdf[\"avg shuffle batch size after split\"]=shufflewritepdf[\"shuffle bytes written\"]*1000000/shufflewritepdf['records read']\n",
    "        shufflewritepdf[\"avg raw batch size after split\"]=shufflewritepdf[\"data size\"]*1000000/shufflewritepdf['records read']\n",
    "        ax=shufflewritepdf.plot(y=[\"avg shuffle batch size after split\",\"avg raw batch size after split\",\"shuffle bytes written\"],secondary_y=[\"shuffle bytes written\"],figsize=(30,8),style=\"-*\",title=\"avg batch KB after split\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        shufflewritepdf[\"avg batch# per splitted partition\"]=shufflewritepdf['records read']/(shufflewritepdf['local blocks read']+shufflewritepdf['remote blocks read'])\n",
    "        ax=shufflewritepdf.plot(y=[\"avg batch# per splitted partition\",'records read'],secondary_y=['records read'],figsize=(30,8),style=\"-*\",title=\"avg batch# per splitted partition\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        fig, ax = plt.subplots(figsize=(30,8))\n",
    "        ax.set_title('shuffle wite bytes with stddev')\n",
    "        ax.errorbar(x=shuffle_pdf.index,y=shuffle_pdf['shuffle bytes written_mean'], yerr=shuffle_pdf['shuffle bytes written_stddev'], linestyle='None', marker='o')\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf['record batch per mapper per reducer']=shuffle_pdf['records read']/(shuffle_pdf[\"map_partnum\"]*shuffle_pdf['reducer_partnum'])\n",
    "        ax=shuffle_pdf.plot(y=[\"record batch per mapper per reducer\"],figsize=(30,8),style=\"b-*\",title=\"record batch per mapper per reducer\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        \n",
    "        inputsize = self.df.select(\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "              .select(\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "              .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "              .groupBy(\"Task ID\") \\\n",
    "              .agg((F.sum(\"Update\")).alias(\"input read\"))\n",
    "        stageinput=self.df.where(\"event='SparkListenerTaskEnd'\" )\\\n",
    "                                .join(inputsize,on=[\"Task ID\"],how=\"left\")\\\n",
    "                                .fillna(0) \\\n",
    "                                .select(F.col('Host'), F.col(\"real_queryid\"),F.col('Stage ID'),F.col('Task ID'),\n",
    "                                        F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),\n",
    "                                        F.round((F.col('`input read`')+F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input'))\n",
    "        baisstage=stageinput.groupBy(\"real_queryid\",\"Stage ID\").agg(F.mean(\"elapsedtime\").alias(\"elapsed\"),F.mean(\"input\").alias(\"input\"),\n",
    "                                                            (F.stddev(\"elapsedtime\")).alias(\"elapsedtime_err\"),\n",
    "                                                            (F.stddev(\"input\")).alias(\"input_err\"),\n",
    "                                                            (F.max(\"elapsedtime\")-F.mean(\"elapsedtime\")).alias(\"elapsed_max\"),\n",
    "                                                            (F.mean(\"elapsedtime\")-F.min(\"elapsedtime\")).alias(\"elapsed_min\"),\n",
    "                                                            (F.max(\"input\")-F.mean(\"input\")).alias(\"input_max\"),\n",
    "                                                            (F.mean(\"input\")-F.min(\"input\")).alias(\"input_min\")).orderBy(\"real_queryid\",\"Stage ID\")\n",
    "        dfx=baisstage.toPandas()\n",
    "        fig, ax = plt.subplots(figsize=(30,8))\n",
    "        ax.set_title('input size')\n",
    "        ax.errorbar(x=dfx.index,y=dfx['input'], yerr=dfx['input_err'], fmt='ok', ecolor='red', lw=3)\n",
    "        ax.errorbar(x=dfx.index,y=dfx['input'],yerr=[dfx['input_min'],dfx['input_max']],\n",
    "                     fmt='.k', ecolor='gray', lw=1)\n",
    "        self.print_real_queryid(ax,dfx)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(30,8))\n",
    "        ax.set_title('stage time')\n",
    "\n",
    "        ax.errorbar(x=dfx.index,y=dfx['elapsed'], yerr=dfx['elapsedtime_err'], fmt='ok', ecolor='red', lw=5)\n",
    "        ax.errorbar(x=dfx.index,y=dfx['elapsed'],yerr=[dfx['elapsed_min'],dfx['elapsed_max']],\n",
    "                     fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "        self.print_real_queryid(ax,dfx)\n",
    "        return (shuffle_pdf,dfx)\n",
    "    \n",
    "    def get_stages_w_odd_partitions(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        return appals.df.where(\"Event='SparkListenerTaskEnd'\")\\\n",
    "                    .groupBy(\"Stage ID\",\"real_queryid\")\\\n",
    "                    .agg((F.sum(F.col('Finish Time')-F.col('Launch Time'))/1000).alias(\"elapsed time\"),\n",
    "                         F.count('*').alias('partitions'))\\\n",
    "                    .where(F.col(\"partitions\")%(appals.executor_cores*appals.executor_instances/appals.taskcpus)!=0)\\\n",
    "                    .orderBy(F.desc(\"elapsed time\")).toPandas()\n",
    "   \n",
    "    def get_scaned_column_v1(appals):\n",
    "        def get_scans(node):\n",
    "            if node['nodeName'].startswith(\"Scan arrow\"):\n",
    "                scans.append(node)\n",
    "            for c in node['children']:\n",
    "                get_scans(c)\n",
    "\n",
    "        alltable=[]\n",
    "        for qid in range(1,23):\n",
    "            scans=[]\n",
    "            plans=appals.queryplans.where(\"real_queryid=\"+str(qid)).collect()\n",
    "            get_scans(plans[0])\n",
    "            for s in scans:\n",
    "                alltable.append([qid,\",\".join([l.split(\":\")[0] for l in re.split(r'[<>]',s['metadata']['ReadSchema'])[1].split(\",\")])])\n",
    "        return alltable\n",
    "    \n",
    "    def get_scaned_column_v2(appals):\n",
    "        def get_scans(node):\n",
    "            if node['nodeName'].startswith(\"ColumnarBatchScan\"):\n",
    "                scans.append(node)\n",
    "            for c in node['children']:\n",
    "                get_scans(c)\n",
    "\n",
    "        alltable=[]\n",
    "        for qid in range(1,23):\n",
    "            scans=[]\n",
    "            plans=appals.queryplans.where(\"real_queryid=\"+str(qid)).collect()\n",
    "            get_scans(plans[0])\n",
    "            for s in scans:\n",
    "                alltable.append([qid,\",\".join([l.split(\"#\")[0] for l in re.split(r\"[\\[\\]]\",s['simpleString'])[1].split(\",\")])])\n",
    "        return alltable\n",
    "    \n",
    "    def compare_query(appals,queryid,appbaseals):\n",
    "        print(f\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~Query{queryid}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        appals.show_critical_path_time_breakdown(queryid=22)\n",
    "        s1=appals.get_stage_stat(queryid=queryid)\n",
    "        s2=appbaseals.get_stage_stat(queryid=queryid)\n",
    "        ls=s1[['Stage ID','elapsed time']]\n",
    "        ls.columns=['l sid','l time']\n",
    "        rs=s2[['Stage ID','elapsed time']]\n",
    "        rs.columns=['r sid','r time']\n",
    "        js=ls.join(rs)\n",
    "        js['gap']=js['r time'] - js['l time']\n",
    "        js['gap']=js['gap'].round(2)\n",
    "        display(js)\n",
    "        display(s1)\n",
    "        display(s2)\n",
    "        stagesmap={}\n",
    "        for x in range(0,min(len(s1),len(s2))):\n",
    "            stagesmap[s1['Stage ID'][x]]=s2['Stage ID'][x]\n",
    "        totaltime=sum(s1['elapsed time'])\n",
    "        acctime=0\n",
    "        s1time=s1.sort_values(\"elapsed time\",ascending=False,ignore_index=True)\n",
    "        ldfx=appals.get_metric_output_rowcnt(queryid=queryid)\n",
    "        rdfx=appbaseals.get_metric_output_rowcnt(queryid=queryid)\n",
    "\n",
    "        for x in range(0,len(s1time)):\n",
    "            sid1=int(s1time['Stage ID'][x])\n",
    "            sid2=int(stagesmap[sid1])\n",
    "            print(f\"============================================================\")\n",
    "            display(ldfx[ldfx['Stage ID']==sid1])\n",
    "            display(rdfx[ldfx['Stage ID']==sid2])\n",
    "            print(f\" Gazelle  Query {queryid}  Stage {sid1}\")\n",
    "            xf=appals.get_query_plan(stageid=sid1,show_simple_string=True)\n",
    "            print(f\" Photon  Query {queryid}  Stage {sid2}\")\n",
    "            xf=appbaseals.get_query_plan(stageid=sid2,show_simple_string=True)\n",
    "            acctime+=s1time['elapsed time'][x]\n",
    "            if acctime/totaltime>=0.9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11864a75",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# application run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "66e744e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pyhdfs\n",
    "\n",
    "\n",
    "fs = pyhdfs.HdfsClient(hosts=\"monarch-dev-015-20221117-namenode-1:50070\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7aa8a142",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dev', 'profile', 'shared', 'test', 'tmp', 'user']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.listdir(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db2298",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "30a149c5",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Application_Run:\n",
    "    def __init__(self, appid,**kwargs):\n",
    "        self.appid=appid\n",
    "        \n",
    "        basedir=kwargs.get(\"basedir\",\"profile\")\n",
    "        self.filedir=\"/\"+basedir+\"/\"+self.appid+\"/\"\n",
    "        self.basedir=basedir\n",
    "        \n",
    "        slaves=fs.list_status(\"/\"+basedir+\"/\"+appid)\n",
    "        slaves=[f['pathSuffix'] for f in slaves if f['type']=='DIRECTORY' and f['pathSuffix']!=\"summary.parquet\"]\n",
    "        \n",
    "        jobids=kwargs.get(\"jobids\",None)\n",
    "        \n",
    "        self.clients=slaves\n",
    "        \n",
    "        sarclnt={}\n",
    "        for idx,l in enumerate(self.clients):\n",
    "            sarclnt[l]={'sar_cpu':{'als':Sar_cpu_analysis(self.filedir + l + \"/\"+\"sar_cpu.sar\"),'pid':idx},\n",
    "                'sar_disk':{'als':Sar_disk_analysis(self.filedir + l + \"/\"+\"sar_disk.sar\"),'pid':idx},\n",
    "                'sar_mem':{'als':Sar_mem_analysis(self.filedir + l + \"/\"+\"sar_mem.sar\"),'pid':idx},\n",
    "                'sar_nic':{'als':Sar_nic_analysis(self.filedir + l + \"/\"+\"sar_nic.sar\"),'pid':idx}\n",
    "            }\n",
    "            if fs.exists(self.filedir + l + \"/sar_page.sar\"):\n",
    "                sarclnt[l]['sar_page']={'als':Sar_PageCache_analysis(self.filedir + l + \"/\"+\"sar_page.sar\"),'pid':idx}\n",
    "                \n",
    "        self.analysis={\n",
    "            \"sar\": sarclnt\n",
    "        }\n",
    "        \n",
    "        realappid=appid[5:] if not appid.startswith(\"application_\") else appid\n",
    "\n",
    "        if fs.exists(\"/shared/spark-logs/\"+realappid):\n",
    "            self.analysis['app']={'als':App_Log_Analysis(\"/shared/spark-logs/\"+realappid,jobids)}\n",
    "        elif fs.exists(\"/shared/spark-logs/\"+realappid + \"_1\"):\n",
    "            self.analysis['app']={'als':App_Log_Analysis(\"/shared/spark-logs/\"+realappid+\"_1\",jobids)}\n",
    "        \n",
    "        self.starttime=0\n",
    "        if fs.exists(self.filedir+\"starttime\"):\n",
    "            with fs.open(self.filedir+\"starttime\") as f:\n",
    "                st = f.read().decode('ascii')\n",
    "                self.starttime=int(st)\n",
    "    \n",
    "    def generate_trace_view(self,showsar=True,showemon=False,showgpu=True,showhbm=False,**kwargs):\n",
    "        traces=[]\n",
    "        shownodes=kwargs.get(\"shownodes\",self.clients)\n",
    "        for l in shownodes:\n",
    "            if l not in self.clients:\n",
    "                print(l,\"is not in clients\",self.clients)\n",
    "                return\n",
    "        self.clients=shownodes\n",
    "        \n",
    "        xgbtcks=kwargs.get('xgbtcks',(\"calltrain\",'enter','begin','end'))\n",
    "        \n",
    "        if \"app\" in self.analysis:\n",
    "            appals=self.analysis['app']['als']\n",
    "            appals.starttime=self.starttime\n",
    "            traces.extend(appals.generate_trace_view_list(self.analysis['app'],**kwargs))\n",
    "            self.starttime=appals.starttime\n",
    "        \n",
    "        if 'instant' in self.analysis:\n",
    "            als=self.analysis['instant']['als']\n",
    "            als.starttime=self.starttime\n",
    "            traces.extend(als.generate_trace_view_list(**kwargs))\n",
    "        \n",
    "        counttime=kwargs.get(\"counttime\",False)\n",
    "        \n",
    "        pidmap={}\n",
    "        if showsar:\n",
    "            for l in self.clients:\n",
    "                for alskey, sarals in self.analysis[\"sar\"][l].items():\n",
    "                    t1 = time.time()\n",
    "                    if alskey!=\"emon\":\n",
    "                        sarals['als'].starttime=self.starttime\n",
    "                        traces.extend(sarals['als'].generate_trace_view_list(sarals['pid'],node=l, **kwargs))\n",
    "                    elif showemon:\n",
    "                        sarals['als'].load_data()\n",
    "                        pidmap[l]=sarals['pid']\n",
    "                    if counttime:\n",
    "                        print(l,alskey,\" spend time: \", time.time()-t1)\n",
    "        if showemon:\n",
    "            t1 = time.time()\n",
    "            emondfs=get_emon_parquets([self.appid,],self.basedir)\n",
    "            emons=Emon_Analysis_All(emondfs)\n",
    "            emons.starttime=self.starttime\n",
    "            traces.extend(emons.generate_trace_view_list(0,pidmap=pidmap,**kwargs))\n",
    "            if counttime:\n",
    "                print(\"emon process spend time: \", time.time()-t1)\n",
    "            self.emons=emons\n",
    "        \n",
    "        if showhbm:\n",
    "            for l in self.clients:\n",
    "                t1 = time.time()\n",
    "                hbm_analysis=HBM_analysis(self.filedir + l + \"/numactl.csv\")\n",
    "                hbm_analysis.starttime=self.starttime\n",
    "                traces.extend(hbm_analysis.generate_trace_view_list(0,**kwargs))\n",
    "                if counttime:\n",
    "                    print(l, \" hbm process spend time: \", time.time()-t1)\n",
    "        \n",
    "        for idx,l in enumerate(self.clients):\n",
    "            traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":idx,\"tid\":0,\"args\":{\"sort_index \":idx}}))\n",
    "            traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":idx+100,\"tid\":0,\"args\":{\"sort_index \":idx+100}}))\n",
    "            traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":idx+200,\"tid\":0,\"args\":{\"sort_index \":idx+200}}))\n",
    "        \n",
    "        if \"app\" in self.analysis:\n",
    "            for pid in self.analysis['app']['als'].pids:\n",
    "                traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":pid+200,\"tid\":0,\"args\":{\"sort_index \":pid+200}}))\n",
    "\n",
    "        allcnt=\"\"\n",
    "        for c in self.clients:\n",
    "            paths=self.filedir+c\n",
    "            if fs.exists(paths+\"/xgbtck.txt\"):\n",
    "                with fs.open(paths+\"/xgbtck.txt\") as f:\n",
    "                    tmp = f.read().decode('ascii')\n",
    "                    allcnt=allcnt+tmp\n",
    "        allcnt=allcnt.strip().split(\"\\n\")\n",
    "        if len(allcnt) > 1:\n",
    "            allcnt=[l.split(\" \") for l in allcnt]\n",
    "            cnts=pandas.DataFrame([[l[0],l[1],l[2],l[3]] for l in allcnt if len(l)>1 and l[1] in xgbtcks])\n",
    "            if len(cnts) > 0:\n",
    "                cnts.columns=['xgbtck','name','rank','time']\n",
    "                cntgs=cnts.groupby(\"name\").agg({\"time\":\"min\"})\n",
    "                cntgs=cntgs.reset_index()\n",
    "                cntgs.columns=['name','ts']\n",
    "                cntgs['ph']=\"i\"\n",
    "                cntgs['ts']=pandas.to_numeric(cntgs['ts'])-self.starttime\n",
    "                cntgs['pid']=0\n",
    "                cntgs['tid']=0\n",
    "                cntgs['s']='g'\n",
    "                traces.extend([json.dumps(l) for l in cntgs.to_dict(orient='records')])\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ],\n",
    "            \"displayTimeUnit\": \"ns\"\n",
    "        }'''\n",
    "\n",
    "        with open('/data/nvme1n1/binwei/trace_view/'+self.appid+'.json', 'w') as outfile:  \n",
    "            outfile.write(output)\n",
    "\n",
    "        display(HTML(\"<a href = http://127.0.0.1:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+self.appid+\".json>http://127.0.0.1:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+self.appid+\".json</a>\"))\n",
    "        \n",
    "    def get_sar_stat(app_ww44,**kwargs):\n",
    "        starttime=kwargs.get(\"starttime\",0)\n",
    "        endtime=kwargs.get(\"endtime\",0)\n",
    "        \n",
    "        cpustat=[app_ww44.analysis[\"sar\"][l]['sar_cpu']['als'].get_stat(**kwargs) for l in app_ww44.clients]\n",
    "        cpustat=reduce(lambda l,r:l.join(r),cpustat)\n",
    "        diskstat=[app_ww44.analysis[\"sar\"][l]['sar_disk']['als'].get_stat(**kwargs) for l in app_ww44.clients]\n",
    "        diskstat=reduce(lambda l,r:l.join(r),diskstat)\n",
    "        memstat=[app_ww44.analysis[\"sar\"][l]['sar_mem']['als'].get_stat(**kwargs) for l in app_ww44.clients]\n",
    "        memstat=reduce(lambda l,r:l.join(r),memstat)\n",
    "        nicstat=[app_ww44.analysis[\"sar\"][l]['sar_nic']['als'].get_stat(**kwargs) for l in app_ww44.clients]\n",
    "        nicstat=reduce(lambda l,r:l.join(r),nicstat)\n",
    "        pagestat=[app_ww44.analysis[\"sar\"][l]['sar_page']['als'].get_stat(**kwargs) for l in app_ww44.clients]\n",
    "        pagestat=reduce(lambda l,r:l.join(r),pagestat)\n",
    "        pandas.options.display.float_format = '{:,,.2f}'.format\n",
    "        return pandas.concat([cpustat,diskstat,memstat,nicstat,pagestat])\n",
    "    \n",
    "    def get_summary(app, **kwargs):\n",
    "        output=[]\n",
    "        \n",
    "        appals=app.analysis[\"app\"][\"als\"]\n",
    "        \n",
    "        out=appals.get_query_time(plot=False)\n",
    "        \n",
    "        lrun=app.appid\n",
    "        \n",
    "        df=spark.read.json(appals.file)\n",
    "        query_endtime=df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd'\").agg(F.max(\"time\").alias(\"end\")).collect()\n",
    "        query_endtime=query_endtime[0]['end']\n",
    "        query_starttime=df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart'\").agg(F.min(\"time\").alias(\"start\")).collect()\n",
    "        query_starttime=query_starttime[0]['start']\n",
    "\n",
    "        qtime=df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd'\").select(F.col(\"executionid\"),F.col(\"time\").alias(\"end\")).join(df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart'\").select(F.col(\"executionid\"),F.col(\"time\").alias(\"start\")),on=\"executionid\").agg(((F.max(\"end\")-F.min(\"start\"))/1000).alias(\"query_elapsed\")).collect()\n",
    "        jtime=df.where(\"Event='SparkListenerJobStart'\").select(\"job ID\",\"Submission Time\").join(df.where(\"Event='SparkListenerJobEnd'\").select(\"job ID\",\"Completion Time\"),on=\"Job ID\").agg(((F.max(\"Completion Time\")-F.min(\"Submission Time\"))/1000).alias('job time')).collect()\n",
    "\n",
    "        v={'query_time':[qtime[0]['query_elapsed']],'job_time':[jtime[0]['job time']]}\n",
    "        pqdf=pandas.DataFrame(v).T\n",
    "        pqdf.columns=[lrun]\n",
    "            \n",
    "        cmpcolumns=['runtime','disk spilled','shuffle_write','f_wait_time','input read','acc_task_time','output rows']\n",
    "        outcut=out[cmpcolumns]\n",
    "        \n",
    "        pdsout=pandas.DataFrame(outcut.sum(),columns=[lrun])\n",
    "        pdstime=pdsout\n",
    "\n",
    "        node=\"\"\n",
    "        for l in fs.list_status(app.filedir):\n",
    "            if l['type']==\"DIRECTORY\" and l['pathSuffix']!=\"summary.parquet\":\n",
    "                node=l['pathSuffix']\n",
    "                break\n",
    "        sardf=app.get_sar_stat(starttime=query_starttime,endtime=query_endtime,**kwargs)\n",
    "        \n",
    "        def get_sar_agg(sardf):\n",
    "            aggs=[]\n",
    "            for x in sardf.index:\n",
    "                if \"total\" in x:\n",
    "                    aggs.append(sardf.loc[x].sum())\n",
    "                elif \"max\" in x:\n",
    "                    aggs.append(sardf.loc[x].max())\n",
    "                else:\n",
    "                    aggs.append(sardf.loc[x].mean())\n",
    "\n",
    "            sardf['agg']=aggs\n",
    "            return sardf\n",
    "        sardf=get_sar_agg(sardf)\n",
    "\n",
    "        sarsum=sardf[[\"agg\"]]\n",
    "\n",
    "        sarsum.columns=[lrun]\n",
    "        \n",
    "        summary=pandas.concat([pqdf,pdstime,sarsum])\n",
    "            \n",
    "        df_sum=spark.createDataFrame(summary.T.reset_index())\n",
    "        for c in df_sum.columns:\n",
    "            df_sum=df_sum.withColumnRenamed(c,c.replace(\" \",\"_\").replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "        df_sum.write.mode(\"overwrite\").parquet(app.filedir+\"summary.parquet\")\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def show_queryplan_diff(app2, queryid,**kwargs):\n",
    "        lbasedir=kwargs.get(\"basedir\",app2.basedir)\n",
    "        r_appid=kwargs.get(\"r_appid\",app2.appid)\n",
    "        \n",
    "        app=kwargs.get(\"rapp\",Application_Run(r_appid,basedir=lbasedir))\n",
    "\n",
    "        appals=app.analysis[\"app\"][\"als\"]\n",
    "        appals2=app2.analysis[\"app\"][\"als\"]\n",
    "\n",
    "        hotstagel=appals.get_hottest_stages(plot=False)\n",
    "        hotstager=appals2.get_hottest_stages(plot=False)\n",
    "        hotstagel.style.format(lambda x: '''{:,,.2f}'''.format(x))\n",
    "\n",
    "        loperators=appals.getOperatorCount()\n",
    "        roperators=appals2.getOperatorCount()\n",
    "        loperators_rowcnt=appals.get_metric_output_rowcnt()\n",
    "        roperators_rowcnt=appals2.get_metric_output_rowcnt()\n",
    "\n",
    "        lrun=app.appid\n",
    "        rrun=app2.appid\n",
    "\n",
    "        output=[]\n",
    "\n",
    "        def show_query_diff(queryid):\n",
    "            lops=pandas.DataFrame(loperators[queryid])\n",
    "            lops.columns=['calls_l']\n",
    "            lops=lops.loc[lops['calls_l'] >0]\n",
    "\n",
    "            rops=pandas.DataFrame(roperators[queryid])\n",
    "            rops.columns=[\"calls_r\"]\n",
    "            rops=rops.loc[rops['calls_r'] >0]\n",
    "            lops_row=pandas.DataFrame(loperators_rowcnt[queryid])\n",
    "            lops_row.columns=[\"rows_l\"]\n",
    "            lops_row=lops_row.loc[lops_row['rows_l'] >0]\n",
    "\n",
    "            rops_row=pandas.DataFrame(roperators_rowcnt[queryid])\n",
    "            rops_row.columns=[\"rows_r\"]\n",
    "            rops_row=rops_row.loc[rops_row['rows_r'] >0]\n",
    "\n",
    "            opscmp=pandas.merge(pandas.merge(pandas.merge(lops,rops,how=\"outer\",left_index=True,right_index=True),lops_row,how=\"outer\",left_index=True,right_index=True),rops_row,how=\"outer\",left_index=True,right_index=True)\n",
    "            opscmp=opscmp.fillna(\"\")\n",
    "\n",
    "            def set_bk_color_opscmp(x):\n",
    "                calls_l= 0 if x['calls_l']==\"\" else x['calls_l']\n",
    "                calls_r= 0 if x['calls_r']==\"\" else x['calls_r']\n",
    "                rows_l= 0 if x['rows_l']==\"\" else x['rows_l']\n",
    "                rows_r= 0 if x['rows_r']==\"\" else x['rows_r']\n",
    "\n",
    "                if calls_l > calls_r or rows_l > rows_r:\n",
    "                    return ['background-color:#eb6b34']*4\n",
    "                if calls_l < calls_r or rows_l < rows_r:\n",
    "                    return ['background-color:#8ad158']*4\n",
    "                return ['color:#dbd4d0']*4\n",
    "\n",
    "            output.append(opscmp.style.apply(set_bk_color_opscmp,axis=1).render())\n",
    "\n",
    "            planl=appals.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "            planr=appals2.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "            output.append(\"<table><tr><td>\"+planl+\"</td><td>\"+planr+\"</td></tr></table>\")\n",
    "\n",
    "        x=queryid\n",
    "        print(\"query \",x,\" queryplan diff \")\n",
    "        #output.append(f\"<p><font size=4 color=red>query{x} is different,{lrun} time: {df1['runtime'][x][0]}, {rrun} time: {df1['runtime'][x][1]}</font></p>\")\n",
    "        show_query_diff(x)\n",
    "        display(HTML(\"\\n\".join(output)))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c77136",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5dc4c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc257a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f45f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a50a10",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9d13a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be072df8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
